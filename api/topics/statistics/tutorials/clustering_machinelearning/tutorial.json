{"layout":"tutorial_hands_on","title":"Clustering in Machine Learning","zenodo_link":"https://zenodo.org/record/3813447","questions":["How to use clustering algorithms to categorize data in different clusters"],"objectives":["Learn clustering background","Learn hierarchical clustering algorithm","Learn k-means clustering algorithm","Learn DBSCAN clustering algorithm","Apply clustering algorithms to different datasets","Learn how to visualize clusters"],"key_points":["Using clustering methods, clusters inside a dataset are drawn using hierarchical, k-means and DBSCAN","For each clustering algorithm, the number of clusters and their respective hyperparameters should be optimised based on the dataset"],"time_estimation":"2H","contributors":[{"name":"Alireza Khanteymoori","email":"khanteymoori@gmail.com","orcid":"0000-0001-6811-9196","joined":"2019-07","former_affiliations":["uni-freiburg","elixir-europe"],"id":"khanteymoori","url":"https://training.galaxyproject.org/training-material/api/contributors/khanteymoori.json","page":"https://training.galaxyproject.org/training-material/hall-of-fame/khanteymoori/"},{"name":"Anup Kumar","email":"anup.rulez@gmail.com","twitter":"musafirtweetsz","joined":"2018-08","elixir_node":"de","affiliations":["uni-freiburg","eurosciencegateway","elixir-europe"],"id":"anuprulez","url":"https://training.galaxyproject.org/training-material/api/contributors/anuprulez.json","page":"https://training.galaxyproject.org/training-material/hall-of-fame/anuprulez/"}],"js_requirements":{"mathjax":null,"mermaid":false},"short_id":"T00264","url":"/topics/statistics/tutorials/clustering_machinelearning/tutorial.html","topic_name":"statistics","tutorial_name":"clustering_machinelearning","dir":"topics/statistics/tutorials/clustering_machinelearning","symlink":null,"id":"statistics/clustering_machinelearning","ref_tutorials":["<p>The goal of <a href=\"https://en.wikipedia.org/wiki/Unsupervised_learning\">unsupervised learning</a> is to discover hidden patterns in any unlabeled data. One of the approaches to unsupervised learning is <a href=\"https://en.wikipedia.org/wiki/Cluster_analysis\">clustering</a>. In this tutorial, we will discuss clustering, its types and a few algorithms to find clusters in data. Clustering groups data points based on their similarities. Each group is called a cluster and contains data points with high similarity and low similarity with data points in other clusters. In short, data points of a cluster are more similar to each other than they are to the data points of other clusters. The goal of clustering is to divide a set of data points in such a way that similar items fall into the same cluster, whereas dissimilar data points fall in different clusters. Further in this tutorial, we will discuss ideas on how to choose different metrics of similarity  between data points and use them in different clustering algorithms.</p>\n\n<p>Clustering is crucial in multiple research fields in BioInformatics such as analyzing unlabeled data which can be gene expressions profiles, biomedical images and so on. For example, clustering is often used in gene expression analysis to find groups of genes with similar expression patterns which may provide a useful understanding of gene functions and regulations, cellular processes and so on. For more details, please refer to <a href=\"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0171429\">ref1</a> and <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5135122/\">ref2</a>.</p>\n\n<p>We represent an observation/sample/data point as an n-dimensional vector and many such data points constitute a dataset. To show an example, let us assume that a dataset, shown in Figure 1, contains many samples and each sample has two dimensions each:</p>\n\n<figure id=\"figure-1\" style=\"max-width: 90%;\"><img src=\"images/data_before_clustering.png\" alt=\"data. \" width=\"394\" height=\"262\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/data_before_clustering.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 1</strong>:</span> Sample data before clustering</figcaption></figure>\n\n<p>Clustering reveals the following three groups, indicated by different colors:</p>\n\n<figure id=\"figure-2\" style=\"max-width: 90%;\"><img src=\"images/data_after_clustering.png\" alt=\"data. \" width=\"370\" height=\"248\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/data_after_clustering.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 2</strong>:</span> Sample data after clustering</figcaption></figure>\n\n<p>Clustering is divided into two subgroups based on the assignment of data points to clusters:</p>\n\n<ul>\n  <li>\n    <p>Hard: Each data point is assigned to exactly one cluster. One example is <a href=\"https://en.wikipedia.org/wiki/K-means_clustering\">k-means</a> clustering.</p>\n  </li>\n  <li>\n    <p>Soft: Each data point is assigned a probability or likelihood of being in a cluster. One example is <a href=\"https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\">expectation-maximization</a> (EM) algorithm.</p>\n  </li>\n</ul>\n\n<blockquote class=\"agenda\">\n  <agenda-title></agenda-title>\n\n  <p>In this tutorial, we will cover:</p>\n\n<ol id=\"markdown-toc\">\n  <li><a href=\"#types-of-clustering-algorithms\" id=\"markdown-toc-types-of-clustering-algorithms\">Types of clustering algorithms</a></li>\n  <li><a href=\"#clustering-distance-measures\" id=\"markdown-toc-clustering-distance-measures\">Clustering distance measures</a></li>\n  <li><a href=\"#different-clustering-approaches\" id=\"markdown-toc-different-clustering-approaches\">Different clustering approaches</a>    <ol>\n      <li><a href=\"#hierarchical-clustering\" id=\"markdown-toc-hierarchical-clustering\">Hierarchical clustering</a></li>\n      <li><a href=\"#k-means-clustering\" id=\"markdown-toc-k-means-clustering\">K-means clustering</a></li>\n      <li><a href=\"#dbscan-clustering\" id=\"markdown-toc-dbscan-clustering\">DBSCAN clustering</a></li>\n    </ol>\n  </li>\n  <li><a href=\"#applying-clustering-algorithms-on-multiple-datasets\" id=\"markdown-toc-applying-clustering-algorithms-on-multiple-datasets\">Applying clustering algorithms on multiple datasets</a>    <ol>\n      <li><a href=\"#visualise-datasets\" id=\"markdown-toc-visualise-datasets\">Visualise datasets</a></li>\n      <li><a href=\"#find-clusters\" id=\"markdown-toc-find-clusters\">Find clusters</a></li>\n      <li><a href=\"#visualise-clusters\" id=\"markdown-toc-visualise-clusters\">Visualise clusters</a></li>\n    </ol>\n  </li>\n  <li><a href=\"#conclusion\" id=\"markdown-toc-conclusion\">Conclusion</a></li>\n</ol>\n\n</blockquote>\n\n<h1 id=\"types-of-clustering-algorithms\">Types of clustering algorithms</h1>\n\n<p>There are many algorithms available for data clustering which use different ways to establish similarity between data points. The clustering algorithms can be broadly divided into many categories such as connectivity model, centroid model, density model, distribution model, group model, graph-based model and so on. Some of these are discussed below:</p>\n\n<ul>\n  <li>\n    <p>Connectivity model: This model assigns higher similarity to data points which are closer in one or multi-dimensional space than those points which are farther away. There are two approaches - first, it categorises all data points into different clusters and then merges the data points in relation to the distances among them. Second, it categorises all data points into one single cluster and then partitions them into different clusters as the distance increases. This model is easy to understand but has problems in handling large datasets. One example is <a href=\"https://en.wikipedia.org/wiki/Hierarchical_clustering\">hierarchical clustering</a> and its variants.</p>\n  </li>\n  <li>\n    <p>Centroid model: It is an iterative clustering algorithm in which similarity is based on the proximity of a data point to the centroids of the clusters. <a href=\"https://en.wikipedia.org/wiki/K-means_clustering\">K-means</a> clustering is one example of this model. It needs a number of clusters before running and then divides data points into these many clusters iteratively. Therefore, to use k-means, users should acquire some prior knowledge about the dataset.</p>\n  </li>\n  <li>\n    <p>Density model: This model searches one or multi-dimensional space for dense regions (having a large number of data points in a small region). A popular example of a density model is <a href=\"https://en.wikipedia.org/wiki/DBSCAN\">DBSCAN</a>.</p>\n  </li>\n</ul>\n\n<p>In this tutorial, we will go through three clustering algorithms - hierarchical clustering, k-means, DBSCAN, and a comparison between these methods. Further, we will discuss their parameters and how to apply them to find clusters in the <a href=\"https://en.wikipedia.org/wiki/Iris_flower_data_set\">iris flower dataset</a> and a few other datasets.</p>\n\n<h1 id=\"clustering-distance-measures\">Clustering distance measures</h1>\n\n<p>Clustering groups similar data points and requires a metric or measure to compute a degree of similarity or dissimilarity of data points. Two main types of measures are distance and similarity. The smaller the distance between two objects, the more similar they are to each other. Moreover, the type of distance measure depends on the problem and one measure may not work with all kinds of problems.</p>\n\n<p>Many clustering algorithms use distance measures to determine the similarity or dissimilarity between any pair of data points. A valid distance measure should be symmetric and obtains its minimum value (usually zero) in case of identical data points. By computing the distance or (dis)similarity between each pair of observations, a dissimilarity or distance matrix is obtained.</p>\n\n<p>The choice of a distance measure is crucial in clustering. It defines how the similarity of two elements <code class=\"language-plaintext highlighter-rouge\">(x, y)</code> is calculated as it influences the shape of the clusters. The classical distance measures are <a href=\"https://en.wikipedia.org/wiki/Euclidean_distance\">euclidean</a> and <a href=\"https://en.wikipedia.org/wiki/Taxicab_geometry\">manhattan</a> distances. For the most common clustering algorithms, the default distance measure is euclidean. If the euclidean distance is chosen, then observations having high magnitudes of their respective features will be clustered together. The same holds for the observations having low magnitudes of their respective features. In Figure 3, we group the cells using euclidean distance and their distance matrix.</p>\n\n<figure id=\"figure-3\" style=\"max-width: 90%;\"><div style=\"overflow-x: auto\"><object data=\"images/raceid_distance.svg\" type=\"image/svg+xml\" alt=\"Distances. \">Distances.</object></div><a target=\"_blank\" href=\"images/raceid_distance.svg\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 3</strong>:</span> Euclidean distance between three points (R, P, V) across three features (G1, G2, G3)</figcaption></figure>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <ol>\n    <li>Why are there zeroes along the diagonal of the above example distance matrix?</li>\n    <li>Is there any symmetry in this matrix?</li>\n  </ol>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <ol>\n      <li>The distance between a point to itself is zero.</li>\n      <li>The distance between point <em>a</em> to point <em>b</em> is the same as the distance between point <em>b</em> to point <em>a</em> using the Euclidean distance metric.</li>\n    </ol>\n\n  </blockquote>\n\n</blockquote>\n\n<p>Other dissimilarity measures exist such as correlation-based distances, which are widely used for gene expression data analyses. Correlation-based distance considers two objects to be similar if their features are highly correlated, even though the observed values may be far apart in terms of euclidean distance. The distance between the two objects is 0 when they are perfectly correlated. <a href=\"https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\">Pearson’s correlation</a> is quite sensitive to outliers. This does not matter when clustering samples because the correlation is over thousands of genes. However, it is important to be aware of the possible impact of outliers. This can be mitigated by using <a href=\"https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\">Spearman’s correlation</a> instead of Pearson’s correlation.</p>\n\n<h1 id=\"different-clustering-approaches\">Different clustering approaches</h1>\n\n<h2 id=\"hierarchical-clustering\">Hierarchical clustering</h2>\n\n<p>Hierarchical clustering creates a hierarchy of clusters. It starts with all the data points assigned to clusters of their own. Then, the two nearest clusters are merged into the same cluster. In the end, the algorithm terminates when there is only one cluster left.</p>\n\n<p>Following are the steps that are performed during hierarchical clustering:</p>\n\n<ol>\n  <li>\n    <p>In the beginning, every data point in the dataset is treated as a cluster which means that we have <code class=\"language-plaintext highlighter-rouge\">N</code> clusters at the beginning of the algorithm for a dataset of size <code class=\"language-plaintext highlighter-rouge\">N</code>.</p>\n  </li>\n  <li>\n    <p>The distance between all the points is calculated and two points closest to each other are merged together to form a new cluster.</p>\n  </li>\n  <li>\n    <p>Next, the point which is closest to the cluster formed in step 2, will be merged to the cluster.</p>\n  </li>\n  <li>\n    <p>Steps 2 and 3 are repeated until one large cluster is created.</p>\n  </li>\n  <li>\n    <p>Finally, this large cluster is divided into K small clusters with the help of dendrograms.</p>\n  </li>\n</ol>\n\n<p>Let’s now see how dendrograms help in hierarchical clustering.</p>\n\n<figure id=\"figure-4\" style=\"max-width: 90%;\"><img src=\"images/Hierarchical_clustering_1.png\" alt=\"data. \" width=\"560\" height=\"420\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/Hierarchical_clustering_1.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 4</strong>:</span> Hierarchical clustering</figcaption></figure>\n\n<p>All data points are chosen at the bottom and each one is assigned to a separate cluster. Then, the two closest clusters are merged till just one cluster is left at the top. From the dendrogram thus formed, the distance between two clusters can be determined by computing the height at which two clusters are merged.</p>\n\n<p>By looking at the dendrogram, the clusters can be observed showing different groups in the best way. The optimal number of clusters is the number of vertical lines in the dendrogram cut by a horizontal line that can transverse maximum distance vertically without intersecting a cluster.</p>\n\n<p>In the above example, the best choice of the number of clusters will be 4 as the red horizontal line in the dendrogram below covers maximum vertical distance AB. For more details, please read <a href=\"https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/\">this blogpost entitled <em>“Clustering | Introduction, Different Methods, and Applications”</em></a>.</p>\n<figure id=\"figure-5\" style=\"max-width: 90%;\"><img src=\"images/Hierarchical_clustering_2.png\" alt=\"data. \" width=\"768\" height=\"480\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/Hierarchical_clustering_2.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 5</strong>:</span> Hierarchical clustering</figcaption></figure>\n\n<p>This algorithm explained above uses the bottom-up approach. It is also possible to follow the top-down approach starting with all data points assigned in the same cluster and recursively performing splits till each data point is assigned a separate cluster. The decision of merging two clusters is taken based on the proximity of these clusters.</p>\n\n<blockquote class=\"comment\">\n  <comment-title>Background of the iris dataset</comment-title>\n  <p>The iris flower dataset or Fisher’s iris dataset is a multivariate dataset introduced by the British statistician and biologist Ronald Fisher in his 1936 paper (<span class=\"citation\"><a href=\"#Fisher1936\">Fisher 1936</a></span>).\nEach row of the table represents an iris flower, including its species and dimensions of its botanical parts, sepal and petal, in centimeters.\nFor more history of this dataset read here <a href=\"https://en.wikipedia.org/wiki/Iris_flower_data_set\">Wikipedia</a>.</p>\n</blockquote>\n\n<p>At the first step, we should upload the iris dataset and two other datasets which will be used at the end of the tutorial.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Data upload</hands-on-title>\n\n  <ol>\n    <li>\n      <p><strong>Import</strong> <i class=\"fas fa-upload\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-upload</span> the file <code class=\"language-plaintext highlighter-rouge\">iris.csv</code> from <a href=\"https://zenodo.org/record/3813447/files/iris.csv\">Zenodo</a> or from the data library</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>https://zenodo.org/record/3813447/files/iris.csv\nhttps://zenodo.org/record/3813447/files/circles.csv\nhttps://zenodo.org/record/3813447/files/moon.csv\n</code></pre></div>      </div>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-importing-via-links\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-importing-via-links\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Importing via links</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Copy the link location</li>   <li>     <p>Click <i class=\"fas fa-upload\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-upload</span> <strong>Upload Data</strong> at the top of the tool panel</p>   </li>   <li>Select <i class=\"fa fa-edit\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-wf-edit</span> <strong>Paste/Fetch Data</strong></li>   <li>     <p>Paste the link(s) into the text field</p>   </li>   <li>     <p>Press <strong>Start</strong></p>   </li>   <li><strong>Close</strong> the window</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-importing-data-from-a-data-library\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-importing-data-from-a-data-library\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Importing data from a data library</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <p>As an alternative to uploading the data from a URL or your computer, the files may also have been made available from a <em>shared data library</em>:</p>   <ol>   <li>Go into <strong>Shared data</strong> (top panel) then <strong>Data libraries</strong></li>   <li>Navigate to  the correct folder as indicated by your instructor.     <ul>       <li>On most Galaxies tutorial data will be provided in a folder named <strong>GTN - Material –&gt; Topic Name -&gt; Tutorial Name</strong>.</li>     </ul>   </li>   <li>Select the desired files</li>   <li>Click on <strong>Add to History</strong> <i class=\"fas fa-caret-down\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-dropdown</span> near the top and select <strong>as Datasets</strong> from the dropdown menu</li>   <li>     <p>In the pop-up window, choose</p>     <ul>       <li><em>“Select history”</em>: the history you want to import the data to (or create a new one)</li>     </ul>   </li>   <li>Click on <strong>Import</strong></li> </ol> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p><strong>Rename</strong> <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> the datasets to <code class=\"language-plaintext highlighter-rouge\">iris</code>, <code class=\"language-plaintext highlighter-rouge\">circles</code> and <code class=\"language-plaintext highlighter-rouge\">moon</code> respectively.</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-renaming-a-dataset\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-renaming-a-dataset\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Renaming a dataset</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Click on the <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, change the <strong>Name</strong> field</li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p>Check the <strong>datatype</strong></p>\n      <ul>\n        <li>Click on the history item to expand it to get more information.</li>\n        <li>The datatype of the iris dataset should be <code class=\"language-plaintext highlighter-rouge\">csv</code>.</li>\n        <li><strong>Change</strong> <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> the datatype <em>if</em> it is different than <code class=\"language-plaintext highlighter-rouge\">csv</code>.\n          <ul>\n            <li>Option 1: Datatypes can be <strong>autodetected</strong></li>\n            <li>Option 2: Datatypes can be <strong>manually set</strong></li>\n          </ul>\n        </li>\n      </ul>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-detecting-the-datatype-file-format\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-detecting-the-datatype-file-format\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Detecting the datatype (file format)</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Click on the <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, click on the <i class=\"fas fa-database\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-chart-select-data</span> <strong>Datatypes</strong> tab on the top</li>   <li>Click the <strong>Auto-detect</strong> button to have Galaxy try to autodetect it.</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-changing-the-datatype\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-changing-the-datatype\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Changing the datatype</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Click on the <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, click <i class=\"fas fa-database\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-chart-select-data</span> <strong>Datatypes</strong> tab on the top</li>   <li>In the <i class=\"fas fa-database\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-chart-select-data</span> <strong>Assign Datatype</strong>, select <code class=\"language-plaintext highlighter-rouge\">csv</code> from “<em>New type</em>” dropdown     <ul>       <li>Tip: you can start typing the datatype into the field to filter the dropdown menu</li>     </ul>   </li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n  </ol>\n\n</blockquote>\n\n<p>Our objective is to categorise similar flowers in different groups (Figure 6). We know that we have <strong>3</strong> species of iris flowers (versicolor, virginica, setosa) with\n<strong>50</strong> samples for each. These species look very much alike as shown in the figure below.</p>\n\n<figure id=\"figure-6\" style=\"max-width: 90%;\"><img src=\"images/iris_flowers.png\" alt=\"3 species of iris flowers. \" width=\"600\" height=\"181\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/iris_flowers.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 6</strong>:</span> 3 species of iris flowers</figcaption></figure>\n\n<p>In our dataset, we have the following features measured for each flower: <a href=\"https://en.wikipedia.org/wiki/Petal\">petal</a> length, petal width, <a href=\"https://en.wikipedia.org/wiki/Sepal\">sepal</a> length, sepal width</p>\n\n<p>Figure 7 shows the dendrogram of these data.</p>\n\n<figure id=\"figure-7\" style=\"max-width: 90%;\"><img src=\"images/Hierarchical_iris.png\" alt=\"data. \" width=\"1446\" height=\"611\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/Hierarchical_iris.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 7</strong>:</span> Iris data hierarchical clustering</figcaption></figure>\n\n<p>We will apply hierarchical clustering to the iris dataset to find clusters based on two features (of flowers) - sepal length and width.\n<strong>Hint</strong>: Please find the <code class=\"language-plaintext highlighter-rouge\">Numeric Clustering</code> tool in the <code class=\"language-plaintext highlighter-rouge\">Statistics</code> tool section.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Hierarchical clustering</hands-on-title>\n\n  <ol>\n    <li><strong>Numeric Clustering</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following clustering parameters:\n      <ul>\n        <li><em>“Select the format of input data”</em>: <code class=\"language-plaintext highlighter-rouge\">Tabular Format (tabular,txt)</code>\n          <ul>\n            <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Data file with numeric values”</em>: <code class=\"language-plaintext highlighter-rouge\">iris</code></li>\n            <li><i class=\"far fa-check-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-check</span> <em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n            <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns EXCLUDING some by column header name(s)</code>\n              <ul>\n                <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“Type header name(s)”</em>: <code class=\"language-plaintext highlighter-rouge\">Species</code></li>\n              </ul>\n            </li>\n            <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Clustering Algorithm”</em>: <code class=\"language-plaintext highlighter-rouge\">Hierarchical Agglomerative Clustering</code></li>\n            <li>In <em>“Advanced options”</em>\n              <ul>\n                <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“Number of clusters”</em>: <code class=\"language-plaintext highlighter-rouge\">2</code></li>\n                <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Affinity”</em>: <code class=\"language-plaintext highlighter-rouge\">Euclidean</code></li>\n                <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Linkage”</em>: <code class=\"language-plaintext highlighter-rouge\">ward</code></li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li>Rename the generated file to <code class=\"language-plaintext highlighter-rouge\">Hierarchical clustering</code></li>\n  </ol>\n</blockquote>\n\n<p>If you view the result table, you can see the last column is the label for each cluster and as you see, all the setosa samples are grouped in one cluster and two other species (versicolor and virginica) are grouped in the second cluster. From Figure 6, it is obvious that versicolor and virginica are more similar to each other.</p>\n\n<h3 id=\"visualize-hierarchical-clustering\">Visualize hierarchical clustering</h3>\n\n<p>The resulting candidate clustering can be visualized using the <code class=\"language-plaintext highlighter-rouge\">Scatterplot with ggplot2</code> tool. Each sample is color-coded based on its clustering for that sample.\nLet’s visualize the clustering results to see how groups have been built. <strong>Hint</strong>: Please find the <code class=\"language-plaintext highlighter-rouge\">Scatterplot with ggplot2</code> tool in the <code class=\"language-plaintext highlighter-rouge\">Graph/Display data</code> tool section.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Visualize hierarchical clustering result</hands-on-title>\n\n  <ol>\n    <li><strong>Scatterplot with ggplot2</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following parameters:\n      <ul>\n        <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Input tabular dataset”</em>: <strong>Hierarchical clustering</strong></li>\n        <li><em>“Column to plot on x-axis”</em>: <code class=\"language-plaintext highlighter-rouge\">1</code></li>\n        <li><em>“Column to plot on y-axis”</em>: <code class=\"language-plaintext highlighter-rouge\">2</code></li>\n        <li><em>“Plot title”</em>: <code class=\"language-plaintext highlighter-rouge\">Hierarchical clustering in iris data</code></li>\n        <li><em>“Label for x axis”</em>: <code class=\"language-plaintext highlighter-rouge\">Sepal length</code></li>\n        <li><em>“Label for y axis”</em>: <code class=\"language-plaintext highlighter-rouge\">Sepal width</code></li>\n        <li>In <em>“Advanced Options”</em>:\n          <ul>\n            <li><em>“Data point options”</em>: <code class=\"language-plaintext highlighter-rouge\">User defined point options</code>\n              <ul>\n                <li><em>“relative size of points”</em>: <code class=\"language-plaintext highlighter-rouge\">2.0</code></li>\n              </ul>\n            </li>\n            <li><em>“Plotting multiple groups”</em>: <code class=\"language-plaintext highlighter-rouge\">Plot multiple groups of data on one plot</code>\n              <ul>\n                <li><em>“column differentiating the different groups”</em>: <code class=\"language-plaintext highlighter-rouge\">6</code></li>\n                <li><em>“Color schemes to differentiate your groups”</em>: <code class=\"language-plaintext highlighter-rouge\">Set 2 - predefined color pallete</code></li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n        <li>In <em>“Output options”</em>:\n          <ul>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“width of output”</em>: <code class=\"language-plaintext highlighter-rouge\">7.0</code></li>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“height of output”</em>: <code class=\"language-plaintext highlighter-rouge\">5.0</code></li>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“dpi of output”</em>: <code class=\"language-plaintext highlighter-rouge\">175.0</code></li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li><strong>View</strong> <i class=\"far fa-eye\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-eye</span> the resulting plot</li>\n    <li>Rename to <code class=\"language-plaintext highlighter-rouge\">Hierarchical scatter plot</code></li>\n  </ol>\n</blockquote>\n\n<figure id=\"figure-8\" style=\"max-width: 90%;\"><img src=\"images/hierarchical_scatter.png\" alt=\"data. \" width=\"2100\" height=\"2100\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/hierarchical_scatter.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 8</strong>:</span> Hierarchical clustering scatter plot</figcaption></figure>\n\n<h2 id=\"k-means-clustering\">K-means clustering</h2>\n\n<p>K-means clustering is the most commonly used unsupervised machine learning algorithm for partitioning a given dataset into a set of k clusters, where k represents the number of groups pre-specified by the user. In k-means clustering, each cluster is represented by its center or centroid which corresponds to the mean of points assigned to the cluster. The basic idea behind k-means clustering is to define clusters and their centroids such that the total intra-cluster variation is minimized.</p>\n\n<p>K-means is popular because of its speed and scalability. Many variants of the k-means algorithm such as <a href=\"https://en.wikipedia.org/wiki/Lloyd%27s_algorithm\">Lloyd’s algorithm</a>, k-medians algorithms and so on are available. The standard algorithm defines the total within-cluster variation as the sum of squared Euclidean distances between items and the corresponding centroid. K is a hyperparameter of the algorithm and the k-means algorithm can be summarized as follows:</p>\n\n<ol>\n  <li>\n    <p>Specify the number of clusters (k) to be created (to be specified by users).</p>\n  </li>\n  <li>\n    <p>Select k data points randomly from the dataset as the initial cluster centers or means.</p>\n  </li>\n  <li>\n    <p>Assign each data point to their closest centroid, based on the euclidean distance between a data point and its centroid.</p>\n  </li>\n  <li>\n    <p>For each of the k clusters update cluster centroid by calculating the new mean values of all the data points in the cluster.</p>\n  </li>\n  <li>\n    <p>Iteratively minimize the total within the sum of squares: iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached.</p>\n  </li>\n</ol>\n\n<p>The parameters that minimize the cost function are learned through an iterative process of assigning data points to clusters and then moving the clusters. A restriction for the k-means algorithm is that the dataset should be continuous.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>K-means clustering</hands-on-title>\n\n  <ol>\n    <li><strong>Numeric Clustering</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following clustering parameters:\n      <ul>\n        <li><em>“Select the format of input data”</em>: <code class=\"language-plaintext highlighter-rouge\">Tabular Format (tabular,txt)</code>\n          <ul>\n            <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Data file with numeric values”</em>: <code class=\"language-plaintext highlighter-rouge\">iris</code></li>\n            <li><i class=\"far fa-check-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-check</span> <em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n            <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns EXCLUDING some by column header name(s)</code>\n              <ul>\n                <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“Type header name(s)”</em>: <code class=\"language-plaintext highlighter-rouge\">Species</code></li>\n              </ul>\n            </li>\n            <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Clustering Algorithm”</em>: <code class=\"language-plaintext highlighter-rouge\">KMeans</code></li>\n            <li>In <em>“Advanced options”</em>\n              <ul>\n                <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“Number of clusters”</em>: <code class=\"language-plaintext highlighter-rouge\">2</code></li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li>Rename the generated file to <code class=\"language-plaintext highlighter-rouge\">k-means clustering</code></li>\n  </ol>\n</blockquote>\n\n<h3 id=\"visualize-k-means-clustering\">Visualize k-means clustering</h3>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Visualize k-means clustering result</hands-on-title>\n\n  <ol>\n    <li><strong>Scatterplot with ggplot2</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following parameters:\n      <ul>\n        <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Input tabular dataset”</em>: <strong>k-means clustering</strong></li>\n        <li><em>“Column to plot on x-axis”</em>: <code class=\"language-plaintext highlighter-rouge\">1</code></li>\n        <li><em>“Column to plot on y-axis”</em>: <code class=\"language-plaintext highlighter-rouge\">2</code></li>\n        <li><em>“Plot title”</em>: <code class=\"language-plaintext highlighter-rouge\">K-means clustering in iris data</code></li>\n        <li><em>“Label for x axis”</em>: <code class=\"language-plaintext highlighter-rouge\">Sepal length</code></li>\n        <li><em>“Label for y axis”</em>: <code class=\"language-plaintext highlighter-rouge\">Sepal width</code></li>\n        <li>In <em>“Advanced Options”</em>:\n          <ul>\n            <li><em>“Data point options”</em>: <code class=\"language-plaintext highlighter-rouge\">User defined point options</code>\n              <ul>\n                <li><em>“relative size of points”</em>: <code class=\"language-plaintext highlighter-rouge\">2.0</code></li>\n              </ul>\n            </li>\n            <li><em>“Plotting multiple groups”</em>: <code class=\"language-plaintext highlighter-rouge\">Plot multiple groups of data on one plot</code>\n              <ul>\n                <li><em>“column differentiating the different groups”</em>: <code class=\"language-plaintext highlighter-rouge\">6</code></li>\n                <li><em>“Color schemes to differentiate your groups”</em>: <code class=\"language-plaintext highlighter-rouge\">Set 2 - predefined color pallete</code></li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n        <li>In <em>“Output options”</em>:\n          <ul>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“width of output”</em>: <code class=\"language-plaintext highlighter-rouge\">7.0</code></li>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“height of output”</em>: <code class=\"language-plaintext highlighter-rouge\">5.0</code></li>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“dpi of output”</em>: <code class=\"language-plaintext highlighter-rouge\">175.0</code></li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li><strong>View</strong> <i class=\"far fa-eye\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-eye</span> the resulting plot</li>\n    <li>Rename to <code class=\"language-plaintext highlighter-rouge\">k-means scatter plot</code></li>\n  </ol>\n</blockquote>\n\n<figure id=\"figure-9\" style=\"max-width: 90%;\"><img src=\"images/k_means_scatter.png\" alt=\"data. \" width=\"2100\" height=\"2100\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/k_means_scatter.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 9</strong>:</span> K-means clustering scatter plot</figcaption></figure>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <p>How to choose the right number of expected clusters (k)?</p>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <p>Major difficulty found with k-means is the choice of the number of clusters. Different methods are proposed to solve this problem.\nHere, we provide a simple solution. The idea is to compute k-means clustering using different values of clusters k. Next, the within sum of squares is drawn according to the number of clusters. The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.</p>\n    <figure id=\"figure-10\" style=\"max-width: 90%;\"><img src=\"images/number_of_clusters.png\" alt=\"data. \" width=\"1036\" height=\"576\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/number_of_clusters.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 10</strong>:</span> Optimal number of clusters</figcaption></figure>\n    <p>The plot above represents the variance within the clusters. It decreases as k increases, but it can be seen as a bend (or “elbow”) at k = 4. This bend indicates that\nadditional clusters beyond the fourth have little value.</p>\n  </blockquote>\n</blockquote>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <p>What are the differences between k-means and hierarchical clustering techniques</p>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <ol>\n      <li>\n        <p>Hierarchical clustering has difficulties in handling large data well but k-means clustering can. This is because the time complexity (of Lloyd’s variant) of k-means is linear (O(nkdi), n = number of data points, k = number clusters, d = data point dimensions and i = number of iterations) while the time-complexity of the optimal hierarchical clustering is quadratic (O(n2)).</p>\n      </li>\n      <li>\n        <p>K-means works well when the clusters are spherical (like circle in 2D, sphere in 3D) in shape. But, when the clusters are of arbitrary geometrical shapes, the performance suffers.</p>\n      </li>\n      <li>\n        <p>K-means clustering requires prior knowledge of the number of clusters. It does not learn the number of clusters from data. But, for the hierarchical clustering it is not necessary.</p>\n      </li>\n    </ol>\n\n  </blockquote>\n\n</blockquote>\n\n<h2 id=\"dbscan-clustering\">DBSCAN clustering</h2>\n\n<p>DBSCAN (Density-based spatial clustering of applications with noise) is a popular clustering algorithm and finds clusters as regions of high density followed by regions of low density. Clusters found by DBSCAN can be of any shape, as opposed to k-means which works well if the clusters are spherical in shape. The central component of the DBSCAN algorithm are the core samples which are present in the areas of high density. A cluster is, therefore, a set of core samples close to one other (measured by some distance measure) and a set of non-core samples that are close to core samples (but are not core samples themselves). There are two important parameters in DBSCAN algorithm - <code class=\"language-plaintext highlighter-rouge\">min_samples</code> is the number of samples in a neighborhood for a point to be considered as a core point and <code class=\"language-plaintext highlighter-rouge\">eps</code> is the maximum distance (between two samples) for a sample to be considered as in the neighborhood of the other. Higher the value of <code class=\"language-plaintext highlighter-rouge\">min_samples</code> or lower the value of eps indicate higher density necessary to form a cluster. DBSCAN does not require one to specify the number of clusters in the data a priori, as opposed to k-means.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>DBSCAN clustering</hands-on-title>\n\n  <ol>\n    <li><strong>Numeric Clustering</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following clustering parameters:\n      <ul>\n        <li><em>“Select the format of input data”</em>: <code class=\"language-plaintext highlighter-rouge\">Tabular Format (tabular,txt)</code>\n          <ul>\n            <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Data file with numeric values”</em>: <code class=\"language-plaintext highlighter-rouge\">iris</code></li>\n            <li><i class=\"far fa-check-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-check</span> <em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n            <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns EXCLUDING some by column header name(s)</code>\n              <ul>\n                <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“Type header name(s)”</em>: <code class=\"language-plaintext highlighter-rouge\">Species</code></li>\n              </ul>\n            </li>\n            <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Clustering Algorithm”</em>: <code class=\"language-plaintext highlighter-rouge\">DBSCAN</code></li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li>Rename the generated file to <code class=\"language-plaintext highlighter-rouge\">DBSCAN clustering</code></li>\n  </ol>\n</blockquote>\n\n<h3 id=\"visualise-dbscan-clustering\">Visualise DBSCAN clustering</h3>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Visualize DBSCAN clustering result</hands-on-title>\n\n  <ol>\n    <li><strong>Scatterplot with ggplot2</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following parameters:\n      <ul>\n        <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Input tabular dataset”</em>: <strong>DBSCAN clustering</strong></li>\n        <li><em>“Column to plot on x-axis”</em>: <code class=\"language-plaintext highlighter-rouge\">1</code></li>\n        <li><em>“Column to plot on y-axis”</em>: <code class=\"language-plaintext highlighter-rouge\">2</code></li>\n        <li><em>“Plot title”</em>: <code class=\"language-plaintext highlighter-rouge\">DBSCAN clustering in iris data</code></li>\n        <li><em>“Label for x axis”</em>: <code class=\"language-plaintext highlighter-rouge\">Sepal length</code></li>\n        <li><em>“Label for y axis”</em>: <code class=\"language-plaintext highlighter-rouge\">Sepal width</code></li>\n        <li>In <em>“Advanced Options”</em>:\n          <ul>\n            <li><em>“Data point options”</em>: <code class=\"language-plaintext highlighter-rouge\">User defined point options</code>\n              <ul>\n                <li><em>“relative size of points”</em>: <code class=\"language-plaintext highlighter-rouge\">2.0</code></li>\n              </ul>\n            </li>\n            <li><em>“Plotting multiple groups”</em>: <code class=\"language-plaintext highlighter-rouge\">Plot multiple groups of data on one plot</code>\n              <ul>\n                <li><em>“column differentiating the different groups”</em>: <code class=\"language-plaintext highlighter-rouge\">6</code></li>\n                <li><em>“Color schemes to differentiate your groups”</em>: <code class=\"language-plaintext highlighter-rouge\">Set 2 - predefined color pallete</code></li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n        <li>In <em>“Output options”</em>:\n          <ul>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“width of output”</em>: <code class=\"language-plaintext highlighter-rouge\">7.0</code></li>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“height of output”</em>: <code class=\"language-plaintext highlighter-rouge\">5.0</code></li>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“dpi of output”</em>: <code class=\"language-plaintext highlighter-rouge\">175.0</code></li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li><strong>View</strong> <i class=\"far fa-eye\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-eye</span> the resulting plot:</li>\n    <li>Rename to <code class=\"language-plaintext highlighter-rouge\">DBSCAN scatter plot</code></li>\n  </ol>\n</blockquote>\n\n<figure id=\"figure-11\" style=\"max-width: 90%;\"><img src=\"images/dbscan_scatter.png\" alt=\"data. \" width=\"2100\" height=\"2100\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/dbscan_scatter.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 11</strong>:</span> DBSCAN clustering scatter plot</figcaption></figure>\n\n<p>You will also notice that the green points (factor = -1) in the plot are not contained within any cluster. DBSCAN does not necessarily categorize every data point, and is therefore works very well with handling outliers in a dataset.</p>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <p>How can we evaluate the clustering results?</p>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <p>Clustering is an unsupervised learning algorithm; there are no labels or ground truth to compare with the clusters. However, we can still evaluate the performance of the algorithm using intrinsic measures.\nThere is a performance measure for clustering evaluation which is called the <a href=\"https://en.wikipedia.org/wiki/Silhouette_(clustering)\">silhouette coefficient</a>. It is a measure of the compactness and separation of the clusters.\nIt increases as the quality of the clusters increase; it is large for compact clusters that are far from each other and small for large, overlapping clusters. The silhouette coefficient is calculated per instance; for a set of instances, it is calculated as the mean of the individual sample score.</p>\n  </blockquote>\n</blockquote>\n\n<h1 id=\"applying-clustering-algorithms-on-multiple-datasets\">Applying clustering algorithms on multiple datasets</h1>\n\n<p>We can apply the same steps on the other datasets such <code class=\"language-plaintext highlighter-rouge\">moon</code> and <code class=\"language-plaintext highlighter-rouge\">circles</code> datasets (already imported) which are generated using <a href=\"https://scikit-learn.org/stable/datasets/index.html#generated-datasets\">scikit-learn</a> methods.</p>\n\n<h2 id=\"visualise-datasets\">Visualise datasets</h2>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Visualize scatter plot of data</hands-on-title>\n\n  <ol>\n    <li>\n      <p><strong>Scatterplot with ggplot2</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following parameters:</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-select-multiple-datasets\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-select-multiple-datasets\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Select multiple datasets</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ol>   <li>Click on <i class=\"far fa-copy\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-files</span> <strong>Multiple datasets</strong></li>   <li>Select several files by keeping the <kbd>Ctrl</kbd> (or <kbd>COMMAND</kbd>) key pressed and clicking on the files of interest</li> </ol> </blockquote>\n      <p><!--END_SNIPPET--></p>\n      <ul>\n        <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Input tabular dataset”</em>: <code class=\"language-plaintext highlighter-rouge\">circles</code> and <code class=\"language-plaintext highlighter-rouge\">moon</code> as <strong>multiple datasets</strong></li>\n        <li><em>“Column to plot on x-axis”</em>: <code class=\"language-plaintext highlighter-rouge\">1</code></li>\n        <li><em>“Column to plot on y-axis”</em>: <code class=\"language-plaintext highlighter-rouge\">2</code></li>\n        <li><em>“Plot title”</em>: <code class=\"language-plaintext highlighter-rouge\">Scatter Plot</code></li>\n        <li><em>“Label for x axis”</em>: <code class=\"language-plaintext highlighter-rouge\">X</code></li>\n        <li><em>“Label for y axis”</em>: <code class=\"language-plaintext highlighter-rouge\">Y</code></li>\n        <li>In <em>“Output options”</em>:\n          <ul>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“width of output”</em>: <code class=\"language-plaintext highlighter-rouge\">7.0</code></li>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“height of output”</em>: <code class=\"language-plaintext highlighter-rouge\">5.0</code></li>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“dpi of output”</em>: <code class=\"language-plaintext highlighter-rouge\">175.0</code></li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li>\n      <p><strong>View</strong> <i class=\"far fa-eye\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-eye</span> the resulting plots</p>\n    </li>\n  </ol>\n</blockquote>\n\n<figure id=\"figure-12\" style=\"max-width: 90%;\"><img src=\"images/circles_moon_scatter.png\" alt=\"data. \" width=\"1696\" height=\"726\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/circles_moon_scatter.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 12</strong>:</span> Scatter plot of circles and moon datasets</figcaption></figure>\n\n<h2 id=\"find-clusters\">Find clusters</h2>\n\n<p>Now you can find clusters in these datasets using the aforementioned algorithms.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Hierarchical clustering of circles and moon datasets</hands-on-title>\n\n  <ol>\n    <li>\n      <p><strong>Numeric Clustering</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following clustering parameters:</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-select-multiple-datasets-1\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-select-multiple-datasets-1\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Select multiple datasets</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ol>   <li>Click on <i class=\"far fa-copy\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-files</span> <strong>Multiple datasets</strong></li>   <li>Select several files by keeping the <kbd>Ctrl</kbd> (or <kbd>COMMAND</kbd>) key pressed and clicking on the files of interest</li> </ol> </blockquote>\n      <p><!--END_SNIPPET--></p>\n      <ul>\n        <li><em>“Select the format of input data”</em>: <code class=\"language-plaintext highlighter-rouge\">Tabular Format (tabular,txt)</code>\n          <ul>\n            <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Data file with numeric values”</em>: <code class=\"language-plaintext highlighter-rouge\">circles</code> and <code class=\"language-plaintext highlighter-rouge\">moon</code> as <strong>multiple datasets</strong></li>\n            <li><i class=\"far fa-check-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-check</span> <em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n            <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns</code></li>\n            <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Clustering Algorithm”</em>: <code class=\"language-plaintext highlighter-rouge\">Hierarchical Agglomerative Clustering</code></li>\n            <li>In <em>“Advanced option”</em>\n              <ul>\n                <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“Number of clusters”</em>: <code class=\"language-plaintext highlighter-rouge\">2</code></li>\n                <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Affinity”</em>: <code class=\"language-plaintext highlighter-rouge\">Euclidean</code></li>\n                <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Linkage”</em>: <code class=\"language-plaintext highlighter-rouge\">ward</code></li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li>\n      <p>Rename the generated files to <code class=\"language-plaintext highlighter-rouge\">circles hierarchical clustering</code> and <code class=\"language-plaintext highlighter-rouge\">moon hierarchical clustering</code> respectively</p>\n    </li>\n  </ol>\n</blockquote>\n\n<h2 id=\"visualise-clusters\">Visualise clusters</h2>\n\n<p>Then, you can visualize the clustering results using the following steps:</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Visualize hierarchical clustering result on circles and moon datasets.</hands-on-title>\n\n  <ol>\n    <li>\n      <p><strong>Scatterplot with ggplot2</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following parameters:</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-select-multiple-datasets-2\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-select-multiple-datasets-2\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Select multiple datasets</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ol>   <li>Click on <i class=\"far fa-copy\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-files</span> <strong>Multiple datasets</strong></li>   <li>Select several files by keeping the <kbd>Ctrl</kbd> (or <kbd>COMMAND</kbd>) key pressed and clicking on the files of interest</li> </ol> </blockquote>\n      <p><!--END_SNIPPET--></p>\n      <ul>\n        <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Input tabular dataset”</em>: <code class=\"language-plaintext highlighter-rouge\">circles hierarchical clustering</code> and <code class=\"language-plaintext highlighter-rouge\">moon hierarchical clustering</code> as <strong>multiple datasets</strong></li>\n        <li><em>“Column to plot on x-axis”</em>: <code class=\"language-plaintext highlighter-rouge\">1</code></li>\n        <li><em>“Column to plot on y-axis”</em>: <code class=\"language-plaintext highlighter-rouge\">2</code></li>\n        <li><em>“Plot title”</em>: <code class=\"language-plaintext highlighter-rouge\">Hierarchical clustering</code></li>\n        <li><em>“Label for x axis”</em>: <code class=\"language-plaintext highlighter-rouge\">X</code></li>\n        <li><em>“Label for y axis”</em>: <code class=\"language-plaintext highlighter-rouge\">Y</code></li>\n        <li>In <em>“Advanced Options”</em>:\n          <ul>\n            <li><em>“Data point options”</em>: <code class=\"language-plaintext highlighter-rouge\">User defined point options</code>\n              <ul>\n                <li><em>“relative size of points”</em>: <code class=\"language-plaintext highlighter-rouge\">2.0</code></li>\n              </ul>\n            </li>\n            <li><em>“Plotting multiple groups”</em>: <code class=\"language-plaintext highlighter-rouge\">Plot multiple groups of data on one plot</code>\n              <ul>\n                <li><em>“column differentiating the different groups”</em>: <code class=\"language-plaintext highlighter-rouge\">3</code></li>\n                <li><em>“Color schemes to differentiate your groups”</em>: <code class=\"language-plaintext highlighter-rouge\">Set 2 - predefined color pallete</code></li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n        <li>In <em>“Output options”</em>:\n          <ul>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“width of output”</em>: <code class=\"language-plaintext highlighter-rouge\">7.0</code></li>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“height of output”</em>: <code class=\"language-plaintext highlighter-rouge\">5.0</code></li>\n            <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“dpi of output”</em>: <code class=\"language-plaintext highlighter-rouge\">175.0</code></li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li><strong>View</strong> <i class=\"far fa-eye\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-eye</span> the resulting plots</li>\n    <li>Rename the generated files to <code class=\"language-plaintext highlighter-rouge\">Circles scatter plot</code> and <code class=\"language-plaintext highlighter-rouge\">Moon scatter plot</code> respectively</li>\n  </ol>\n</blockquote>\n\n<p>You can apply the other two algorithms (k-means and DBSCAN) to moon and circles datasets in the same way as explained above. In the k-means algorithm, please use <code class=\"language-plaintext highlighter-rouge\">k=2</code> and for the DBSCAN algorithm, the parameters should not be the default ones as used earlier. They should be set as follows: for the circles dataset (<code class=\"language-plaintext highlighter-rouge\">maximum neighborhood distance=0.2</code> and <code class=\"language-plaintext highlighter-rouge\">minimal core point density=5</code>) and for the moon dataset (<code class=\"language-plaintext highlighter-rouge\">maximum neighborhood distance=0.3</code> and <code class=\"language-plaintext highlighter-rouge\">minimal core point density=4</code>). You can see the scatter plots of the clustering results for all three clustering algorithms in Figure 13 and 14.</p>\n\n<figure id=\"figure-13\" style=\"max-width: 90%;\"><img src=\"images/circles_clustering.png\" alt=\"data. \" width=\"6300\" height=\"2100\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/circles_clustering.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 13</strong>:</span> Plot of clustering algorithms on circles dataset</figcaption></figure>\n\n<figure id=\"figure-14\" style=\"max-width: 90%;\"><img src=\"images/moon_clustering.png\" alt=\"data. \" width=\"6300\" height=\"2100\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/moon_clustering.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 14</strong>:</span> Plot of clustering algorithms on moon dataset</figcaption></figure>\n\n<h1 id=\"conclusion\">Conclusion</h1>\n\n<p>In this tutorial, we discussed 3 clustering algorithms which are used to discover structures or patterns in unlabeled data. You learned about the hierarchical, k-means and DBSCAN algorithms. By following steps specified for each clustering tool, we learned how to perform clustering and visualize results using clustering and plotting tools, respectively in Galaxy. There are many other clustering approaches which can be tried out on these datasets to find how they perform and how they compare to the 3 clustering algorithms explained in this tutorial. Different datasets can also be analysed using these algorithms. The clustering algorithms have some parameters which can be altered while performing the analyses to see if they affect the clustering or not. While using clustering algorithms, we need to take care of some important aspects like treating outliers in data and making sure each cluster has sufficient population. Some data pre-processors can also be used to clean the datasets.</p>\n"],"ref_slides":[],"hands_on":true,"slides":false,"mod_date":"2024-01-15 22:13:15 +0000","pub_date":"2020-05-08 17:04:18 +0000","version":29,"workflows":[{"workflow":"clustering.ga","tests":true,"url":"https://training.galaxyproject.org/training-material/topics/statistics/tutorials/clustering_machinelearning/workflows/clustering.ga","path":"topics/statistics/tutorials/clustering_machinelearning/workflows/clustering.ga","wfid":"statistics-clustering_machinelearning","wfname":"clustering","trs_endpoint":"https://training.galaxyproject.org/training-material/api/ga4gh/trs/v2/tools/statistics-clustering_machinelearning/versions/clustering","license":null,"creators":[],"name":"Clustering in Machine Learning","title":"Clustering in Machine Learning","test_results":null,"modified":"2024-06-15 00:05:39 +0000","mermaid":"flowchart TD\n  0[\"ℹ️ Input Dataset\\niris\"];\n  style 0 stroke:#2c3143,stroke-width:4px;\n  1[\"ℹ️ Input Dataset\\ncircles\"];\n  style 1 stroke:#2c3143,stroke-width:4px;\n  10[\"Numeric Clustering\"];\n  2 -->|output| 10;\n  11[\"Numeric Clustering\"];\n  2 -->|output| 11;\n  12[\"Numeric Clustering\"];\n  3 -->|tabular| 12;\n  13[\"Numeric Clustering\"];\n  3 -->|tabular| 13;\n  14[\"Numeric Clustering\"];\n  3 -->|tabular| 14;\n  15[\"Scatterplot with ggplot2\"];\n  5 -->|outfile| 15;\n  275528af-4af3-4dc6-a958-737f4d2f89bd[\"Output\\nheirarchical_clustering_circles\"];\n  15 --> 275528af-4af3-4dc6-a958-737f4d2f89bd;\n  style 275528af-4af3-4dc6-a958-737f4d2f89bd stroke:#2c3143,stroke-width:4px;\n  16[\"Scatterplot with ggplot2\"];\n  6 -->|outfile| 16;\n  870b03ca-6b2c-42ab-bbcc-6c4bbecf8ea9[\"Output\\nkmeans_clustering_circles\"];\n  16 --> 870b03ca-6b2c-42ab-bbcc-6c4bbecf8ea9;\n  style 870b03ca-6b2c-42ab-bbcc-6c4bbecf8ea9 stroke:#2c3143,stroke-width:4px;\n  17[\"Scatterplot with ggplot2\"];\n  7 -->|outfile| 17;\n  9664ba05-4664-4ceb-8f93-0974b20bf9e1[\"Output\\ndbscan_clustering_circles\"];\n  17 --> 9664ba05-4664-4ceb-8f93-0974b20bf9e1;\n  style 9664ba05-4664-4ceb-8f93-0974b20bf9e1 stroke:#2c3143,stroke-width:4px;\n  18[\"Scatterplot with ggplot2\"];\n  9 -->|outfile| 18;\n  d82a4483-7f38-4614-96b4-2a9b3316c808[\"Output\\nheirarchical_clustering_moon\"];\n  18 --> d82a4483-7f38-4614-96b4-2a9b3316c808;\n  style d82a4483-7f38-4614-96b4-2a9b3316c808 stroke:#2c3143,stroke-width:4px;\n  19[\"Scatterplot with ggplot2\"];\n  10 -->|outfile| 19;\n  00c0ca58-4ee5-4606-8c1b-c172431e5dbd[\"Output\\ndbscan_clustering_moon\"];\n  19 --> 00c0ca58-4ee5-4606-8c1b-c172431e5dbd;\n  style 00c0ca58-4ee5-4606-8c1b-c172431e5dbd stroke:#2c3143,stroke-width:4px;\n  2[\"ℹ️ Input Dataset\\nmoon\"];\n  style 2 stroke:#2c3143,stroke-width:4px;\n  20[\"Scatterplot with ggplot2\"];\n  11 -->|outfile| 20;\n  5717bf22-83e3-46a2-8f6d-12a1fa41439b[\"Output\\nkmeans_clustering_moon\"];\n  20 --> 5717bf22-83e3-46a2-8f6d-12a1fa41439b;\n  style 5717bf22-83e3-46a2-8f6d-12a1fa41439b stroke:#2c3143,stroke-width:4px;\n  21[\"Scatterplot with ggplot2\"];\n  12 -->|outfile| 21;\n  0ace8714-ff3a-4efd-a649-70924b1e6230[\"Output\\nheirarchical_clustering_iris\"];\n  21 --> 0ace8714-ff3a-4efd-a649-70924b1e6230;\n  style 0ace8714-ff3a-4efd-a649-70924b1e6230 stroke:#2c3143,stroke-width:4px;\n  22[\"Scatterplot with ggplot2\"];\n  13 -->|outfile| 22;\n  e6f69d7d-c9e3-4023-8daf-87f6b0afc3fd[\"Output\\nkmeans_clustering_iris\"];\n  22 --> e6f69d7d-c9e3-4023-8daf-87f6b0afc3fd;\n  style e6f69d7d-c9e3-4023-8daf-87f6b0afc3fd stroke:#2c3143,stroke-width:4px;\n  23[\"Scatterplot with ggplot2\"];\n  14 -->|outfile| 23;\n  119d1cde-567a-47f0-99de-00d3557c38a2[\"Output\\ndbscan_clustering_iris\"];\n  23 --> 119d1cde-567a-47f0-99de-00d3557c38a2;\n  style 119d1cde-567a-47f0-99de-00d3557c38a2 stroke:#2c3143,stroke-width:4px;\n  3[\"Convert CSV to tabular\"];\n  0 -->|output| 3;\n  4[\"Scatterplot with ggplot2\"];\n  1 -->|output| 4;\n  5[\"Numeric Clustering\"];\n  1 -->|output| 5;\n  6[\"Numeric Clustering\"];\n  1 -->|output| 6;\n  7[\"Numeric Clustering\"];\n  1 -->|output| 7;\n  8[\"Scatterplot with ggplot2\"];\n  2 -->|output| 8;\n  9[\"Numeric Clustering\"];\n  2 -->|output| 9;"}],"api":"https://training.galaxyproject.org/training-material/api/topics/statistics/tutorials/clustering_machinelearning/tutorial.json","tools":["csv_to_tabular","toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_numeric_clustering/sklearn_numeric_clustering/1.0.8.1","toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_point/ggplot2_point/2.2.1+galaxy2"],"supported_servers":{"exact":[{"url":"https://usegalaxy.be/","name":"UseGalaxy.be","usegalaxy":false},{"url":"https://usegalaxy.eu","name":"UseGalaxy.eu","usegalaxy":true},{"url":"https://usegalaxy.no/","name":"UseGalaxy.no","usegalaxy":false},{"url":"https://usegalaxy.org","name":"UseGalaxy.org (Main)","usegalaxy":true},{"url":"https://usegalaxy.org.au","name":"UseGalaxy.org.au","usegalaxy":true}],"inexact":[{"url":"https://usegalaxy.cz/","name":"UseGalaxy.cz","usegalaxy":false},{"url":"https://usegalaxy.fr/","name":"UseGalaxy.fr","usegalaxy":false}]},"topic_name_human":"Statistics and machine learning","admin_install":{"install_tool_dependencies":true,"install_repository_dependencies":true,"install_resolver_dependencies":true,"tools":[{"name":"sklearn_numeric_clustering","owner":"bgruening","revisions":"1dd433d2c92c","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"ggplot2_point","owner":"iuc","revisions":"e3a675da7fd0","tool_panel_section_label":"Graph/Display Data","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"}]},"admin_install_yaml":"---\ninstall_tool_dependencies: true\ninstall_repository_dependencies: true\ninstall_resolver_dependencies: true\ntools:\n- name: sklearn_numeric_clustering\n  owner: bgruening\n  revisions: 1dd433d2c92c\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: ggplot2_point\n  owner: iuc\n  revisions: e3a675da7fd0\n  tool_panel_section_label: Graph/Display Data\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n","tours":false,"video":false,"slides_recordings":false,"translations":{"tutorial":[],"slides":[],"video":false},"license":"CC-BY-4.0","type":"tutorial"}
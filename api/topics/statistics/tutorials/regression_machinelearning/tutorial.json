{"layout":"tutorial_hands_on","title":"Regression in Machine Learning","zenodo_link":"https://zenodo.org/record/2545213","questions":["How to use regression techniques to create predictive models from biological datasets?"],"objectives":["Learn regression background","Apply regression based machine learning algorithms","Learn ageing biomarkers and predict age from DNA methylation datasets","Learn how visualizations can be used to analyze predictions"],"key_points":["Using regression, real-valued targets are learned using the training set and predicted using the test set.","For each regression algorithm, its parameters should be optimized based on the dataset."],"time_estimation":"2H","contributors":[{"name":"Alireza Khanteymoori","email":"khanteymoori@gmail.com","orcid":"0000-0001-6811-9196","joined":"2019-07","former_affiliations":["uni-freiburg","elixir-europe"],"id":"khanteymoori","url":"https://training.galaxyproject.org/training-material/api/contributors/khanteymoori.json","page":"https://training.galaxyproject.org/training-material/hall-of-fame/khanteymoori/"},{"name":"Anup Kumar","email":"anup.rulez@gmail.com","twitter":"musafirtweetsz","joined":"2018-08","elixir_node":"de","affiliations":["uni-freiburg","eurosciencegateway","elixir-europe"],"id":"anuprulez","url":"https://training.galaxyproject.org/training-material/api/contributors/anuprulez.json","page":"https://training.galaxyproject.org/training-material/hall-of-fame/anuprulez/"},{"name":"Simon Bray","joined":"2019-05","elixir_node":"de","former_affiliations":["uni-freiburg","elixir-europe"],"id":"simonbray","url":"https://training.galaxyproject.org/training-material/api/contributors/simonbray.json","page":"https://training.galaxyproject.org/training-material/hall-of-fame/simonbray/"}],"recordings":[{"captioners":["anuprulez"],"date":"2021-02-15","galaxy_version":"21.01","length":"1H29M","youtube_id":"qxaWQjtEOzM","speakers":["anuprulez"]}],"js_requirements":{"mathjax":null,"mermaid":false},"short_id":"T00271","url":"/topics/statistics/tutorials/regression_machinelearning/tutorial.html","topic_name":"statistics","tutorial_name":"regression_machinelearning","dir":"topics/statistics/tutorials/regression_machinelearning","symlink":null,"id":"statistics/regression_machinelearning","ref_tutorials":["<p>In this tutorial you will learn how to use Galaxy tools to solve <a href=\"https://en.wikipedia.org/wiki/Regression_analysis\">regression</a> problems. First, we will introduce the concept of regression briefly, and then examine linear regression, which models the relationship between a target variable and some explanatory variables (also known as independent variables). Next, we will discuss gradient boosting regression, an more advanced regressor model which can model nonlinear relationships between variables. Then, we will show how to visualize the results in each step. Finally, we will discuss how to train our models by finding the values of their parameters that minimize a cost function. We will work through a real problem to learn how the models and learning algorithms work.</p>\n\n<p>In this tutorial we will build a regression model for chronological age prediction, based on DNA methylation. This is based on the work of <a href=\"https://www.sciencedirect.com/science/article/pii/S1872497317301643?via%3Dihub\">Jana Naue et al. 2017</a>, in which biomarkers are examined to predict the chronological age of humans by analyzing the DNA methylation patterns. Different machine learning algorithms are used in this study to make an age prediction.</p>\n\n<p>It has been recognized that within each individual, the level of <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3174260/\">DNA methylation</a> changes with age. This knowledge is used to select useful biomarkers from DNA methylation datasets. The <a href=\"https://en.wikipedia.org/wiki/CpG_site\">CpG sites</a> with the highest correlation to age are selected as the biomarkers (and therefore features for building a regression model). In this tutorial, specific biomarkers are analyzed by machine learning algorithms to create an age prediction model.</p>\n\n<blockquote class=\"agenda\">\n  <agenda-title></agenda-title>\n\n  <p>In this tutorial, we will cover:</p>\n\n<ol id=\"markdown-toc\">\n  <li><a href=\"#regression\" id=\"markdown-toc-regression\">Regression</a></li>\n  <li><a href=\"#analyze-dna-methylation-dataset\" id=\"markdown-toc-analyze-dna-methylation-dataset\">Analyze DNA methylation dataset</a>    <ol>\n      <li><a href=\"#get-training-and-test-datasets\" id=\"markdown-toc-get-training-and-test-datasets\">Get training and test datasets</a></li>\n      <li><a href=\"#learn-the-linear-model-from-training-dataset\" id=\"markdown-toc-learn-the-linear-model-from-training-dataset\">Learn the linear model from training dataset</a></li>\n      <li><a href=\"#predict-age-using-test-dataset\" id=\"markdown-toc-predict-age-using-test-dataset\">Predict age using test dataset</a></li>\n      <li><a href=\"#visualize-the-prediction\" id=\"markdown-toc-visualize-the-prediction\">Visualize the prediction</a></li>\n      <li><a href=\"#using-ensemble-methods-for-regression\" id=\"markdown-toc-using-ensemble-methods-for-regression\">Using ensemble methods for regression</a></li>\n      <li><a href=\"#create-data-processing-pipeline\" id=\"markdown-toc-create-data-processing-pipeline\">Create data processing pipeline</a></li>\n    </ol>\n  </li>\n  <li><a href=\"#conclusion\" id=\"markdown-toc-conclusion\">Conclusion</a></li>\n</ol>\n\n</blockquote>\n\n<h1 id=\"regression\">Regression</h1>\n\n<p>Regression analysis attempts to determine the relationship between one target variable and a series of independent variables. A regressor learns the mapping between the features of a dataset row (i.e. the values for each of the independent variables) and its target value. It tries to fit a curve for the targets, which can be linear or non-linear. When the targets in a dataset are real numbers, the machine learning task is known as regression and each sample in the dataset has a real-valued output or target. Figure <a href=\"#figure-1\">1</a> shows how a regression curve is fitted which best explains all these points. Here, the curve is a straight line, so this is an example of linear regression. The regression task is to learn this curve, which explains the underlying distribution of the data points. The curve can then be used to make predictions for new data points; the target for a new sample will lie on the curve learned by the regression task.</p>\n\n<figure id=\"figure-1\" style=\"max-width: 90%;\"><img src=\"images/regression.png\" alt=\"regression. \" width=\"353\" height=\"259\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/regression.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 1</strong>:</span> Regression fit through data points.</figcaption></figure>\n\n<p>Linear regression is a technique used to analyze a linear relationship between input variables (independent variables) and a single target variable. A linear relationship means that the data points tend to follow a straight line. Simple linear regression involves only a single input variable. Figure 2 shows a dataset with a linear relationship.</p>\n\n<figure id=\"figure-2\" style=\"max-width: 90%;\"><img src=\"images/linear_regression_generated_data.png\" alt=\"regression. \" width=\"397\" height=\"252\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/linear_regression_generated_data.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 2</strong>:</span> Data points with a linear relationship.</figcaption></figure>\n\n<p>In linear regression, our goal is to find the line that best models the path of the data points. Figure 3 shows the dataset we used in Figure 2 with a line of best fit through it. The position of the line is determined by certain fitting coefficients (in this simple case, the gradient and intercept) and linear regression helps us pick appropriate values for these coefficients. In this example we have only one input variable and the problem is therefore simple linear regression. Note that in real problems we have more than one input variable. In this case, we call it multiple linear regression. Adding extra input variables just means that we’ll need to find more weights.</p>\n\n<figure id=\"figure-3\" style=\"max-width: 90%;\"><img src=\"images/linear_regression_regressor.png\" alt=\"regression. \" width=\"397\" height=\"253\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/linear_regression_regressor.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 3</strong>:</span> Regression fit through data points.</figcaption></figure>\n\n<blockquote class=\"details\">\n  <details-title>Cost function</details-title>\n\n  <p>Once we have a prediction in regression problems, we need some way to tell if it’s reasonable or not. A cost function helps us do this. The cost function compares all the predictions against their actual values and provides a single number that we can use as a measure. Two common cost functions are the error and squared error. The error is how far our prediction is away from the actual value (Figure 4).</p>\n\n  <figure id=\"figure-4\" style=\"max-width: 90%;\"><img src=\"images/cost_function.png\" alt=\"regression. \" width=\"394\" height=\"253\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/cost_function.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 4</strong>:</span> Distance between true value and prediction.</figcaption></figure>\n  <p>We know an error above the actual value and an error below the actual value should be about as bad as each other. The squared error makes this clear because both positive and negative values of the error result in a positive squared error. We will use the Mean Squared Error (MSE) function as our cost function. This function finds the average squared error value for all of our data points. Cost functions are important to us because they measure how accurate our model is against the target values.</p>\n\n</blockquote>\n\n<h1 id=\"analyze-dna-methylation-dataset\">Analyze DNA methylation dataset</h1>\n\n<p>As a benchmark, we will use the <a href=\"https://www.sciencedirect.com/science/article/pii/S1872497317301643?via%3Dihub\">DNA methylation dataset</a> to predict the chronological age. One important reason to choose this dataset for an age prediction task is that DNA methylation changes with age and this change occurs at specific CpG sites in humans. \nIt has been recognized that DNA methylation analysis, which mostly occurs in a CpG sequence context, can give additional information besides the DNA profile.  It has been shown that DNA methylation changes with age within each individual. This alteration in DNA methylation occurs at specific CpG sites in all individuals, but with individual differences in “speed”, showing more DNA methylation differences in older twins compared to young ones.</p>\n\n<p>Epigenomic and phenotypic changes which are age-dependent are also contained in these cells. This knowledge is used to select useful biomarkers from the DNA methylation dataset. The CpG sites with the highest correlation to age are selected as biomarkers (features). In this study, specific biomarkers are analyzed by machine learning algorithms to create an age prediction model.</p>\n\n<p>In this tutorial, we will apply a couple of (<a href=\"https://scikit-learn.org/stable/\">scikit-learn</a>) machine learning tools to DNA methylation datasets to predict the chronological age of humans.</p>\n\n<h2 id=\"get-training-and-test-datasets\">Get training and test datasets</h2>\n\n<p>Whole blood samples are collected from humans with their ages falling in the range 18-69 and the best age-correlated CpG sites in the genome are chosen as features. The dataset is divided into two parts - training and test sets. The training set is used to train a regressor and the test set is used to evaluate the performance of the trained model. We proceed with the analysis by uploading new datasets and creating a new history.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Data upload</hands-on-title>\n\n  <ol>\n    <li>Create a new history for this tutorial</li>\n    <li>\n      <p>Import the files from <a href=\"https://zenodo.org/record/2545213\">Zenodo</a></p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>https://zenodo.org/record/2545213/files/train_rows.csv\nhttps://zenodo.org/record/2545213/files/test_rows_labels.csv\nhttps://zenodo.org/record/2545213/files/test_rows.csv\n</code></pre></div>      </div>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-importing-via-links\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-importing-via-links\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Importing via links</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Copy the link location</li>   <li>     <p>Click <i class=\"fas fa-upload\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-upload</span> <strong>Upload Data</strong> at the top of the tool panel</p>   </li>   <li>Select <i class=\"fa fa-edit\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-wf-edit</span> <strong>Paste/Fetch Data</strong></li>   <li>     <p>Paste the link(s) into the text field</p>   </li>   <li>     <p>Press <strong>Start</strong></p>   </li>   <li><strong>Close</strong> the window</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p>Rename the datasets as <code class=\"language-plaintext highlighter-rouge\">train_rows</code>, <code class=\"language-plaintext highlighter-rouge\">test_rows_labels</code> and <code class=\"language-plaintext highlighter-rouge\">test_rows</code> respectively.</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-renaming-a-dataset\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-renaming-a-dataset\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Renaming a dataset</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Click on the <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, change the <strong>Name</strong> field</li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p>Check that the datatype of all the three datasets is <code class=\"language-plaintext highlighter-rouge\">tabular</code>.</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-changing-the-datatype\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-changing-the-datatype\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Changing the datatype</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Click on the <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, click <i class=\"fas fa-database\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-chart-select-data</span> <strong>Datatypes</strong> tab on the top</li>   <li>In the <i class=\"fas fa-database\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-chart-select-data</span> <strong>Assign Datatype</strong>, select <code class=\"language-plaintext highlighter-rouge\">datatypes</code> from “<em>New type</em>” dropdown     <ul>       <li>Tip: you can start typing the datatype into the field to filter the dropdown menu</li>     </ul>   </li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n  </ol>\n\n</blockquote>\n\n<p class=\"comment\">The datasets from this study contain features (present as columns). The last column in the dataset refers to <code class=\"language-plaintext highlighter-rouge\">Age</code>, which is used as labels/targets. Since the targets are real numbers, the machine learning task becomes a regression problem. Using these features and targets, a model is built which learns a mapping between these input features and targets. The training set contains <code class=\"language-plaintext highlighter-rouge\">208</code> rows corresponding to individual samples and <code class=\"language-plaintext highlighter-rouge\">13</code> features (age-correlated CpG sites in DNA methylation dataset). The last column is <code class=\"language-plaintext highlighter-rouge\">Age</code>. The test set contains <code class=\"language-plaintext highlighter-rouge\">104</code> rows and the same number of features as the training set. The <code class=\"language-plaintext highlighter-rouge\">Age</code> column in the test set is predicted after training on the training set. Another dataset <code class=\"language-plaintext highlighter-rouge\">test_rows_labels</code> contains the true age values of the test set which is used to compute scores between true and predicted age.\nThe <code class=\"language-plaintext highlighter-rouge\">train_rows</code> contains a column <code class=\"language-plaintext highlighter-rouge\">Age</code> which is the label or target. We will evaluate our model on <code class=\"language-plaintext highlighter-rouge\">test_rows</code> and compare the predicted age with the true age in <code class=\"language-plaintext highlighter-rouge\">test_rows_labels</code></p>\n\n<blockquote class=\"details\">\n  <details-title>Test-train data splitting data within Galaxy</details-title>\n  <p>For this task, test and training datasets are provided. However, if you want to analyze your own data, you can check out the <strong>Split Dataset</strong> tool, which can split a dataset into training and test subsets with a number of advanced options.</p>\n\n</blockquote>\n\n<h2 id=\"learn-the-linear-model-from-training-dataset\">Learn the linear model from training dataset</h2>\n\n<p>At the first step, to learn the mapping between several features and the targets, we will apply a linear regressor. It learns features from the training dataset and maps all the rows to their respective targets. The process of mapping gives a trained model.</p>\n\n<p>The dataset is divided into two parts - training and test sets. The training set is used to train a regressor and the test set is used to evaluate the performance of the trained model.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Train a model</hands-on-title>\n\n  <ol>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_generalized_linear/sklearn_generalized_linear/1.0.11.0\" title=\"Generalized linear models for classification and regression tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Generalized linear models for classification and regression</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.11.0)</span>:\n      <ul>\n        <li><em>“Select a Classification Task”</em>: <code class=\"language-plaintext highlighter-rouge\">Train a model</code>\n          <ul>\n            <li><em>“Select a linear method”</em>: <code class=\"language-plaintext highlighter-rouge\">Linear Regression model</code>\n              <ul>\n                <li><em>“Select input type”</em>: <code class=\"language-plaintext highlighter-rouge\">tabular data</code>\n                  <ul>\n                    <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Training samples dataset”</em>: <code class=\"language-plaintext highlighter-rouge\">train_rows</code></li>\n                    <li><i class=\"far fa-check-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-check</span> <em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n                    <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns EXCLUDING some by column header name(s)</code>\n                      <ul>\n                        <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“Type header name(s)”</em>: <code class=\"language-plaintext highlighter-rouge\">Age</code></li>\n                      </ul>\n                    </li>\n                    <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Dataset containing class labels or target values”</em>: <code class=\"language-plaintext highlighter-rouge\">train_rows</code></li>\n                    <li><i class=\"far fa-check-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-check</span> <em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n                    <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">Select columns by column header name(s)</code>\n                      <ul>\n                        <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“Type header name(s)”</em>: <code class=\"language-plaintext highlighter-rouge\">Age</code></li>\n                      </ul>\n                    </li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li>Rename the generated file to <code class=\"language-plaintext highlighter-rouge\">LinearRegression_model</code></li>\n  </ol>\n</blockquote>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <p>What is learned by the linear regressor?</p>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <p>The linear regressor learns the coefficients of the best fit line to the data.</p>\n\n  </blockquote>\n\n</blockquote>\n\n<h2 id=\"predict-age-using-test-dataset\">Predict age using test dataset</h2>\n\n<p>After learning on the training dataset, we should evaluate the performance on the test dataset to know whether the model created by the learning algorithm from the training dataset is good or not. This model is used to predict a new sample and a similar accuracy is expected.</p>\n\n<p>Now, we will predict age in the test dataset using this model in order to see if the model has learned important features which can be generalized on a new dataset. The test dataset (<code class=\"language-plaintext highlighter-rouge\">test_rows</code>) contains the same number of features but does not contain the <code class=\"language-plaintext highlighter-rouge\">Age</code> column; the age will instead be predicted using the trained model.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Predict targets using the model</hands-on-title>\n\n  <ol>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_generalized_linear/sklearn_generalized_linear/1.0.11.0\" title=\"Generalized linear models for classification and regression tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Generalized linear models for classification and regression</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.11.0)</span>:\n      <ul>\n        <li><em>“Select a Classification Task”</em>: <code class=\"language-plaintext highlighter-rouge\">Load a model and predict</code>\n          <ul>\n            <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Models”</em>: <code class=\"language-plaintext highlighter-rouge\">LinearRegression_model</code></li>\n            <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Data (tabular)”</em>: <code class=\"language-plaintext highlighter-rouge\">test_rows</code></li>\n            <li><i class=\"far fa-check-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-check</span> <em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n            <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Select the type of prediction”</em>: <code class=\"language-plaintext highlighter-rouge\">Predict class labels</code></li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li>Rename the generated file to <code class=\"language-plaintext highlighter-rouge\">predicted_data_linear</code></li>\n  </ol>\n</blockquote>\n\n<h2 id=\"visualize-the-prediction\">Visualize the prediction</h2>\n\n<p>We will evaluate the predictions by comparing them to the expected targets. In the previous step, we generated predictions for the test dataset (<code class=\"language-plaintext highlighter-rouge\">predicted_data_linear</code>). We have one more dataset (<code class=\"language-plaintext highlighter-rouge\">test_rows_labels</code>) which contains the true age values of the test set. Using the true and predicted values of age in the test set, we will verify the performance by analyzing the plots. As you can see, <code class=\"language-plaintext highlighter-rouge\">predicted_data_linear</code> has no header, so first we need to remove the header from <code class=\"language-plaintext highlighter-rouge\">test_rows_labels</code> to perform the comparison.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Remove the header</hands-on-title>\n\n  <ol>\n    <li><strong>Remove beginning of a file</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following parameters:\n      <ul>\n        <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Remove first”</em>: <code class=\"language-plaintext highlighter-rouge\">1</code></li>\n        <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“from”</em>: <code class=\"language-plaintext highlighter-rouge\">test_rows_labels</code></li>\n      </ul>\n    </li>\n    <li>Rename the generated file to <code class=\"language-plaintext highlighter-rouge\">test_rows_labels_without_header</code></li>\n  </ol>\n</blockquote>\n\n<p>Now we visualize and analyze the predictions using the <strong>Plot actual vs predicted curves and residual plots</strong> tool in Galaxy.\nHint: Please find the above tool in “Graph/Display data” tool section in Galaxy.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Check and visualize the predictions</hands-on-title>\n  <p><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/plotly_regression_performance_plots/plotly_regression_performance_plots/0.1\" title=\"Plot actual vs predicted curves and residual plots tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Plot actual vs predicted curves and residual plots</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 0.1)</span>:</p>\n  <ul>\n    <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Select input data file”</em>: <code class=\"language-plaintext highlighter-rouge\">test_rows_labels_without_header</code></li>\n    <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Select predicted data file”</em>: <code class=\"language-plaintext highlighter-rouge\">predicted_data_linear</code></li>\n  </ul>\n</blockquote>\n\n<p>The visualization tool creates the following plots:</p>\n\n<ol>\n  <li>\n    <p>True vs. predicted targets curves: In this plot the corresponding points in both these curves should be close to each other for a good regression performance. We can see that the plot shows this behaviour.</p>\n\n    <figure id=\"figure-5\" style=\"max-width: 90%;\"><img src=\"images/true_pred_curves.png\" alt=\"true_predicted. \" width=\"700\" height=\"450\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/true_pred_curves.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 5</strong>:</span> True vs predicted targets curves.</figcaption></figure>\n  </li>\n  <li>\n    <p><a href=\"https://towardsdatascience.com/everything-you-need-to-know-about-scatter-plots-for-data-visualisation-924144c0bc5\">Scatter plot</a> for true vs. predicted targets: We can see in the scatter plot (figure <a href=\"#figure-6\">6</a>) that most of the points lie along the x=y curve. It means that the true and predicted ages are close to each other. The root mean square error (<code class=\"language-plaintext highlighter-rouge\">RMSE</code>) is <code class=\"language-plaintext highlighter-rouge\">4.1</code> and the R2 score is <code class=\"language-plaintext highlighter-rouge\">0.93</code>.</p>\n\n    <figure id=\"figure-6\" style=\"max-width: 90%;\"><img src=\"images/true_vs_pred_scatter.png\" alt=\"scatter_plot. \" width=\"700\" height=\"450\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/true_vs_pred_scatter.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 6</strong>:</span> Scatter plot for true vs. predicted targets.</figcaption></figure>\n  </li>\n  <li>\n    <p><a href=\"http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/\">Residual plot</a> between residual (predicted - true) and predicted targets: The residual plot shown in figure <a href=\"#figure-7\">7</a> is generated to see if there is any visible pattern between residual (predicted age - true age) and predicted age. For a good regression performance, this plot should exhibit a random pattern and the points should be symmetrically distributed along the y=0 line.</p>\n\n    <figure id=\"figure-7\" style=\"max-width: 90%;\"><img src=\"images/residual_plot.png\" alt=\"residual_plot. \" width=\"700\" height=\"450\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/residual_plot.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 7</strong>:</span> Residual plot between residual (predicted - true) and predicted targets. The plot shows a random pattern of points.</figcaption></figure>\n  </li>\n</ol>\n\n<p>These plots are important to visualize the quality of regression and the true and predicted targets - how close or far they are from each other. The closer they are, the better the prediction.</p>\n\n<blockquote class=\"details\">\n  <details-title>R2 (coefficient of determination)</details-title>\n  <p>In both the parts, learning on datasets is done using cross-validation and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\">R2</a> scoring metric is used to evaluate the performance of the trained model. The closer it is to 1.0, the better it is. If it is negative, then the trained model is not good. To infer how its values exhibit model performance, we can compare the figures <a href=\"#figure-8\">8</a> and <a href=\"#figure-9\">9</a>. In both the plots, the true and predicted targets are plotted in a scatter plot. For a good model, most of the points should lie along the <code class=\"language-plaintext highlighter-rouge\">x = y</code> line as the true and predicted targets are close to each other. In figure <a href=\"#figure-8\">8</a>, we can see that the points are scattered and do not show any pattern. Therefore, the R2 score is <code class=\"language-plaintext highlighter-rouge\">-0.06</code>. On the other hand, figure <a href=\"#figure-9\">9</a> shows a better pattern as most of the points lie along the line and the R2 score is almost <code class=\"language-plaintext highlighter-rouge\">1.0</code>. For RNA-seq dataset, we will compute the cross-validated R2 score using the training set and for DNA methylation dataset, we will compute the R2 score for the test set.</p>\n\n  <figure id=\"figure-8\" style=\"max-width: 90%;\"><img src=\"../../images/age-prediction-with-ml/model_bad.png\" alt=\"model_bad. \" width=\"800\" height=\"500\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/age-prediction-with-ml/model_bad.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 8</strong>:</span> This shows an example of a bad model as most of the points are scattered.</figcaption></figure>\n\n  <figure id=\"figure-9\" style=\"max-width: 90%;\"><img src=\"../../images/age-prediction-with-ml/model_good.png\" alt=\"model_good. \" width=\"800\" height=\"500\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/age-prediction-with-ml/model_good.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 9</strong>:</span> This shows an example of a good model as most of the points lie along the x = y line.</figcaption></figure>\n\n</blockquote>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <p>Inspect the plots. What can you say about the predictions?</p>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <p>Figures 5, 6 and 7 show that the prediction is acceptable and the predicted age lies about close to the true age, but the reults can be improved by using better algorithms such as ensemble-based regressors.</p>\n\n  </blockquote>\n</blockquote>\n\n<h2 id=\"using-ensemble-methods-for-regression\">Using ensemble methods for regression</h2>\n\n<p>Linear regression is a useful technique but isn’t always the right choice for our real problems. Linear regression is a good choice when there is a linear relationship between your independent and dependent variables and you are trying to predict continuous values.</p>\n\n<p>It is not a good choice when the relationship between independent and dependent variables is more complicated. For example, Figure 10 shows a dataset that does not have a linear relationship so linear regression would not be a good choice.</p>\n\n<figure id=\"figure-10\" style=\"max-width: 90%;\"><img src=\"images/nonlinear_regression_generated_data.png\" alt=\"regression. \" width=\"393\" height=\"252\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/nonlinear_regression_generated_data.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 10</strong>:</span> Data points with nonlinear relationship.</figcaption></figure>\n\n<p>To learn the mapping between several features and the targets, in the next step, we will apply the\n<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor\">Gradient boosting regressor</a>. It is an ensemble-based regressor because its prediction is the collective performance of multiple weak learners (e.g. decision trees). It learns features from training dataset and maps all the rows to their respective targets (real numbers). The process of mapping gives a trained model.</p>\n\n<blockquote class=\"comment\">\n  <comment-title></comment-title>\n  <p>An <a href=\"https://scikit-learn.org/stable/modules/ensemble.html#ensemble\"><em>ensemble</em></a> method uses multiple learning models internally for better predictions and boosting is a method of converting weak learners into strong learners.</p>\n</blockquote>\n\n<p><a href=\"https://www.sciencedirect.com/science/article/pii/S1872497317301643?via%3Dihub\">Jana Naue et al. (2017)</a> used <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\">Random Forest</a> (another ensemble-based regressor) as regressor and we can conclude from this study that an ensemble-based regressor works well on this DNA methylation dataset. Therefore, we will use gradient boosting to build a prediction model.</p>\n\n<p>Like the random forest method, gradient boosting is an ensemble-based regressor, because it uses multiple decision tree regressors internally and makes predictions by taking the collective performances of predictions made by multiple decision trees. It has a good predictive power and is robust to outliers. It creates an ensemble of weak learners (decision trees) and iteratively minimizes errors. One disadvantage, which comes from its basic principle of boosting, is that it cannot be parallelized. <em>Hint:</em> Please find the following tool in “Machine learning” tool section in Galaxy.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Train a model</hands-on-title>\n\n  <ol>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_ensemble/sklearn_ensemble/1.0.11.0\" title=\"Ensemble methods for classification and regression tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Ensemble methods for classification and regression</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.11.0)</span>:\n      <ul>\n        <li><em>“Select a Classification Task”</em>: <code class=\"language-plaintext highlighter-rouge\">Train a model</code>\n          <ul>\n            <li><em>“Select an ensemble method”</em>: <code class=\"language-plaintext highlighter-rouge\">Gradient Boosting Regressor</code> (<em>Note:</em> choose <code class=\"language-plaintext highlighter-rouge\">Gradient Boosting Regressor</code> not <code class=\"language-plaintext highlighter-rouge\">Gradient Boosting Classifier</code>)\n              <ul>\n                <li><em>“Select input type”</em>: <code class=\"language-plaintext highlighter-rouge\">tabular data</code>\n                  <ul>\n                    <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Training samples dataset”</em>: <code class=\"language-plaintext highlighter-rouge\">train_rows</code></li>\n                    <li><i class=\"far fa-check-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-check</span> <em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n                    <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns EXCLUDING some by column header name(s)</code>\n                      <ul>\n                        <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“Type header name(s)”</em>: <code class=\"language-plaintext highlighter-rouge\">Age</code></li>\n                      </ul>\n                    </li>\n                    <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Dataset containing class labels or target values”</em>: <code class=\"language-plaintext highlighter-rouge\">train_rows</code></li>\n                    <li><i class=\"far fa-check-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-check</span> <em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n                    <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">Select columns by column header name(s)</code>\n                      <ul>\n                        <li><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-text</span> <em>“Type header name(s)”</em>: <code class=\"language-plaintext highlighter-rouge\">Age</code></li>\n                      </ul>\n                    </li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li>Rename the generated file to <code class=\"language-plaintext highlighter-rouge\">gradient_boosting_model</code></li>\n  </ol>\n</blockquote>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <p>What is learned by the gradient boosting regressor?</p>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <p><strong>Gradient boosting</strong> regressor learns multiple attributes like <strong>feature_importances_</strong> (weight for each feature/column),\n<strong>oob_improvement_</strong> (which stores incremental improvements in learning), <strong>estimators_</strong> (collection of weak learners) and a few more.\nThese attributes are used to predict the target for a new sample and are stored in the trained model. They can be accessed by using the <strong>Estimator attributes</strong> tool on the <code class=\"language-plaintext highlighter-rouge\">gradient_boosting_model</code> dataset.</p>\n\n  </blockquote>\n\n</blockquote>\n\n<p>After learning on the training dataset, we should evaluate the performance on the test dataset.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Predict targets using the model</hands-on-title>\n\n  <ol>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_ensemble/sklearn_ensemble/1.0.11.0\" title=\"Ensemble methods for classification and regression tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Ensemble methods for classification and regression</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.11.0)</span>:\n      <ul>\n        <li><em>“Select a Classification Task”</em>: <code class=\"language-plaintext highlighter-rouge\">Load a model and predict</code>\n          <ul>\n            <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Models”</em>: <code class=\"language-plaintext highlighter-rouge\">gradient_boosting_model</code></li>\n            <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Data (tabular)”</em>: <code class=\"language-plaintext highlighter-rouge\">test_rows</code></li>\n            <li><i class=\"far fa-check-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-check</span> <em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n            <li><i class=\"fas fa-filter\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-select</span> <em>“Select the type of prediction”</em>: <code class=\"language-plaintext highlighter-rouge\">Predict class labels</code></li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li>Rename the generated file to <code class=\"language-plaintext highlighter-rouge\">predicted_data_gradient_boosting</code></li>\n  </ol>\n</blockquote>\n\n<p>Now we can visualize and analyze the predictions using the <strong>Plot actual vs predicted curves and residual plots</strong> tool.</p>\n<blockquote class=\"hands_on\">\n  <hands-on-title>Check and visualize the predictions</hands-on-title>\n  <p><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/plotly_regression_performance_plots/plotly_regression_performance_plots/0.1\" title=\"Plot actual vs predicted curves and residual plots tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Plot actual vs predicted curves and residual plots</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 0.1)</span>:</p>\n  <ul>\n    <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Select input data file”</em>: <code class=\"language-plaintext highlighter-rouge\">test_rows_labels</code> (<em>Note:</em> use the <code class=\"language-plaintext highlighter-rouge\">test_rows_labels</code> dataset, not the <code class=\"language-plaintext highlighter-rouge\">test_rows_labels_without_header</code> one)</li>\n    <li><i class=\"far fa-file\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-file</span> <em>“Select predicted data file”</em>: <code class=\"language-plaintext highlighter-rouge\">predicted_data_gradient_boosting</code></li>\n  </ul>\n</blockquote>\n\n<figure id=\"figure-11\" style=\"max-width: 90%;\"><img src=\"images/true_vs_pred_scatter_ensemble.png\" alt=\"scatter_plot. \" width=\"700\" height=\"450\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/true_vs_pred_scatter_ensemble.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 11</strong>:</span> Scatter plot for true vs. predicted targets using gradient boosting regressor.</figcaption></figure>\n\n<p>R2 score is 0.93, the same as the linear model, but the RMSE is smaller (3.85), which tells us that the predicted targets are closer to the true targets.</p>\n\n<h2 id=\"create-data-processing-pipeline\">Create data processing pipeline</h2>\n\n<p>In the final step, we will create a pipeline learner with the <strong>Pipeline builder</strong> tool but this time, we just specify the regressor. The <strong>Pipeline builder</strong> tool will wrap this regressor and return a zip file. By choosing <code class=\"language-plaintext highlighter-rouge\">Yes</code> from the boolean option, tunable hyperparameters will be output in a separate file.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Create pipeline</hands-on-title>\n\n  <p><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_build_pipeline/sklearn_build_pipeline/1.0.11.0\" title=\"Pipeline builder tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Pipeline builder</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.11.0)</span>:</p>\n  <ul>\n    <li>In <em>“Final Estimator”</em>:\n      <ul>\n        <li><em>“Choose the module that contains target estimator”</em>: <code class=\"language-plaintext highlighter-rouge\">sklearn.ensemble</code>\n          <ul>\n            <li><em>“Choose estimator class”</em>: <code class=\"language-plaintext highlighter-rouge\">GradientBoostingRegressor</code></li>\n            <li><em>“Type in parameter settings if different from default”</em>: <code class=\"language-plaintext highlighter-rouge\">random_state=42</code></li>\n          </ul>\n        </li>\n        <li>In <em>“Output parameters for searchCV?”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n      </ul>\n\n      <p><code class=\"language-plaintext highlighter-rouge\">random_state</code> could be set to any arbitrary integer number; its purpose is to ensure a determistic and therefore reproducible result.</p>\n    </li>\n  </ul>\n\n</blockquote>\n\n<h3 id=\"search-for-the-best-values-of-hyperparameters\">Search for the best values of hyperparameters</h3>\n\n<p>After the <strong>New Pipeline/Estimator</strong> dataset and its tunable hyperparameters are produced by the <strong>Pipeline builder</strong> tool, we will use the <strong>Hyperparameter search</strong> tool to find the best values for each hyperparameter. These values will lead us to create the best model based on the search space chosen for each hyperparameter. We use only one parameter <code class=\"language-plaintext highlighter-rouge\">n_estimators</code> of <code class=\"language-plaintext highlighter-rouge\">Gradient boosting</code> regressor for this task. This parameter specifies the number of boosting stages the learning process has to go through. The default value of <code class=\"language-plaintext highlighter-rouge\">n_estimators</code> for this regressor is <code class=\"language-plaintext highlighter-rouge\">100</code>. However, we are not sure if this gives the best accuracy. Therefore, it is important to try setting this parameter to different values to find the optimal one. We choose some values which are less than <code class=\"language-plaintext highlighter-rouge\">100</code> and a few more than <code class=\"language-plaintext highlighter-rouge\">100</code>. The hyperparameter search will look for the optimal number of estimators and gives the best-trained model as one of the outputs. This model is used in the next step to predict age in the test dataset.</p>\n\n<blockquote class=\"details\">\n  <details-title>5-fold cross-validation</details-title>\n\n  <p>This is a model validation technique which estimates the performance of a predictive model on an unseen dataset. A dataset is divided into <code class=\"language-plaintext highlighter-rouge\">5</code> folds and these folds are categorized into training and validation sets. The idea of cross-validation is shown in figure <a href=\"#figure-12\">12</a>. The complete dataset is divided into <code class=\"language-plaintext highlighter-rouge\">5</code> equal parts. 4 out of the 5 parts are used for training and the remaining 1 part is used for validating the performance of training. This is done for <code class=\"language-plaintext highlighter-rouge\">5</code> folds/iterations; each time the validation set (1/5 of the dataset) is different. In all five folds, the complete dataset is used for training and validation. The final validation performance is averaged over <code class=\"language-plaintext highlighter-rouge\">5</code> folds.</p>\n\n  <figure id=\"figure-12\" style=\"max-width: 90%;\"><img src=\"../../images/age-prediction-with-ml/5fold_cv.png\" alt=\"5fold_cv. \" width=\"622\" height=\"347\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/age-prediction-with-ml/5fold_cv.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 12</strong>:</span> 5-fold cross validation.</figcaption></figure>\n  <p>The image demonstrates how the 5-fold cross-validation works. The complete dataset is divided into 5 equal parts/folds. 4 parts (80%) of the data (training set shown in yellow) are used for training the model and the remaining one part is used for evaluating (validation set shown in blue) the trained model. This is repeated for 5 times till every part/fold is used as the validation set. The accuracies computed for different validation folds are averaged to give 5-fold cross-validation accuracy.</p>\n\n</blockquote>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Hyperparameter search</hands-on-title>\n\n  <p><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_searchcv/sklearn_searchcv/1.0.11.0\" title=\"Hyperparameter search tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Hyperparameter search</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.11.0)</span>:</p>\n  <ul>\n    <li><em>“Select a model selection search scheme”</em>: <code class=\"language-plaintext highlighter-rouge\">GridSearchCV - Exhaustive search over specified parameter values for an estimator </code>\n      <ul>\n        <li><i class=\"far fa-copy\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-files</span> <em>“Choose the dataset containing pipeline/estimator object”</em>: <code class=\"language-plaintext highlighter-rouge\">h5mlm</code> file (output of <strong>Pipeline builder</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span>)</li>\n        <li><em>“Is the estimator a deep learning model?”</em>: <code class=\"language-plaintext highlighter-rouge\">No</code></li>\n        <li>In <em>“Search parameters Builder”</em>:\n          <ul>\n            <li>In <em>“Parameter settings for search”</em>:\n              <ul>\n                <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“1: Parameter settings for search”</em>\n                  <ul>\n                    <li><em>“Choose a parameter name (with current value)”</em>: <code class=\"language-plaintext highlighter-rouge\">n_estimators: 100</code></li>\n                    <li><em>“Search list”</em>: <code class=\"language-plaintext highlighter-rouge\">[25, 50, 75, 100, 200]</code></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n        <li>In <em>“Advanced Options for SearchCV”</em>:\n          <ul>\n            <li>\n              <p><em>“Select the primary metric (scoring)”</em>: <code class=\"language-plaintext highlighter-rouge\">Regression -- 'r2'</code></p>\n\n              <p>A scoring metric can be set. In this tutorial, we use <code class=\"language-plaintext highlighter-rouge\">Regression -- 'r2'</code></p>\n            </li>\n            <li>\n              <p><em>“Select the cv splitter”</em>: <code class=\"language-plaintext highlighter-rouge\">KFold</code></p>\n\n              <p>There are different ways to split the dataset into training and validation sets. In our tutorial, we will use <code class=\"language-plaintext highlighter-rouge\">KFold</code> which splits the dataset into <code class=\"language-plaintext highlighter-rouge\">K</code> consecutive parts. It is used for cross-validation. It is set to <code class=\"language-plaintext highlighter-rouge\">5</code> using another parameter <code class=\"language-plaintext highlighter-rouge\">n_splits</code>.</p>\n\n              <ul>\n                <li><em>“n_splits”</em>: <code class=\"language-plaintext highlighter-rouge\">5</code></li>\n                <li><em>“Whether to shuffle data before splitting”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n                <li>\n                  <p><em>“Random seed number”</em>: <code class=\"language-plaintext highlighter-rouge\">3111696</code></p>\n\n                  <p>It is set to an integer and used to retain the randomness/accuracy when <em>“Whether to shuffle data before splitting”</em> is <code class=\"language-plaintext highlighter-rouge\">Yes</code> across successive experiments.</p>\n                </li>\n              </ul>\n            </li>\n            <li>\n              <p><em>“Raise fit error”</em>: <code class=\"language-plaintext highlighter-rouge\">No</code></p>\n\n              <p>While setting different values for a parameter during hyperparameter search, it can happen that wrong values are set, which may generate an error. To avoid stopping the execution of a regressor, it is set to <code class=\"language-plaintext highlighter-rouge\">No</code> which means even if a wrong parameter value is encountered, the regressor does not stop running and simply skips that value.</p>\n            </li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li><em>“Select input type”</em>: <code class=\"language-plaintext highlighter-rouge\">tabular data</code>\n      <ul>\n        <li><i class=\"far fa-copy\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-files</span> <em>“Training samples dataset”</em>: <code class=\"language-plaintext highlighter-rouge\">train_rows</code> tabular file</li>\n        <li><em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n        <li><em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns EXCLUDING some by column header name(s)</code>\n          <ul>\n            <li><em>“Type header name(s)”</em>: <code class=\"language-plaintext highlighter-rouge\">Age</code></li>\n          </ul>\n        </li>\n        <li><i class=\"far fa-copy\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-files</span> <em>“Dataset containing class labels or target values”</em>: <code class=\"language-plaintext highlighter-rouge\">train_rows</code> tabular file</li>\n        <li><em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n        <li><em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">Select columns by column header name(s)</code>\n          <ul>\n            <li><em>“Type header name(s)”</em>: <code class=\"language-plaintext highlighter-rouge\">Age</code></li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n    <li><em>“Whether to hold a portion of samples for test exclusively?”</em>: <code class=\"language-plaintext highlighter-rouge\">Nope</code></li>\n    <li><em>“Save best estimator?”</em>: <code class=\"language-plaintext highlighter-rouge\">Fitted best estimator or Detailed cv_results_ from nested CV</code></li>\n  </ul>\n\n</blockquote>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <p>What is the optimal number of estimators for the given dataset?</p>\n\n  <p>Hint: Please look at the <code class=\"language-plaintext highlighter-rouge\">mean_test_score</code> column in the tabular result from the <strong>Hyperparameter search</strong> tool.</p>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <ol>\n      <li>(Even though the default value of the number of estimators for the gradient boosting regressor is <code class=\"language-plaintext highlighter-rouge\">100</code>, <code class=\"language-plaintext highlighter-rouge\">75</code> gives the best accuracy. That’s why it is important to perform hyperparameter search to tune these parameters for any dataset). 50 estimators also give almost the same accuracy.</li>\n    </ol>\n\n  </blockquote>\n\n</blockquote>\n\n<p>Using the <strong>Hyperparameter search</strong> tool, we optimized our model, based on the training data. Now, we will predict the age from the test dataset using this model.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Predict age</hands-on-title>\n\n  <ol>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_ensemble/sklearn_ensemble/1.0.11.0\" title=\"Ensemble methods for classification and regression tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Ensemble methods for classification and regression</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.11.0)</span>:\n      <ul>\n        <li><em>“Select a Classification Task”</em>: <code class=\"language-plaintext highlighter-rouge\">Load a model and predict</code>\n          <ul>\n            <li><i class=\"far fa-copy\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-files</span> <em>“Models”</em>: <code class=\"language-plaintext highlighter-rouge\">h5mlm</code> file (output of <strong>Hyperparameter search</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span>)</li>\n            <li><i class=\"far fa-copy\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-files</span> <em>“Data (tabular)”</em>: <code class=\"language-plaintext highlighter-rouge\">test_rows</code> tabular file</li>\n            <li><em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n  </ol>\n\n</blockquote>\n\n<p>Now we will verify the performance by creating and analyzing the plots.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Create regression plots</hands-on-title>\n\n  <p><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/plotly_regression_performance_plots/plotly_regression_performance_plots/0.1\" title=\"Plot actual vs predicted curves and residual plots tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Plot actual vs predicted curves and residual plots</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 0.1)</span>:</p>\n  <ul>\n    <li><i class=\"far fa-copy\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-files</span> <em>“Select input data file”</em>: <code class=\"language-plaintext highlighter-rouge\">test_rows_labels</code> tabular file</li>\n    <li><i class=\"far fa-copy\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-files</span> <em>“Select predicted data file”</em>: <code class=\"language-plaintext highlighter-rouge\">tabular</code> file (output of <strong>Ensemble methods for classification and regression</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span>)</li>\n  </ul>\n\n</blockquote>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <p>Inspect the plots. What can you say about the predictions?</p>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <p>The figures show that the prediction is very good because the predicted age lies close to the true age.</p>\n  </blockquote>\n</blockquote>\n\n<figure id=\"figure-13\" style=\"max-width: 90%;\"><img src=\"../../images/age-prediction-with-ml/scatter_plot.png\" alt=\"Scatter plot. \" width=\"700\" height=\"450\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/age-prediction-with-ml/scatter_plot.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 13</strong>:</span> Scatter plot for true and predicted age for test set using hyperparameter search.</figcaption></figure>\n\n<p>We can see in the scatter plot (figure <a href=\"#figure-13\">13</a>) that most of the points lie along the <code class=\"language-plaintext highlighter-rouge\">x = y</code> curve. This tells us that the true and predicted ages are close to each other. The root mean square error (<code class=\"language-plaintext highlighter-rouge\">RMSE</code>) is <code class=\"language-plaintext highlighter-rouge\">3.74</code> and the R2 score is <code class=\"language-plaintext highlighter-rouge\">0.94</code>.</p>\n\n<figure id=\"figure-14\" style=\"max-width: 90%;\"><img src=\"../../images/age-prediction-with-ml/residual_plot.png\" alt=\"Residuals. \" width=\"700\" height=\"450\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/age-prediction-with-ml/residual_plot.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 14</strong>:</span> The plot shows the residuals (predicted age - true) age against the predicted age.</figcaption></figure>\n\n<p>As you can see in the figure, there is no visible pattern between the plotted points, which means our model is good.</p>\n\n<figure id=\"figure-15\" style=\"max-width: 90%;\"><img src=\"../../images/age-prediction-with-ml/true_vs_predicted_plot.png\" alt=\"True vs predicted age. \" width=\"700\" height=\"450\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/age-prediction-with-ml/true_vs_predicted_plot.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 15</strong>:</span> True vs predicted age for all the samples in the test set.</figcaption></figure>\n<p>We can see that the predicted values are close to the true values.</p>\n\n<p>Figure <a href=\"#figure-13\">13</a> shows that we achieved an R2 score of <code class=\"language-plaintext highlighter-rouge\">0.94</code> and root mean square score of <code class=\"language-plaintext highlighter-rouge\">3.74</code> for the test set using Gradient boosting regressor. <a href=\"https://www.sciencedirect.com/science/article/pii/S1872497317301643?via%3Dihub\">Jana Naue et al. 2017</a> mention a similar root mean square score (<code class=\"language-plaintext highlighter-rouge\">3.93</code>) using the random forest regressor. The root mean square score shows the difference between the true and predicted age of humans. The R2 score (<code class=\"language-plaintext highlighter-rouge\">0.94</code>) is close to the best achievable score of <code class=\"language-plaintext highlighter-rouge\">1.0</code> which shows that the trained model is good. Overall, the second part of the analysis also shows that using the machine learning tools in Galaxy, we can achieve state-of-the-art predictions mentioned in the recent scientific studies.</p>\n\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>By following these steps, we learned how to perform regression and visualize the predictions using Galaxy’s machine learning and plotting tools. The features of the training dataset are mapped to the real-valued targets. This mapping is used to make predictions on an unseen (test) dataset. \nThe quality of predictions is visualized using multiple plots to ascertain the robustness of machine learning tasks. There are many other regressors in the machine learning suite which can be tried out on these datasets to find how they perform. Different datasets can also be analyzed using these regressors. The regressors have many parameters which can be altered while performing the analyses to see if they affect the prediction accuracy. It may be beneficial to perform a hyperparameter search to tune these parameters for different datasets.</p>\n"],"ref_slides":[],"hands_on":true,"slides":false,"mod_date":"2024-05-29 14:28:52 +0000","pub_date":"2020-01-25 09:55:53 +0000","version":25,"workflows":[{"workflow":"ml_regression.ga","tests":true,"url":"https://training.galaxyproject.org/training-material/topics/statistics/tutorials/regression_machinelearning/workflows/ml_regression.ga","path":"topics/statistics/tutorials/regression_machinelearning/workflows/ml_regression.ga","wfid":"statistics-regression_machinelearning","wfname":"ml_regression","trs_endpoint":"https://training.galaxyproject.org/training-material/api/ga4gh/trs/v2/tools/statistics-regression_machinelearning/versions/ml_regression","license":"MIT","creators":[{"class":"Organization","identifier":"https://orcid.org/0000-0002-2068-4695","name":"Anup Kumar"}],"name":"ml_regression","title":"ml_regression","test_results":null,"modified":"2024-06-16 00:05:38 +0000","mermaid":"flowchart TD\n  0[\"ℹ️ Input Dataset\\ntrain_rows\"];\n  style 0 stroke:#2c3143,stroke-width:4px;\n  1[\"ℹ️ Input Dataset\\ntest_rows_labels\"];\n  style 1 stroke:#2c3143,stroke-width:4px;\n  2[\"ℹ️ Input Dataset\\ntest_rows\"];\n  style 2 stroke:#2c3143,stroke-width:4px;\n  3[\"Pipeline Builder\"];\n  4[\"Generalized linear models\"];\n  0 -->|output| 4;\n  0 -->|output| 4;\n  5[\"Ensemble methods\"];\n  0 -->|output| 5;\n  0 -->|output| 5;\n  6[\"Remove beginning\"];\n  1 -->|output| 6;\n  7[\"Hyperparameter Search\"];\n  3 -->|outfile| 7;\n  0 -->|output| 7;\n  0 -->|output| 7;\n  8[\"Generalized linear models\"];\n  2 -->|output| 8;\n  4 -->|outfile_fit| 8;\n  9[\"Ensemble methods\"];\n  2 -->|output| 9;\n  5 -->|outfile_fit| 9;\n  10[\"Ensemble methods\"];\n  2 -->|output| 10;\n  7 -->|outfile_object| 10;\n  11[\"Plot actual vs predicted curves and residual plots\"];\n  6 -->|out_file1| 11;\n  8 -->|outfile_predict| 11;\n  12[\"Plot actual vs predicted curves and residual plots\"];\n  1 -->|output| 12;\n  9 -->|outfile_predict| 12;\n  13[\"Plot actual vs predicted curves and residual plots\"];\n  1 -->|output| 13;\n  10 -->|outfile_predict| 13;"}],"api":"https://training.galaxyproject.org/training-material/api/topics/statistics/tutorials/regression_machinelearning/tutorial.json","tools":["Remove beginning1","toolshed.g2.bx.psu.edu/repos/bgruening/plotly_regression_performance_plots/plotly_regression_performance_plots/0.1","toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_build_pipeline/sklearn_build_pipeline/1.0.11.0","toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_ensemble/sklearn_ensemble/1.0.11.0","toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_generalized_linear/sklearn_generalized_linear/1.0.11.0","toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_searchcv/sklearn_searchcv/1.0.11.0"],"supported_servers":{"exact":[{"url":"https://usegalaxy.eu","name":"UseGalaxy.eu","usegalaxy":true},{"url":"https://usegalaxy.org","name":"UseGalaxy.org (Main)","usegalaxy":true},{"url":"https://usegalaxy.org.au","name":"UseGalaxy.org.au","usegalaxy":true}],"inexact":[{"url":"https://usegalaxy.be/","name":"UseGalaxy.be","usegalaxy":false},{"url":"https://usegalaxy.cz/","name":"UseGalaxy.cz","usegalaxy":false},{"url":"https://usegalaxy.no/","name":"UseGalaxy.no","usegalaxy":false}]},"topic_name_human":"Statistics and machine learning","admin_install":{"install_tool_dependencies":true,"install_repository_dependencies":true,"install_resolver_dependencies":true,"tools":[{"name":"plotly_regression_performance_plots","owner":"bgruening","revisions":"389227fa1864","tool_panel_section_label":"Graph/Display Data","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"sklearn_build_pipeline","owner":"bgruening","revisions":"4c4ec859c31a","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"sklearn_ensemble","owner":"bgruening","revisions":"060ca94ac049","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"sklearn_generalized_linear","owner":"bgruening","revisions":"d4808d5b83da","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"sklearn_searchcv","owner":"bgruening","revisions":"7626ea9c2e1b","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"}]},"admin_install_yaml":"---\ninstall_tool_dependencies: true\ninstall_repository_dependencies: true\ninstall_resolver_dependencies: true\ntools:\n- name: plotly_regression_performance_plots\n  owner: bgruening\n  revisions: 389227fa1864\n  tool_panel_section_label: Graph/Display Data\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: sklearn_build_pipeline\n  owner: bgruening\n  revisions: 4c4ec859c31a\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: sklearn_ensemble\n  owner: bgruening\n  revisions: 060ca94ac049\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: sklearn_generalized_linear\n  owner: bgruening\n  revisions: d4808d5b83da\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: sklearn_searchcv\n  owner: bgruening\n  revisions: 7626ea9c2e1b\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n","tours":false,"video":false,"slides_recordings":false,"translations":{"tutorial":[],"slides":[],"video":false},"license":"CC-BY-4.0","type":"tutorial"}
{"layout":"tutorial_hands_on","title":"Introduction to Machine Learning using R","level":"Intermediate","zenodo_link":"","requirements":[{"type":"internal","topic_name":"galaxy-interface","tutorials":["rstudio"]},{"type":"internal","topic_name":"data-science","tutorials":["r-basics","r-advanced"]}],"follow_up_training":[{"type":"internal","topic_name":"statistics","tutorials":["age-prediction-with-ml"]}],"questions":["What are the main categories in Machine Learning algorithms?","How can I perform exploratory data analysis?","What are the main part of a clustering process?","How can a create a decision tree?","How can I assess a linear regression model?"],"objectives":["Understand the ML taxonomy and the commonly used machine learning algorithms for analysing -omics data","Understand differences between ML algorithms categories and to which kind of problem they can be applied","Understand different applications of ML in different -omics studies","Use some basic, widely used R packages for ML","Interpret and visualize the results obtained from ML analyses on omics datasets","Apply the ML techniques to analyse their own datasets"],"time_estimation":"3H","tags":["interactive-tools"],"key_points":["To be added"],"contributors":[{"name":"Fotis E. Psomopoulos","email":"fpsom@certh.gr","twitter":"fopsom","matrix":"fpsom:matrix.org","orcid":"0000-0002-0222-4273","linkedin":"fpsom","joined":"2019-03","elixir_node":"gr","affiliations":["gallantries","elixir-europe"],"id":"fpsom","url":"https://training.galaxyproject.org/training-material/api/contributors/fpsom.json","page":"https://training.galaxyproject.org/training-material/hall-of-fame/fpsom/"},{"name":"Gallantries: Bridging Training Communities in Life Science, Environment and Health","short_name":"Gallantries","start_date":"2020-09-01","end_date":"2023-09-30","joined":"2020-09","avatar":"https://gallantries.github.io/assets/images/gallantries-logo.png","github":false,"funder":true,"url":"https://training.galaxyproject.org/training-material/api/funders/gallantries.json","funder_name":"Erasmus+ Programme","funding_id":"2020-1-NL01-KA203-064717","funding_system":"erasmusplus","funding_statement":"This project (<a href=\"https://erasmus-plus.ec.europa.eu/projects/search/details/2020-1-NL01-KA203-064717\"><code class=\"language-plaintext highlighter-rouge\">2020-1-NL01-KA203-064717</code></a>) is funded with the support of the Erasmus+ programme of the European Union. Their funding has supported a large number of tutorials within the GTN across a wide array of topics.\n<img src=\"https://gallantries.github.io/assets/images/logosbeneficaireserasmusright_en.jpg\" alt=\"eu flag with the text: with the support of the erasmus programme of the european union\" />\n<img src=\"https://www.erasmusplus.nl/assets/images/logo.png\" alt=\"erasmus plus logo\" />","members":["abretaud","bebatut","colineroyaux","fpsom","hexylena","shiltemann","yvanlebras"],"id":"gallantries","page":"https://training.galaxyproject.org/training-material/hall-of-fame/gallantries/"}],"recordings":[{"captioners":["MariaTsayo"],"date":"2021-02-15","galaxy_version":"21.01","length":"1H30M","youtube_id":"RT-g6KyAGdE","speakers":["fpsom"]}],"js_requirements":{"mathjax":null,"mermaid":false},"short_id":"T00267","url":"/topics/statistics/tutorials/intro-to-ml-with-r/tutorial.html","topic_name":"statistics","tutorial_name":"intro-to-ml-with-r","dir":"topics/statistics/tutorials/intro-to-ml-with-r","symlink":null,"id":"statistics/intro-to-ml-with-r","ref_tutorials":["<p>This is an Introduction to Machine Learning in R, in which you’ll learn the basics of unsupervised learning for pattern recognition and supervised learning for prediction. At the end of this workshop, we hope that you will:</p>\n<ul>\n  <li>appreciate the importance of performing exploratory data analysis (or EDA) before starting to model your data.</li>\n  <li>understand the basics of unsupervised learning and know the examples of principal component analysis (PCA) and k-means clustering.</li>\n  <li>understand the basics of supervised learning for prediction and the differences between classification and regression.</li>\n  <li>understand modern machine learning techniques and principles, such as test train split, k-fold cross validation and regularization.</li>\n  <li>be able to write code to implement the above techniques and methodologies using <code class=\"language-plaintext highlighter-rouge\">R</code>, <code class=\"language-plaintext highlighter-rouge\">caret</code> and <code class=\"language-plaintext highlighter-rouge\">glmnet</code>.</li>\n</ul>\n\n<p>We will not be focusing on the mathematical foundation for each of the methods and approaches we’ll be discussing. There are many resources that can provide this context, but for the purposes of this workshop we believe that they are beyond the scope.</p>\n\n<p><em><strong>Note</strong></em>: All material here has been adapted from the course material for the Machine Learning course at SIB (22-23/07/2020) <span class=\"citation\"><a href=\"#ZENODO.3958880\">Baichoo <i>et al.</i> 2020</a></span></p>\n\n<h2 id=\"machine-learning-basic-concepts\">Machine Learning basic concepts</h2>\n\n<p>Machine Learning (ML) is a subset of Artificial Intelligence (AI) in the field of computer science that often uses statistical techniques to give computers the ability to “learn” (i.e., progressively improve performance on a specific task) with data, without being explicitly programmed.</p>\n\n<p>Machine Learning is often closely related, if not used as an alternate term, to fields like Data Mining (the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems), Pattern Recognition, Statistical Inference or Statistical Learning. All these areas often employ the same methods and perhaps the name changes based on the practitioner’s expertise or the application domain.</p>\n\n<h2 id=\"taxonomy-of-ml-and-examples-of-algorithms\">Taxonomy of ML and examples of algorithms</h2>\n\n<p>The main ML tasks are typically classified into two broad categories, depending on whether there is “feedback” or a “teacher” available to the learning system or not.</p>\n\n<ul>\n  <li><strong>Supervised Learning</strong>: The system is presented with example inputs and their desired outputs provided by the “teacher” and the goal of the machine learning algorithm is to create a mapping from the inputs to the outputs. The mapping can be thought of as a function that if it is given as an input one of the training samples it should output the desired value.</li>\n  <li><strong>Unsupervised Learning</strong>: In the unsupervised learning case, the machine learning algorithm is not given any examples of desired output, and is left on its own to find structure in its input.</li>\n</ul>\n\n<p>The main machine learning tasks are separated based on what the system tries to accomplish in the end:</p>\n<ul>\n  <li><strong>Dimensionality Reduction</strong>: simplifies inputs by mapping them into a lower-dimensional space. Topic modeling is a related problem, where a program is given a list of human language documents and is tasked with finding out which documents cover similar topics.</li>\n  <li><strong>Clustering</strong>: a set of inputs is to be divided into groups. Unlike in classification, the groups\nare not known beforehand, making this typically an unsupervised task.</li>\n  <li><strong>Classification</strong>: inputs are divided into two or more classes, and the learner must produce a model that assigns unseen inputs to one or more (multi-label classification) of these classes. This is typically tackled in a supervised manner. Identification of patient vs cases is an example of classification, where the inputs are gene expression and/or clinical profiles and the classes are “patient” and “healthy”.</li>\n  <li><strong>Regression</strong>: also a supervised problem, the outputs are continuous rather than discrete.</li>\n  <li><strong>Association Rules learning</strong> (or dependency modelling): Searches for relationships between inputs. For example, a supermarket might gather data on customer purchasing habits. Using association rule learning, the supermarket can determine which products are frequently bought together and use this information for marketing purposes. This is sometimes referred to as market basket analysis.</li>\n</ul>\n\n<h2 id=\"overview-of-deep-learning\">Overview of Deep learning</h2>\n\n<p>Deep learning is a recent trend in machine learning that models highly non-linear representations of data. In the past years, deep learning has gained a tremendous momentum and prevalence for a variety of applications. Among these are image and speech recognition, driverless cars, natural language processing and many more. Interestingly, the majority of mathematical concepts for deep learning have been known for decades. However, it is only through several recent developments that the full potential of deep learning has been unleashed. The success of deep learning has led to a wide range of frameworks and libraries for various programming languages. Examples include <code class=\"language-plaintext highlighter-rouge\">Caffee</code>, <code class=\"language-plaintext highlighter-rouge\">Theano</code>, <code class=\"language-plaintext highlighter-rouge\">Torch</code> and <code class=\"language-plaintext highlighter-rouge\">TensorFlow</code>, amongst others.</p>\n\n<p>The R programming language has gained considerable popularity among statisticians and data miners for its ease-of-use, as well as its sophisticated visualizations and analyses. With the advent of the deep learning era, the support for deep learning in R has grown ever since, with an increasing number of packages becoming available. This section presents an overview on deep learning in R as provided by the following packages: <code class=\"language-plaintext highlighter-rouge\">MXNetR</code>, <code class=\"language-plaintext highlighter-rouge\">darch</code>, <code class=\"language-plaintext highlighter-rouge\">deepnet</code>, <code class=\"language-plaintext highlighter-rouge\">H2O</code> and <code class=\"language-plaintext highlighter-rouge\">deepr</code>. It’s important noting that the underlying learning algorithms greatly vary from one package to another. As such, the following table shows a list of the available methods/architectures in each of the packages.</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Package</th>\n      <th>Available architectures of neural networks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>MXNetR</td>\n      <td>Feed-forward neural network, convolutional neural network (CNN)</td>\n    </tr>\n    <tr>\n      <td>darch</td>\n      <td>Restricted Boltzmann machine, deep belief network</td>\n    </tr>\n    <tr>\n      <td>deepnet</td>\n      <td>Feed-forward neural network, restricted Boltzmann machine, deep belief network, stacked autoencoders</td>\n    </tr>\n    <tr>\n      <td>H2O</td>\n      <td>Feed-forward neural network, deep autoencoders</td>\n    </tr>\n    <tr>\n      <td>deepr</td>\n      <td>Simplify some functions from H2O and deepnet packages</td>\n    </tr>\n  </tbody>\n</table>\n\n<h2 id=\"applications-of-ml-in-bioinformatics\">Applications of ML in Bioinformatics</h2>\n\n<p>There are several biological domains where machine learning techniques are applied for knowledge extraction from data. The following figure (retrieved from <span class=\"citation\"><a href=\"#Larra_aga_2006\">Larrañaga <i>et al.</i> 2006</a></span>) shows a scheme of the main biological problems where computational methods are being applied.</p>\n\n<figure id=\"figure-1\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/bioinformatics-ml.png\" alt=\"A series of overlapping boxes showing intersections of different topics like text mining and proteomics and evolution and microarrays, with various topics listed in the intersections. Unfortunately the source image is too low resolution even for sighted users.\" width=\"520\" height=\"435\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/bioinformatics-ml.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 1</strong>:</span> Classification of the topics where machine learning methods are applied (<a href=\"https://doi.org/10.1093/bib/bbk007\">https://doi.org/10.1093/bib/bbk007</a>)</figcaption></figure>\n\n<blockquote class=\"tip\">\n  <tip-title>Examples of different Machine Learning / Data Mining techniques that can be applied to different NGS data analysis pipelines.</tip-title>\n  <p>An extensive list of examples of applications of Machine Learning in Bioinformatics can be found in the <span class=\"citation\"><a href=\"#Larra_aga_2006\">Larrañaga <i>et al.</i> 2006</a></span></p>\n</blockquote>\n\n<h2 id=\"how-to-choose-the-right-machine-learning-technique\">How to choose the right Machine Learning technique?</h2>\n\n<p>Tip 4 in the “Ten quick tips for machine learning in computational biology” (<span class=\"citation\"><a href=\"#Chicco_2017\">Chicco 2017</a></span>) provides a nice overview of what one should keep in mind, when choosing the right Machine Learning technique in Bioinformatics.</p>\n\n<blockquote class=\"quote\" cite=\"https://doi.org/10.1186%2Fs13040-017-0155-3\">\n  <p><strong>Which algorithm should you choose to start? In short; The simplest one!</strong></p>\n\n  <p>Once you understand what kind of biological problem you are trying to solve, and which method category can fit your situation, you then have to choose the machine learning algorithm with which to start your project. Even if it always advisable to use multiple techniques and compare their results, the decision on which one to start can be tricky.</p>\n\n  <p>Many textbooks suggest to select a machine learning method by just taking into account the problem representation, while Pedro Domingos (“A few useful things to know about machine learning”, Commun ACM. 2012; 55(10):78–87) suggests to take into account also the cost evaluation, and the performance optimization.</p>\n\n  <p>This algorithm-selection step, which usually occurs at the beginning of a machine learning journey, can be dangerous for beginners. In fact, an inexperienced practitioner might end up choosing a complicated, inappropriate data mining method which might lead him/her to bad results, as well as to lose precious time and energy. Therefore, this is our tip for the algorithm selection: if undecided, start with the simplest algorithm (Hand DJ, “Classifier technology and the illusion of progress”. Stat Sci. 2006; 21(1):1–14).</p>\n\n  <p>By employing a simple algorithm, you will be able to keep everything under control, and better understand what is happening during the application of the method. In addition, a simple algorithm will provide better generalization skills, less chance of overfitting, easier training and faster learning properties than complex methods. As David J. Hand explained, complex models should be employed only if the dataset features provide some reasonable justification for their usage.</p>\n</blockquote>\n\n<h1 id=\"exploratory-data-analysis-eda-and-unsupervised-learning\">Exploratory Data Analysis (EDA) and Unsupervised Learning</h1>\n\n<p>Before diving in the tutorial, we need to open <span class=\"tool\" data-tool=\"interactive_tool_rstudio\" title=\"RStudio tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>RStudio</strong></span>. If you do not know how or never interacted with RStudio, please follow the <a href=\"/training-material/topics/galaxy-interface/tutorials/rstudio/tutorial.html\">dedicated tutorial</a>.</p>\n\n<!--SNIPPET-->\n<blockquote class=\"hands_on\">   <div class=\"box-title hands_on-title\" id=\"hands-on-launch-rstudio\"><i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i> Hands-on: Launch RStudio</div>   <p>Depending on which server you are using, you may be able to run RStudio directly in Galaxy. If that is not available, RStudio Cloud can be an alternative.</p>   <blockquote class=\"tip\">   <tip-title>Launch RStudio in Galaxy</tip-title>   <p>Currently RStudio in Galaxy is only available on <a href=\"https://usegalaxy.eu\">UseGalaxy.eu</a> and <a href=\"https://usegalaxy.org\">UseGalaxy.org</a></p>   <ol>     <li>Open the Rstudio tool <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> by clicking <a href=\"https://usegalaxy.eu/?tool_id=interactive_tool_rstudio\">here to launch RStudio</a></li>     <li>Click Run Tool</li>     <li>The tool will start running and will stay running permanently</li>     <li>Click on the “User” menu at the top and go to “Active InteractiveTools” and locate the RStudio instance you started.</li>   </ol> </blockquote>   <blockquote class=\"tip\">   <tip-title>Launch RStudio Cloud if not available on Galaxy</tip-title>   <p>If RStudio is not available on the Galaxy instance:</p>   <ol>     <li>Register for <a href=\"https://client.login.rstudio.cloud/oauth/login?show_auth=0&amp;show_login=1&amp;show_setup=1\">RStudio Cloud</a>, or login if you already have an account</li>     <li>Create a new project</li>   </ol> </blockquote> </blockquote>\n<p><!--END_SNIPPET--></p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Installing Required Packages</hands-on-title>\n\n  <ol>\n    <li>\n      <p>Run the following code to install required packages</p>\n\n      <div class=\"language-R highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">## To install needed CRAN packages:</span><span class=\"w\">\n</span><span class=\"n\">install.packages</span><span class=\"p\">(</span><span class=\"s2\">\"tidyverse\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">install.packages</span><span class=\"p\">(</span><span class=\"s2\">\"GGally\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">install.packages</span><span class=\"p\">(</span><span class=\"s2\">\"caret\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">install.packages</span><span class=\"p\">(</span><span class=\"s2\">\"gmodels\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">install.packages</span><span class=\"p\">(</span><span class=\"s2\">\"rpart\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">install.packages</span><span class=\"p\">(</span><span class=\"s2\">\"rpart.plot\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">install.packages</span><span class=\"p\">(</span><span class=\"s2\">\"dendextend\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">install.packages</span><span class=\"p\">(</span><span class=\"s2\">\"randomForest\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">install.packages</span><span class=\"p\">(</span><span class=\"s2\">\"mlr3\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">install.packages</span><span class=\"p\">(</span><span class=\"s2\">\"devtools\"</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"c1\">## To install needed Bioconductor packages:</span><span class=\"w\">\n</span><span class=\"k\">if</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"o\">!</span><span class=\"n\">requireNamespace</span><span class=\"p\">(</span><span class=\"s2\">\"BiocManager\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">quietly</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"kc\">TRUE</span><span class=\"p\">))</span><span class=\"w\">\n    </span><span class=\"n\">install.packages</span><span class=\"p\">(</span><span class=\"s2\">\"BiocManager\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">BiocManager</span><span class=\"o\">::</span><span class=\"n\">install</span><span class=\"p\">()</span><span class=\"w\">\n</span><span class=\"n\">BiocManager</span><span class=\"o\">::</span><span class=\"n\">install</span><span class=\"p\">(</span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"s2\">\"limma\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s2\">\"edgeR\"</span><span class=\"p\">))</span><span class=\"w\">\n\n</span><span class=\"c1\"># To install libraries from GitHub source</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">devtools</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">install_github</span><span class=\"p\">(</span><span class=\"s2\">\"vqv/ggbiplot\"</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n  </ol>\n</blockquote>\n\n<h2 id=\"loading-and-exploring-data\">Loading and exploring data</h2>\n\n<p>The data that we will be using for this workshop are from the following sources:</p>\n\n<ul>\n  <li>The <a href=\"http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29\">Breast Cancer Wisconsin (Diagnostic) Data Set</a> from the <a href=\"http://archive.ics.uci.edu/ml/\">UCI Machine Learning repository</a>.</li>\n  <li>RNA-Seq data from the study of tooth growth in mouse embryos from the <a href=\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE76316\">Gene Expression Omnibus ID:GSE76316</a></li>\n</ul>\n\n<p>We will first load up the UCI dataset. The dataset itself does not contain column names, we’ve created a second file with only the column names, which we will use.\nWe will be using <a href=\"https://www.tidyverse.org\">tidyverse</a>, a collection of R packages for Data Science.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Load the UCI Dataset</hands-on-title>\n\n  <ol>\n    <li>\n      <p>Load the data</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">tidyverse</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"c1\"># working with data frames, plotting</span><span class=\"w\">\n\n</span><span class=\"n\">breastCancerData</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s2\">\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"</span><span class=\"p\">,</span><span class=\"w\">\n               </span><span class=\"n\">col_names</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"kc\">FALSE</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">breastCancerDataColNames</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s2\">\"https://raw.githubusercontent.com/fpsom/2020-07-machine-learning-sib/master/data/wdbc.colnames.csv\"</span><span class=\"p\">,</span><span class=\"w\">\n                                     </span><span class=\"n\">col_names</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"kc\">FALSE</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">colnames</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">breastCancerDataColNames</span><span class=\"o\">$</span><span class=\"n\">X1</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n    <li>\n      <p>If all goes well, we can see that our dataset contains 569 observations across 32 variables. This is what the first 6 lines look like:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code># Check out head of dataframe\nbreastCancerData %&gt;% head()\n\n# A tibble: 6 x 32\n      ID Diagnosis Radius.Mean Texture.Mean Perimeter.Mean Area.Mean Smoothness.Mean\n   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n1 8.42e5 M                18.0         10.4          123.      1001           0.118\n2 8.43e5 M                20.6         17.8          133.      1326           0.0847\n3 8.43e7 M                19.7         21.2          130       1203           0.110\n4 8.43e7 M                11.4         20.4           77.6      386.          0.142\n5 8.44e7 M                20.3         14.3          135.      1297           0.100\n6 8.44e5 M                12.4         15.7           82.6      477.          0.128\n# ... with 25 more variables: Compactness.Mean &lt;dbl&gt;, Concavity.Mean &lt;dbl&gt;,\n#   Concave.Points.Mean &lt;dbl&gt;, Symmetry.Mean &lt;dbl&gt;, Fractal.Dimension.Mean &lt;dbl&gt;,\n#   Radius.SE &lt;dbl&gt;, Texture.SE &lt;dbl&gt;, Perimeter.SE &lt;dbl&gt;, Area.SE &lt;dbl&gt;,\n#   Smoothness.SE &lt;dbl&gt;, Compactness.SE &lt;dbl&gt;, Concavity.SE &lt;dbl&gt;, Concave.Points.SE &lt;dbl&gt;,\n#   Symmetry.SE &lt;dbl&gt;, Fractal.Dimension.SE &lt;dbl&gt;, Radius.Worst &lt;dbl&gt;, Texture.Worst &lt;dbl&gt;,\n#   Perimeter.Worst &lt;dbl&gt;, Area.Worst &lt;dbl&gt;, Smoothness.Worst &lt;dbl&gt;,\n#   Compactness.Worst &lt;dbl&gt;, Concavity.Worst &lt;dbl&gt;, Concave.Points.Worst &lt;dbl&gt;,\n#   Symmetry.Worst &lt;dbl&gt;, Fractal.Dimension.Worst &lt;dbl&gt;\n</code></pre></div>      </div>\n    </li>\n    <li>\n      <p>We will also make our <code class=\"language-plaintext highlighter-rouge\">Diagnosis</code> column a factor:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Make Diagnosis a factor</span><span class=\"w\">\n</span><span class=\"n\">breastCancerData</span><span class=\"o\">$</span><span class=\"n\">Diagnosis</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">as.factor</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"o\">$</span><span class=\"n\">Diagnosis</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <blockquote class=\"question\">\n        <question-title></question-title>\n\n        <p>What is a factor?</p>\n\n        <blockquote class=\"solution\">\n          <solution-title></solution-title>\n\n          <p>TODO</p>\n\n        </blockquote>\n      </blockquote>\n    </li>\n  </ol>\n\n</blockquote>\n\n<h2 id=\"what-is-exploratory-data-analysis-eda-and-why-is-it-useful\">What is Exploratory Data Analysis (EDA) and why is it useful?</h2>\n\n<p>Before thinking about modeling, have a look at your data. There is no point in throwing a 10000 layer convolutional neural network (whatever that means) at your data before you even know what you’re dealing with.</p>\n\n<p>We will first remove the first column, which is the unique identifier of each row:</p>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <p>Why?</p>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <p>TODO</p>\n\n  </blockquote>\n</blockquote>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Exploratory Data Analysis</hands-on-title>\n  <ol>\n    <li>\n      <p>Remove the first column</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Remove first column</span><span class=\"w\">\n</span><span class=\"n\">breastCancerDataNoID</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">breastCancerData</span><span class=\"p\">[</span><span class=\"m\">2</span><span class=\"o\">:</span><span class=\"n\">ncol</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"p\">)]</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n    <li>\n      <p>View the dataset. The output should like like this:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code># View head\nbreastCancerDataNoID %&gt;% head()\n\n# A tibble: 6 x 31\n  Diagnosis Radius.Mean Texture.Mean Perimeter.Mean Area.Mean Smoothness.Mean\n  &lt;fct&gt;           &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n1 M                18.0         10.4          123.      1001           0.118\n2 M                20.6         17.8          133.      1326           0.0847\n3 M                19.7         21.2          130       1203           0.110\n4 M                11.4         20.4           77.6      386.          0.142\n5 M                20.3         14.3          135.      1297           0.100\n6 M                12.4         15.7           82.6      477.          0.128\n# ... with 25 more variables: Compactness.Mean &lt;dbl&gt;, Concavity.Mean &lt;dbl&gt;,\n#   Concave.Points.Mean &lt;dbl&gt;, Symmetry.Mean &lt;dbl&gt;, Fractal.Dimension.Mean &lt;dbl&gt;,\n#   Radius.SE &lt;dbl&gt;, Texture.SE &lt;dbl&gt;, Perimeter.SE &lt;dbl&gt;, Area.SE &lt;dbl&gt;,\n#   Smoothness.SE &lt;dbl&gt;, Compactness.SE &lt;dbl&gt;, Concavity.SE &lt;dbl&gt;, Concave.Points.SE &lt;dbl&gt;,\n#   Symmetry.SE &lt;dbl&gt;, Fractal.Dimension.SE &lt;dbl&gt;, Radius.Worst &lt;dbl&gt;, Texture.Worst &lt;dbl&gt;,\n#   Perimeter.Worst &lt;dbl&gt;, Area.Worst &lt;dbl&gt;, Smoothness.Worst &lt;dbl&gt;,\n#   Compactness.Worst &lt;dbl&gt;, Concavity.Worst &lt;dbl&gt;, Concave.Points.Worst &lt;dbl&gt;,\n#   Symmetry.Worst &lt;dbl&gt;, Fractal.Dimension.Worst &lt;dbl&gt;\n</code></pre></div>      </div>\n    </li>\n    <li>\n      <p>We have many variables in this dataset. For the interest of time, we will focus only on the first five. Let’s have a look at a plot:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">GGally</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">ggpairs</span><span class=\"p\">(</span><span class=\"n\">breastCancerDataNoID</span><span class=\"p\">[</span><span class=\"m\">1</span><span class=\"o\">:</span><span class=\"m\">5</span><span class=\"p\">],</span><span class=\"w\"> </span><span class=\"n\">aes</span><span class=\"p\">(</span><span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">Diagnosis</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"m\">0.4</span><span class=\"p\">))</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <figure id=\"figure-2\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/ggpairs5variables.png\" alt=\"ggpairs output of the first 5 variables. \" width=\"1024\" height=\"559\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/ggpairs5variables.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 2</strong>:</span> ggpairs output of the first 5 variables</figcaption></figure>\n    </li>\n    <li>\n      <p>Next, we need to center and scale the data.</p>\n\n      <p>Note that the features have widely varying centers and scales (means and standard deviations), so we’ll want to center and scale them in some situations. We will use the <code class=\"language-plaintext highlighter-rouge\">[caret](https://cran.r-project.org/web/packages/caret/vignettes/caret.html)</code> package for this, and specifically, the <code class=\"language-plaintext highlighter-rouge\">preProcess</code> function.</p>\n\n      <p>The <code class=\"language-plaintext highlighter-rouge\">preProcess</code> function can be used for many operations on predictors, including centering and scaling. The function <code class=\"language-plaintext highlighter-rouge\">preProcess</code> estimates the required parameters for each operation and <code class=\"language-plaintext highlighter-rouge\">predict.preProcess</code> is used to apply them to specific data sets. This function can also be interfaced when calling the <code class=\"language-plaintext highlighter-rouge\">train</code> function.</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">caret</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"c1\"># Center &amp; scale data</span><span class=\"w\">\n</span><span class=\"n\">ppv</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">preProcess</span><span class=\"p\">(</span><span class=\"n\">breastCancerDataNoID</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">method</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"s2\">\"center\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s2\">\"scale\"</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">breastCancerDataNoID_tr</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">ppv</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">breastCancerDataNoID</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n    <li>\n      <p>Let’s have a look on the impact of this process by viewing the summary of the first 5 variables before and after the process:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Summarize first 5 columns of the original data</span><span class=\"w\">\n</span><span class=\"n\">breastCancerDataNoID</span><span class=\"p\">[</span><span class=\"m\">1</span><span class=\"o\">:</span><span class=\"m\">5</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"o\">%&gt;%</span><span class=\"w\"> </span><span class=\"n\">summary</span><span class=\"p\">()</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>It should look like:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Diagnosis  Radius.Mean      Texture.Mean   Perimeter.Mean     Area.Mean\nB:357     Min.   : 6.981   Min.   : 9.71   Min.   : 43.79   Min.   : 143.5\nM:212     1st Qu.:11.700   1st Qu.:16.17   1st Qu.: 75.17   1st Qu.: 420.3\n          Median :13.370   Median :18.84   Median : 86.24   Median : 551.1\n          Mean   :14.127   Mean   :19.29   Mean   : 91.97   Mean   : 654.9\n          3rd Qu.:15.780   3rd Qu.:21.80   3rd Qu.:104.10   3rd Qu.: 782.7\n          Max.   :28.110   Max.   :39.28   Max.   :188.50   Max.   :2501.0\n</code></pre></div>      </div>\n    </li>\n    <li>\n      <p>Let’s check the summary of the re-centered and scaled data</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Summarize first 5 columns of the re-centered and scaled data</span><span class=\"w\">\n</span><span class=\"n\">breastCancerDataNoID_tr</span><span class=\"p\">[</span><span class=\"m\">1</span><span class=\"o\">:</span><span class=\"m\">5</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"o\">%&gt;%</span><span class=\"w\"> </span><span class=\"n\">summary</span><span class=\"p\">()</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>It now should look like this:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Diagnosis  Radius.Mean       Texture.Mean     Perimeter.Mean      Area.Mean\nB:357     Min.   :-2.0279   Min.   :-2.2273   Min.   :-1.9828   Min.   :-1.4532\nM:212     1st Qu.:-0.6888   1st Qu.:-0.7253   1st Qu.:-0.6913   1st Qu.:-0.6666\n          Median :-0.2149   Median :-0.1045   Median :-0.2358   Median :-0.2949\n          Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000\n          3rd Qu.: 0.4690   3rd Qu.: 0.5837   3rd Qu.: 0.4992   3rd Qu.: 0.3632\n          Max.   : 3.9678   Max.   : 4.6478   Max.   : 3.9726   Max.   : 5.2459\n</code></pre></div>      </div>\n\n      <p>As, we can observe here, all variables in our new data have a mean of 0 while maintaining the same distribution of the values. However, this also means that the absolute values do not correspond to the “real”, original data - and is just a representation of them.</p>\n    </li>\n    <li>\n      <p>We can also check whether our plot has changed with the new data:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">GGally</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">ggpairs</span><span class=\"p\">(</span><span class=\"n\">breastCancerDataNoID_tr</span><span class=\"p\">[</span><span class=\"m\">1</span><span class=\"o\">:</span><span class=\"m\">5</span><span class=\"p\">],</span><span class=\"w\"> </span><span class=\"n\">aes</span><span class=\"p\">(</span><span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">Diagnosis</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"m\">0.4</span><span class=\"p\">))</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <figure id=\"figure-3\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/ggpairs5variables_tr.png\" alt=\"ggpairs output of the first 5 variables of the recentered/rescaled data. \" width=\"1024\" height=\"559\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/ggpairs5variables_tr.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 3</strong>:</span> ggpairs output of the first 5 variables of the recentered/rescaled data</figcaption></figure>\n\n      <blockquote class=\"question\">\n        <question-title></question-title>\n\n        <p>Do you see any differences?</p>\n\n        <blockquote class=\"solution\">\n          <solution-title></solution-title>\n\n          <p>TODO</p>\n\n        </blockquote>\n      </blockquote>\n    </li>\n  </ol>\n\n</blockquote>\n\n<h2 id=\"unsupervised-learning\">Unsupervised Learning</h2>\n\n<h3 id=\"dimensionality-reduction-and-pca\">Dimensionality Reduction and PCA</h3>\n\n<p><strong>Machine learning</strong> is the science and art of giving computers the ability to learn to make decisions from data without being explicitly programmed.</p>\n\n<p><strong>Unsupervised learning</strong>, in essence, is the machine learning task of uncovering hidden patterns and structures from <strong>unlabeled data</strong>. For example, a researcher might want to group their samples into distinct groups, based on their gene expression data without in advance what these categories maybe. This is known as <strong>clustering</strong>, one branch of unsupervised learning.</p>\n\n<p><strong>Supervised learning</strong> (which will be addressed later in depth), is the branch of machine learning that involves <strong>predicting labels</strong>, such as whether a tumor will be benign or malignant.</p>\n\n<p>Another form of unsupervised learning, is dimensionality reduction; in the UCI dataset, for example, there are too many features to keep track of. What if we could reduce the number of features yet still keep much of the information?</p>\n\n<p>Principal component analysis (PCA) is one of the most commonly used methods of dimensionality reduction, and extracts the features with the largest variance. What PCA essentially does is the following:</p>\n<ul>\n  <li>The first step of PCA is to decorrelate your data and this corresponds to a linear transformation of the vector space your data lie in;</li>\n  <li>The second step is the actual dimension reduction; what is really happening is that your decorrelation step (the first step above) transforms the features into new and uncorrelated features; this second step then chooses the features that contain most of the information about the data.</li>\n</ul>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Dimensionality Reduction &amp; PCA</hands-on-title>\n\n  <ol>\n    <li>Let’s have a look into the variables that we currently have, and apply PCA to them. As you can see, we will be using only the numerical variables (i.e. we will exclude the first two, <code class=\"language-plaintext highlighter-rouge\">ID</code> and <code class=\"language-plaintext highlighter-rouge\">Diagnosis</code>):\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">ppv_pca</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">prcomp</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"p\">[</span><span class=\"m\">3</span><span class=\"o\">:</span><span class=\"n\">ncol</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"p\">)],</span><span class=\"w\"> </span><span class=\"n\">center</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"kc\">TRUE</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">scale.</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"kc\">TRUE</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n    <li>\n      <p>We can use the <code class=\"language-plaintext highlighter-rouge\">summary()</code> function to get a summary of the PCA:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">ppv_pca</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>The resulting table, shows us the importance of each Principal Component; the standard deviation, the proportion of the variance that it captures, as well as the cumulative proportion of variance capture by the principal components.</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Importance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7     PC8    PC9\nStandard deviation     3.6444 2.3857 1.67867 1.40735 1.28403 1.09880 0.82172 0.69037 0.6457\nProportion of Variance 0.4427 0.1897 0.09393 0.06602 0.05496 0.04025 0.02251 0.01589 0.0139\nCumulative Proportion  0.4427 0.6324 0.72636 0.79239 0.84734 0.88759 0.91010 0.92598 0.9399\n                          PC10   PC11    PC12    PC13    PC14    PC15    PC16    PC17\nStandard deviation     0.59219 0.5421 0.51104 0.49128 0.39624 0.30681 0.28260 0.24372\nProportion of Variance 0.01169 0.0098 0.00871 0.00805 0.00523 0.00314 0.00266 0.00198\nCumulative Proportion  0.95157 0.9614 0.97007 0.97812 0.98335 0.98649 0.98915 0.99113\n                          PC18    PC19    PC20   PC21    PC22    PC23   PC24    PC25    PC26\nStandard deviation     0.22939 0.22244 0.17652 0.1731 0.16565 0.15602 0.1344 0.12442 0.09043\nProportion of Variance 0.00175 0.00165 0.00104 0.0010 0.00091 0.00081 0.0006 0.00052 0.00027\nCumulative Proportion  0.99288 0.99453 0.99557 0.9966 0.99749 0.99830 0.9989 0.99942 0.99969\n                          PC27    PC28    PC29    PC30\nStandard deviation     0.08307 0.03987 0.02736 0.01153\nProportion of Variance 0.00023 0.00005 0.00002 0.00000\nCumulative Proportion  0.99992 0.99997 1.00000 1.00000\n</code></pre></div>      </div>\n    </li>\n  </ol>\n</blockquote>\n\n<p>Principal Components are the underlying structure in the data. They are the directions where there is the most variance, the directions where the data is most spread out. This means that we try to find the straight line that best spreads the data out when it is projected along it. This is the first principal component, the straight line that shows the most substantial variance in the data.</p>\n\n<p>PCA is a type of linear transformation on a given data set that has values for a certain number of variables (coordinates) for a certain amount of spaces. In this way, you transform a set of <code class=\"language-plaintext highlighter-rouge\">x</code> correlated variables over <code class=\"language-plaintext highlighter-rouge\">y</code> samples to a set of <code class=\"language-plaintext highlighter-rouge\">p</code> uncorrelated principal components over the same samples.</p>\n\n<p>Where many variables correlate with one another, they will all contribute strongly to the same principal component. Where your initial variables are strongly correlated with one another, you will be able to approximate most of the complexity in your dataset with just a few principal components. As you add more principal components, you summarize more and more of the original dataset. Adding additional components makes your estimate of the total dataset more accurate, but also more unwieldy.</p>\n\n<p>Every eigenvector has a corresponding eigenvalue. Simply put, an eigenvector is a direction, such as “vertical” or “45 degrees”, while an eigenvalue is a number telling you how much variance there is in the data in that direction. The eigenvector with the highest eigenvalue is, therefore, the first principal component. The number of eigenvalues and eigenvectors that exits is equal to the number of dimensions the data set has. In our case, we had 30 variables (32 original, minus the first two), so we have produced 30 eigenvectors / PCs. And we can see that we can address more than 95% of the variance (0.95157) using only the first 10 PCs.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Deeper look into PCA</hands-on-title>\n\n  <ol>\n    <li>\n      <p>We should also have a deeper look in our PCA object:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">str</span><span class=\"p\">(</span><span class=\"n\">ppv_pca</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>The output should look like this:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>List of 5\n $ sdev    : num [1:30] 3.64 2.39 1.68 1.41 1.28 ...\n $ rotation: num [1:30, 1:30] -0.219 -0.104 -0.228 -0.221 -0.143 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:30] \"Radius.Mean\" \"Texture.Mean\" \"Perimeter.Mean\" \"Area.Mean\" ...\n  .. ..$ : chr [1:30] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n $ center  : Named num [1:30] 14.1273 19.2896 91.969 654.8891 0.0964 ...\n  ..- attr(*, \"names\")= chr [1:30] \"Radius.Mean\" \"Texture.Mean\" \"Perimeter.Mean\" \"Area.Mean\" ...\n $ scale   : Named num [1:30] 3.524 4.301 24.299 351.9141 0.0141 ...\n  ..- attr(*, \"names\")= chr [1:30] \"Radius.Mean\" \"Texture.Mean\" \"Perimeter.Mean\" \"Area.Mean\" ...\n $ x       : num [1:569, 1:30] -9.18 -2.39 -5.73 -7.12 -3.93 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : NULL\n  .. ..$ : chr [1:30] \"PC1\" \"PC2\" \"PC3\" \"PC4\" ...\n - attr(*, \"class\")= chr \"prcomp\"\n</code></pre></div>      </div>\n    </li>\n    <li>\n      <p>The information listed captures the following:</p>\n\n      <ol>\n        <li>The center point (<code class=\"language-plaintext highlighter-rouge\">$center</code>), scaling (<code class=\"language-plaintext highlighter-rouge\">$scale</code>) and the standard deviation(<code class=\"language-plaintext highlighter-rouge\">$sdev</code>) of each original variable</li>\n        <li>The relationship (correlation or anticorrelation, etc) between the initial variables and the principal components (<code class=\"language-plaintext highlighter-rouge\">$rotation</code>)</li>\n        <li>The values of each sample in terms of the principal components (<code class=\"language-plaintext highlighter-rouge\">$x</code>)</li>\n      </ol>\n\n      <p>Let’s try to visualize the results we’ve got so far. We will be using the <a href=\"https://github.com/vqv/ggbiplot\"><code class=\"language-plaintext highlighter-rouge\">ggbiplot</code> library</a> for this purpose.</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">ggbiplot</span><span class=\"p\">(</span><span class=\"n\">ppv_pca</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">choices</span><span class=\"o\">=</span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"m\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"m\">2</span><span class=\"p\">),</span><span class=\"w\">\n         </span><span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">rownames</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"p\">),</span><span class=\"w\">\n         </span><span class=\"n\">ellipse</span><span class=\"o\">=</span><span class=\"kc\">TRUE</span><span class=\"p\">,</span><span class=\"w\">\n         </span><span class=\"n\">groups</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">breastCancerData</span><span class=\"o\">$</span><span class=\"n\">Diagnosis</span><span class=\"p\">,</span><span class=\"w\">\n         </span><span class=\"n\">obs.scale</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">1</span><span class=\"p\">,</span><span class=\"w\">\n         </span><span class=\"n\">var.axes</span><span class=\"o\">=</span><span class=\"kc\">TRUE</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">var.scale</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">1</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">ggtitle</span><span class=\"p\">(</span><span class=\"s2\">\"PCA of Breast Cancer Dataset\"</span><span class=\"p\">)</span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">theme_minimal</span><span class=\"p\">()</span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">theme</span><span class=\"p\">(</span><span class=\"n\">legend.position</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"bottom\"</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <figure id=\"figure-4\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/pc12Visualization_Full.png\" alt=\"Visualization of the first two PCs on the UCI Breast Cancer dataset. \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/pc12Visualization_Full.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 4</strong>:</span> Visualization of the first two PCs on the UCI Breast Cancer dataset</figcaption></figure>\n    </li>\n  </ol>\n\n</blockquote>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <ol>\n    <li>Try changing the parameters of the plot. For example, check the <code class=\"language-plaintext highlighter-rouge\">choices</code> and the <code class=\"language-plaintext highlighter-rouge\">var.scale</code>. Is there an impact? What does this mean?</li>\n    <li>We have been using the entire table of data. What if we restrict our analysis on the <code class=\"language-plaintext highlighter-rouge\">mean</code> values (i.e. columns 3-12)? Is there an impact?</li>\n  </ol>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <p>TODO</p>\n\n  </blockquote>\n</blockquote>\n\n<h3 id=\"clustering\">Clustering</h3>\n\n<p>One popular technique in unsupervised learning is clustering. As the name itself suggests, Clustering algorithms group a set of data points into subsets or clusters. The algorithms’ goal is to create clusters that are coherent internally, but clearly different from each other externally. In other words, entities within a cluster should be as similar as possible and entities in one cluster should be as dissimilar as possible from entities in another.</p>\n\n<p>Broadly speaking there are two ways of clustering data points based on the algorithmic structure and operation, namely agglomerative and divisive.</p>\n\n<ul>\n  <li><strong>Agglomerative</strong>: An agglomerative approach begins with each observation in a distinct (singleton) cluster, and successively merges clusters together until a stopping criterion is satisfied.</li>\n  <li><strong>Divisive</strong>: A divisive method begins with all patterns in a single cluster and performs splitting until a stopping criterion is met.</li>\n</ul>\n\n<p>Essentially, this is the task of grouping your data points, based on something about them, such as closeness in space. Clustering is more of a tool to help you explore a dataset, and should not always be used as an automatic method to classify data. Hence, you may not always deploy a clustering algorithm for real-world production scenario. They are often too unreliable, and a single clustering alone will not be able to give you all the information you can extract from a dataset.</p>\n\n<h3 id=\"k-means\">K-Means</h3>\n\n<p>What we are going to do is group the tumor data points into two clusters using an algorithm called <code class=\"language-plaintext highlighter-rouge\">k-means</code>, which aims to cluster the data in order to minimize the variances of the clusters. The basic idea behind k-means clustering consists of defining clusters so that the total intra-cluster variation (known as total within-cluster variation) is minimized. There are several k-means algorithms available. However, the standard algorithm defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid:</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Let's cluster our data</hands-on-title>\n\n  <ol>\n    <li>\n      <p>Let’s cluster our data points (ignoring their know classes) using k-means and then we’ll compare the results to the actual labels that we know:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">set.seed</span><span class=\"p\">(</span><span class=\"m\">1</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">km.out</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">kmeans</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"p\">[</span><span class=\"m\">3</span><span class=\"o\">:</span><span class=\"n\">ncol</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"p\">)],</span><span class=\"w\"> </span><span class=\"n\">centers</span><span class=\"o\">=</span><span class=\"m\">2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">nstart</span><span class=\"o\">=</span><span class=\"m\">20</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>The <code class=\"language-plaintext highlighter-rouge\">nstart</code> option attempts multiple initial configurations and reports on the best one within the kmeans function. Seeds allow us to create a starting point for randomly generated numbers, so that each time our code is run, the same answer is generated.\nAlso, note that k-means requires the number of clusters to be defined beforehand and given via the <code class=\"language-plaintext highlighter-rouge\">centers</code> option.</p>\n    </li>\n    <li>\n      <p>Let’s check now what the output contains:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">str</span><span class=\"p\">(</span><span class=\"n\">km.out</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>The output will be:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>List of 9\n $ cluster     : int [1:569] 2 2 2 1 2 1 2 1 1 1 ...\n $ centers     : num [1:2, 1:30] 12.6 19.4 18.6 21.7 81.1 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"1\" \"2\"\n  .. ..$ : chr [1:30] \"Radius.Mean\" \"Texture.Mean\" \"Perimeter.Mean\" \"Area.Mean\" ...\n $ totss       : num 2.57e+08\n $ withinss    : num [1:2] 28559677 49383423\n $ tot.withinss: num 77943100\n $ betweenss   : num 1.79e+08\n $ size        : int [1:2] 438 131\n $ iter        : int 1\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n</code></pre></div>      </div>\n\n      <p>The information contained here is:</p>\n      <ul>\n        <li><code class=\"language-plaintext highlighter-rouge\">$cluster</code>: a vector of integers (from 1:k) indicating the cluster to which each point is allocated.</li>\n        <li><code class=\"language-plaintext highlighter-rouge\">$centers</code>: a matrix of cluster centers.</li>\n        <li><code class=\"language-plaintext highlighter-rouge\">$withinss</code>: vector of within-cluster sum of squares, one component per cluster.</li>\n        <li><code class=\"language-plaintext highlighter-rouge\">$tot.withinss</code>: total within-cluster sum of squares (i.e. <code class=\"language-plaintext highlighter-rouge\">sum(withinss)</code>).</li>\n        <li><code class=\"language-plaintext highlighter-rouge\">$size</code>: the number of points in each cluster.</li>\n      </ul>\n    </li>\n    <li>\n      <p>Let’s have a look at the clusters, and we will do this in relationship to the principal components we identified earlier:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">ggplot</span><span class=\"p\">(</span><span class=\"n\">as.data.frame</span><span class=\"p\">(</span><span class=\"n\">ppv_pca</span><span class=\"o\">$</span><span class=\"n\">x</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">aes</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">PC1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">PC2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">as.factor</span><span class=\"p\">(</span><span class=\"n\">km.out</span><span class=\"o\">$</span><span class=\"n\">cluster</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">shape</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">breastCancerData</span><span class=\"o\">$</span><span class=\"n\">Diagnosis</span><span class=\"p\">))</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">geom_point</span><span class=\"p\">(</span><span class=\"w\"> </span><span class=\"n\">alpha</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">0.6</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">size</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">3</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">theme_minimal</span><span class=\"p\">()</span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">theme</span><span class=\"p\">(</span><span class=\"n\">legend.position</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"bottom\"</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">labs</span><span class=\"p\">(</span><span class=\"n\">title</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"K-Means clusters against PCA\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"PC1\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"PC2\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">color</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Cluster\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">shape</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Diagnosis\"</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <figure id=\"figure-5\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/kmeans-pc12-Visualization.png\" alt=\"Visualization of the k-means results against the first two PCs on the UCI Breast Cancer dataset. \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/kmeans-pc12-Visualization.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 5</strong>:</span> Visualization of the k-means results against the first two PCs on the UCI Breast Cancer dataset</figcaption></figure>\n\n      <p>This is a rather complex plotting command that is based on the <code class=\"language-plaintext highlighter-rouge\">ggplot</code> library. For an overview of how <code class=\"language-plaintext highlighter-rouge\">ggplot</code> works, have a look at the <a href=\"/training-material/topics/transcriptomics/tutorials/rna-seq-counts-to-viz-in-r/tutorial.html\">RNA Seq Counts to Viz in R</a> tutorial.</p>\n    </li>\n    <li>\n      <p>Now that we have a cluster for each tumor (clusters 1 and 2), we can check how well they coincide with the labels that we know. To do this we will use a cool method called <strong>cross-tabulation</strong>: a cross-tab is a table that allows you to read off how many data points in clusters 1 and 2 were actually benign or malignant respectively.</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Cross-tab of clustering &amp; known labels</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">gmodels</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">CrossTable</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"o\">$</span><span class=\"n\">Diagnosis</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">km.out</span><span class=\"o\">$</span><span class=\"n\">cluster</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>The output should look like this:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n\nTotal Observations in Table:  569\n\n\n                           | km.out$cluster\nbreastCancerData$Diagnosis |         1 |         2 | Row Total |\n---------------------------|-----------|-----------|-----------|\n                         B |       356 |         1 |       357 |\n                           |    23.988 |    80.204 |           |\n                           |     0.997 |     0.003 |     0.627 |\n                           |     0.813 |     0.008 |           |\n                           |     0.626 |     0.002 |           |\n---------------------------|-----------|-----------|-----------|\n                         M |        82 |       130 |       212 |\n                           |    40.395 |   135.060 |           |\n                           |     0.387 |     0.613 |     0.373 |\n                           |     0.187 |     0.992 |           |\n                           |     0.144 |     0.228 |           |\n---------------------------|-----------|-----------|-----------|\n              Column Total |       438 |       131 |       569 |\n                           |     0.770 |     0.230 |           |\n---------------------------|-----------|-----------|-----------|\n</code></pre></div>      </div>\n\n      <p><em>Question: <strong>How well did the clustering work?</strong></em></p>\n    </li>\n  </ol>\n\n</blockquote>\n\n<h4 id=\"optimal-k\">Optimal k</h4>\n\n<p>One technique to choose the best <code class=\"language-plaintext highlighter-rouge\">k</code> is called the <strong>elbow method</strong>. This method uses within-group homogeneity or within-group heterogeneity to evaluate the variability. In other words, you are interested in the percentage of the variance explained by each cluster. You can expect the variability to increase with the number of clusters, alternatively, heterogeneity decreases. Our challenge is to find the <code class=\"language-plaintext highlighter-rouge\">k</code> that is beyond the diminishing returns. Adding a new cluster does not improve the variability in the data because very few information is left to explain.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Finding the optimal k</hands-on-title>\n\n  <ol>\n    <li>\n      <p>First of all, let’s create a function that computes the total within clusters sum of squares:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">kmean_withinss</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"k\">function</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"n\">cluster</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">kmeans</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"p\">[</span><span class=\"m\">3</span><span class=\"o\">:</span><span class=\"n\">ncol</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"p\">)],</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"p\">)</span><span class=\"w\">\n  </span><span class=\"n\">return</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"n\">cluster</span><span class=\"o\">$</span><span class=\"n\">tot.withinss</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n    <li>\n      <p>We can try for a single <code class=\"language-plaintext highlighter-rouge\">k</code> (e.g. 2), and see the value:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">kmean_withinss</span><span class=\"p\">(</span><span class=\"m\">2</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>[1] 77943100\n</code></pre></div>      </div>\n    </li>\n    <li>\n      <p>However, we need to test this <code class=\"language-plaintext highlighter-rouge\">n</code> times. We will use the <code class=\"language-plaintext highlighter-rouge\">sapply()</code> function to run the algorithm over a range of <code class=\"language-plaintext highlighter-rouge\">k</code>. This technique is faster than creating a loop and store the value each time.</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Set maximum cluster</span><span class=\"w\">\n</span><span class=\"n\">max_k</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"m\">-20</span><span class=\"w\">\n</span><span class=\"c1\"># Run algorithm over a range of k</span><span class=\"w\">\n</span><span class=\"n\">wss</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">sapply</span><span class=\"p\">(</span><span class=\"m\">2</span><span class=\"o\">:</span><span class=\"n\">max_k</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">kmean_withinss</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n    <li>\n      <p>Finally, let’s save the results into a data frame, so that we can work with it:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Create a data frame to plot the graph</span><span class=\"w\">\n</span><span class=\"n\">elbow</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"n\">data.frame</span><span class=\"p\">(</span><span class=\"m\">2</span><span class=\"o\">:</span><span class=\"n\">max_k</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">wss</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n    <li>\n      <p>Now that we have the data, we can plot them and try to identify the “elbow” point:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Plot the graph with gglop</span><span class=\"w\">\n</span><span class=\"n\">ggplot</span><span class=\"p\">(</span><span class=\"n\">elbow</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">aes</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">X2.max_k</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">wss</span><span class=\"p\">))</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">geom_point</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">geom_line</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">scale_x_continuous</span><span class=\"p\">(</span><span class=\"n\">breaks</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">seq</span><span class=\"p\">(</span><span class=\"m\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"m\">20</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">by</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">1</span><span class=\"p\">))</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <figure id=\"figure-6\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/elbow-plot-kmeans.png\" alt=\"Elbow plot for multiple values of k. \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/elbow-plot-kmeans.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 6</strong>:</span> Elbow plot for multiple values of k</figcaption></figure>\n\n      <blockquote class=\"question\">\n        <question-title></question-title>\n\n        <p>What is the optimal <code class=\"language-plaintext highlighter-rouge\">k</code> value?</p>\n\n        <blockquote class=\"solution\">\n          <solution-title></solution-title>\n\n          <p>From the graph, you can see the optimal <code class=\"language-plaintext highlighter-rouge\">k</code> is around 10, where the curve is starting to have a diminishing return.</p>\n\n        </blockquote>\n      </blockquote>\n    </li>\n  </ol>\n\n</blockquote>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <ol>\n    <li>Try re-running the clustering step with the new k. Is there a significant difference?</li>\n    <li>Try to think of alternative metrics that could be used as a “distance” measure, instead of the default “Euclidean”. Do you think there might be an optimal for our case?</li>\n  </ol>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <p>TODO</p>\n\n  </blockquote>\n</blockquote>\n\n<h3 id=\"hierarchical-clustering\">Hierarchical clustering</h3>\n\n<p>k-means clustering requires us to specify the number of clusters, and determining the optimal number of clusters is often not trivial. Hierarchical clustering is an alternative approach which builds a hierarchy from the bottom-up, and doesn’t require us to specify the number of clusters beforehand but requires extra steps to extract final clusters.\nThe algorithm works as follows:</p>\n\n<ul>\n  <li>Put each data point in its own cluster.</li>\n  <li>Identify the closest two clusters and combine them into one cluster.</li>\n  <li>Repeat the above step till all the data points are in a single cluster.</li>\n</ul>\n\n<p>Once this is done, it is usually represented by a dendrogram like structure. There are a few ways to determine how close two clusters are:</p>\n\n<ol>\n  <li><strong>Complete linkage clustering</strong>: Find the maximum possible distance between points belonging to two different clusters.</li>\n  <li><strong>Single linkage clustering</strong>: Find the minimum possible distance between points belonging to two different clusters.</li>\n  <li><strong>Mean linkage clustering</strong>: Find all possible pairwise distances for points belonging to two different clusters and then calculate the average.</li>\n  <li><strong>Centroid linkage clustering</strong>: Find the centroid of each cluster and calculate the distance between centroids of two clusters.</li>\n</ol>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>k-means Clustering</hands-on-title>\n  <ol>\n    <li>\n      <p>We will be applying Hierarchical clustering to our dataset, and see what the result might be. Remember that our dataset has some columns with nominal (categorical) values (columns <code class=\"language-plaintext highlighter-rouge\">ID</code> and <code class=\"language-plaintext highlighter-rouge\">Diagnosis</code>), so we will need to make sure we only use the columns with numerical values. There are no missing values in this dataset that we need to clean before clustering. But the scales of the features are different and we need to normalize it.</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">breastCancerDataScaled</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">as.data.frame</span><span class=\"p\">(</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"p\">[</span><span class=\"m\">3</span><span class=\"o\">:</span><span class=\"n\">ncol</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"p\">)]))</span><span class=\"w\">\n</span><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">breastCancerDataScaled</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n    <li>\n      <p>We can now proceed with creating the distance matrix:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">dist_mat</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">dist</span><span class=\"p\">(</span><span class=\"n\">breastCancerDataScaled</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">method</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s1\">'euclidean'</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>There are several options for <code class=\"language-plaintext highlighter-rouge\">method</code>: <code class=\"language-plaintext highlighter-rouge\">euclidean</code>, <code class=\"language-plaintext highlighter-rouge\">maximum</code>, <code class=\"language-plaintext highlighter-rouge\">manhattan</code>, <code class=\"language-plaintext highlighter-rouge\">canberra</code>, <code class=\"language-plaintext highlighter-rouge\">binary</code> or <code class=\"language-plaintext highlighter-rouge\">minkowski</code>.</p>\n    </li>\n    <li>\n      <p>The next step is to actually perform the hierarchical clustering, which means that at this point we should decide which linkage method we want to use. We can try all kinds of linkage methods and later decide on which one performed better. Here we will proceed with <code class=\"language-plaintext highlighter-rouge\">average</code> linkage method (i.e. UPGMA); other methods include <code class=\"language-plaintext highlighter-rouge\">ward.D</code>, <code class=\"language-plaintext highlighter-rouge\">ward.D2</code>, <code class=\"language-plaintext highlighter-rouge\">single</code>, <code class=\"language-plaintext highlighter-rouge\">complete</code>, <code class=\"language-plaintext highlighter-rouge\">mcquitty</code> (= WPGMA), <code class=\"language-plaintext highlighter-rouge\">median</code> (= WPGMC) and <code class=\"language-plaintext highlighter-rouge\">centroid</code> (= UPGMC).</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">hclust_avg</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">hclust</span><span class=\"p\">(</span><span class=\"n\">dist_mat</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">method</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s1\">'average'</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">hclust_avg</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <figure id=\"figure-7\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/hclust-fig1.png\" alt=\"Hierarchical clustering (attempt 1). \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/hclust-fig1.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 7</strong>:</span> Hierarchical clustering (attempt 1)</figcaption></figure>\n\n      <p>Notice how the dendrogram is built and every data point finally merges into a single cluster with the height(distance) shown on the y-axis.</p>\n    </li>\n    <li>\n      <p>Next, we can cut the dendrogram in order to create the desired number of clusters. In our case, we might want to check whether our two groups (<code class=\"language-plaintext highlighter-rouge\">M</code> and <code class=\"language-plaintext highlighter-rouge\">B</code>) can be identified as sub-trees of our clustering - so we’ll set <code class=\"language-plaintext highlighter-rouge\">k = 2</code> and then plot the result.</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">cut_avg</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">cutree</span><span class=\"p\">(</span><span class=\"n\">hclust_avg</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">2</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">hclust_avg</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">labels</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">breastCancerData</span><span class=\"o\">$</span><span class=\"n\">ID</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">hang</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">-1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">cex</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">0.2</span><span class=\"p\">,</span><span class=\"w\">\n     </span><span class=\"n\">main</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Cluster dendrogram (k = 2)\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">xlab</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Breast Cancer ID\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">ylab</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Height\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"c1\"># k: Cut the dendrogram such that exactly k clusters are produced</span><span class=\"w\">\n</span><span class=\"c1\"># border: Vector with border colors for the rectangles. Coild also be a number vector 1:2</span><span class=\"w\">\n</span><span class=\"c1\"># which: A vector selecting the clusters around which a rectangle should be drawn (numbered from left to right)</span><span class=\"w\">\n</span><span class=\"n\">rect.hclust</span><span class=\"p\">(</span><span class=\"n\">hclust_avg</span><span class=\"w\"> </span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">border</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"s2\">\"red\"</span><span class=\"p\">,</span><span class=\"s2\">\"green\"</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">which</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"m\">1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"m\">2</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"c1\"># Draw a line at the height that the cut takes place</span><span class=\"w\">\n</span><span class=\"n\">abline</span><span class=\"p\">(</span><span class=\"n\">h</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">18</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">col</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s1\">'red'</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">lwd</span><span class=\"o\">=</span><span class=\"m\">3</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">lty</span><span class=\"o\">=</span><span class=\"m\">2</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n      <figure id=\"figure-8\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/hclust-fig2.png\" alt=\"Hierarchical clustering (attempt 2). \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/hclust-fig2.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 8</strong>:</span> Hierarchical clustering (attempt 2)</figcaption></figure>\n    </li>\n    <li>\n      <p>Now we can see the two clusters enclosed in two different colored boxes. We can also use the <code class=\"language-plaintext highlighter-rouge\">color_branches()</code> function from the <code class=\"language-plaintext highlighter-rouge\">dendextend</code> library to visualize our tree with different colored branches.</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">dendextend</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">avg_dend_obj</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">as.dendrogram</span><span class=\"p\">(</span><span class=\"n\">hclust_avg</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"c1\"># We can use either k (number of clusters), or clusters (and specify the cluster type)</span><span class=\"w\">\n</span><span class=\"n\">avg_col_dend</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">color_branches</span><span class=\"p\">(</span><span class=\"n\">avg_dend_obj</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">k</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">groupLabels</span><span class=\"o\">=</span><span class=\"kc\">TRUE</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">avg_col_dend</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Cluster dendrogram with color per cluster (k = 2)\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">xlab</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Breast Cancer ID\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">ylab</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Height\"</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <figure id=\"figure-9\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/hclust-fig3.png\" alt=\"Hierarchical clustering (attempt 3). \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/hclust-fig3.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 9</strong>:</span> Hierarchical clustering (attempt 3)</figcaption></figure>\n    </li>\n    <li>\n      <p>We can change the way branches are colored, to reflect the <code class=\"language-plaintext highlighter-rouge\">Diagnosis</code> value:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">avg_col_dend</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">color_branches</span><span class=\"p\">(</span><span class=\"n\">avg_dend_obj</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">clusters</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">breastCancerData</span><span class=\"o\">$</span><span class=\"n\">Diagnosis</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">avg_col_dend</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Cluster dendrogram with Diagnosis color\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">xlab</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Breast Cancer ID\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">ylab</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Height\"</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <figure id=\"figure-10\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/hclust-fig4.png\" alt=\"Hierarchical clustering (attempt 4). \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/hclust-fig4.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 10</strong>:</span> Hierarchical clustering (attempt 4)</figcaption></figure>\n    </li>\n    <li>\n      <p>TODO? Step Title</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">ggplot</span><span class=\"p\">(</span><span class=\"n\">as.data.frame</span><span class=\"p\">(</span><span class=\"n\">ppv_pca</span><span class=\"o\">$</span><span class=\"n\">x</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">aes</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">PC1</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">PC2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">as.factor</span><span class=\"p\">(</span><span class=\"n\">cut_avg</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">shape</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">breastCancerData</span><span class=\"o\">$</span><span class=\"n\">Diagnosis</span><span class=\"p\">))</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">geom_point</span><span class=\"p\">(</span><span class=\"w\"> </span><span class=\"n\">alpha</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">0.6</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">size</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">3</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">theme_minimal</span><span class=\"p\">()</span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">theme</span><span class=\"p\">(</span><span class=\"n\">legend.position</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"bottom\"</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">labs</span><span class=\"p\">(</span><span class=\"n\">title</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Hierarchical clustering (cut at k=2) against PCA\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"PC1\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"PC2\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">color</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Cluster\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">shape</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Diagnosis\"</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <figure id=\"figure-11\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/hclust-pc12-Visualization.png\" alt=\"Visualization of the Hierarchical clustering (cut at k=2) results against the first two PCs on the UCI Breast Cancer dataset. \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/hclust-pc12-Visualization.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 11</strong>:</span> Visualization of the Hierarchical clustering (cut at k=2) results against the first two PCs on the UCI Breast Cancer dataset</figcaption></figure>\n    </li>\n  </ol>\n\n</blockquote>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <ol>\n    <li>The hierarchical clustering performed so far, only used two methods: <code class=\"language-plaintext highlighter-rouge\">euclidean</code> and <code class=\"language-plaintext highlighter-rouge\">average</code>. Try experimenting with different methods. Do the final results improve?</li>\n    <li>Obviously the cut-off selection (k=2) was not optimal. Try using different cut-offs to ensure that the final clustering could provide some context to the original question.</li>\n  </ol>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <p>TODO</p>\n\n  </blockquote>\n</blockquote>\n\n<h1 id=\"supervised-learning\">Supervised Learning</h1>\n\n<p>Supervised learning is the branch of Machine Learning (ML) that involves predicting labels, such as ‘Survived’ or ‘Not’. Such models learn from labelled data, which is data that includes whether a passenger survived (called “model training”), and then predict on unlabeled data.</p>\n\n<p>These are generally called train and test sets because</p>\n<ul>\n  <li>You want to build a model that learns patterns in the training set, and</li>\n  <li>You then use the model to make predictions on the test set.</li>\n</ul>\n\n<p>We can then calculate the percentage that you got correct: this is known as the accuracy of your model.</p>\n\n<h2 id=\"how-to-start-with-supervised-learning\">How To Start with Supervised Learning</h2>\n\n<p>As you might already know, a good way to approach supervised learning is the following:</p>\n<ul>\n  <li>Perform an Exploratory Data Analysis (EDA) on your data set;</li>\n  <li>Build a quick and dirty model, or a baseline model, which can serve as a comparison against later models that you will build;</li>\n  <li>Iterate this process. You will do more EDA and build another model;</li>\n  <li>Engineer features: take the features that you already have and combine them or extract more information from them to eventually come to the last point, which is</li>\n  <li>Get a model that performs better.</li>\n</ul>\n\n<p>A common practice in all supervised learning is the construction and use of the <strong>train- and test- datasets</strong>. This process takes all of the input randomly splits into the two datasets (training and test); the ratio of the split is usually up to the researcher, and can be anything: 80/20, 70/30, 60/40…</p>\n\n<h2 id=\"supervised-learning-i-classification\">Supervised Learning I: classification</h2>\n\n<p>There are various classifiers available:</p>\n\n<ul>\n  <li><strong>Decision Trees</strong> – These are organized in the form of sets of questions and answers in the tree structure.</li>\n  <li><strong>Naive Bayes Classifiers</strong> – A probabilistic machine learning model that is used for classification.</li>\n  <li><strong>K-NN Classifiers</strong> – Based on the similarity measures like distance, it classifies new cases.</li>\n  <li><strong>Support Vector Machines</strong> – It is a non-probabilistic binary classifier that builds a model to classify a case into one of the two categories. They rely on a <code class=\"language-plaintext highlighter-rouge\">kernel</code> function that essentially projects the data points to higher-dimensional space; depending on this new space, there can be both linear and non-linear SVMs.</li>\n</ul>\n\n<h3 id=\"decision-trees\">Decision trees</h3>\n\n<p>It is a type of supervised learning algorithm. We use it for classification problems. It works for both types of input and output variables. In this technique, we split the population into two or more homogeneous sets. Moreover, it is based on the most significant splitter/differentiator in input variables.</p>\n\n<p>The Decision Tree is a powerful non-linear classifier. A Decision Tree makes use of a tree-like structure to generate relationship among the various features and potential outcomes. It makes use of branching decisions as its core structure.</p>\n\n<p>There are two types of decision trees:</p>\n<ul>\n  <li><strong>Categorical (classification)</strong> Variable Decision Tree: Decision Tree which has a categorical target variable.</li>\n  <li><strong>Continuous (Regression)</strong> Variable Decision Tree: Decision Tree has a continuous target variable.</li>\n</ul>\n\n<p>Regression trees are used when the dependent variable is continuous while classification trees are used when the dependent variable is categorical. In continuous, a value obtained is a mean response of observation. In classification, a value obtained by a terminal node is a mode of observations.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Decision Trees</hands-on-title>\n  <ol>\n    <li>\n      <p>Here, we will use the <code class=\"language-plaintext highlighter-rouge\">rpart</code> and the <code class=\"language-plaintext highlighter-rouge\">rpart.plot</code> package in order to produce and visualize a decision tree. First of all, we’ll create the train and test datasets using a 70/30 ratio and a fixed seed so that we can reproduce the results.</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># split into training and test subsets</span><span class=\"w\">\n</span><span class=\"n\">set.seed</span><span class=\"p\">(</span><span class=\"m\">1000</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">ind</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"m\">2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">nrow</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">replace</span><span class=\"o\">=</span><span class=\"kc\">TRUE</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">prob</span><span class=\"o\">=</span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"m\">0.7</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"m\">0.3</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">breastCancerData.train</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">breastCancerDataNoID</span><span class=\"p\">[</span><span class=\"n\">ind</span><span class=\"o\">==</span><span class=\"m\">1</span><span class=\"p\">,]</span><span class=\"w\">\n</span><span class=\"n\">breastCancerData.test</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">breastCancerDataNoID</span><span class=\"p\">[</span><span class=\"n\">ind</span><span class=\"o\">==</span><span class=\"m\">2</span><span class=\"p\">,]</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n    <li>\n      <p>Now, we will load the library and create our model. We would like to create a model that predicts the <code class=\"language-plaintext highlighter-rouge\">Diagnosis</code> based on the mean of the radius and the area, as well as the SE of the texture. For ths reason we’ll use the notation of <code class=\"language-plaintext highlighter-rouge\">myFormula &lt;- Diagnosis ~ Radius.Mean + Area.Mean + Texture.SE</code>. If we wanted to create a prediction model based on all variables, we will have used <code class=\"language-plaintext highlighter-rouge\">myFormula &lt;- Diagnosis ~ .</code> instead. Finally, <code class=\"language-plaintext highlighter-rouge\">minsplit</code> stands for the the minimum number of instances in a node so that it is split.</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">rpart</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">rpart.plot</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">myFormula</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">Diagnosis</span><span class=\"w\"> </span><span class=\"o\">~</span><span class=\"w\"> </span><span class=\"n\">Radius.Mean</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">Area.Mean</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">Texture.SE</span><span class=\"w\">\n\n</span><span class=\"n\">breastCancerData.model</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">rpart</span><span class=\"p\">(</span><span class=\"n\">myFormula</span><span class=\"p\">,</span><span class=\"w\">\n                                </span><span class=\"n\">method</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"class\"</span><span class=\"p\">,</span><span class=\"w\">\n                                </span><span class=\"n\">data</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">breastCancerData.train</span><span class=\"p\">,</span><span class=\"w\">\n                                </span><span class=\"n\">minsplit</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">10</span><span class=\"p\">,</span><span class=\"w\">\n                                </span><span class=\"n\">minbucket</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">1</span><span class=\"p\">,</span><span class=\"w\">\n                                </span><span class=\"n\">maxdepth</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">3</span><span class=\"p\">,</span><span class=\"w\">\n                                </span><span class=\"n\">cp</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">-1</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">print</span><span class=\"p\">(</span><span class=\"n\">breastCancerData.model</span><span class=\"o\">$</span><span class=\"n\">cptable</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">rpart.plot</span><span class=\"p\">(</span><span class=\"n\">breastCancerData.model</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n    <li>\n      <p>We see the following output and a figure:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>      CP       nsplit rel error   xerror     xstd\n1  0.69930070      0 1.0000000 1.0000000 0.06688883\n2  0.02797203      1 0.3006993 0.3006993 0.04330166\n3  0.00000000      2 0.2727273 0.3006993 0.04330166\n4 -1.00000000      6 0.2727273 0.3006993 0.04330166\n</code></pre></div>      </div>\n\n      <figure id=\"figure-12\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/decisionTreeFull.png\" alt=\"Full decision tree. \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/decisionTreeFull.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 12</strong>:</span> Full decision tree</figcaption></figure>\n\n      <p>The parameters that we used reflect the following aspects of the model:</p>\n      <ul>\n        <li><code class=\"language-plaintext highlighter-rouge\">minsplit</code>: the minimum number of instances in a node so that it is split</li>\n        <li><code class=\"language-plaintext highlighter-rouge\">minbucket</code>: the minimum allowed number of instances in each leaf of the tree</li>\n        <li><code class=\"language-plaintext highlighter-rouge\">maxdepth</code>: the maximum depth of the tree</li>\n        <li><code class=\"language-plaintext highlighter-rouge\">cp</code>: parameter that controls the complexity for a split and is set intuitively (the larger its value, the more probable to apply pruning to the tree)</li>\n      </ul>\n    </li>\n    <li>\n      <p>As we can observe, this might not be the best model. So we can select the tree with the minimum prediction error:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">opt</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">which.min</span><span class=\"p\">(</span><span class=\"n\">breastCancerData.model</span><span class=\"o\">$</span><span class=\"n\">cptable</span><span class=\"p\">[,</span><span class=\"w\"> </span><span class=\"s2\">\"xerror\"</span><span class=\"p\">])</span><span class=\"w\">\n</span><span class=\"n\">cp</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">breastCancerData.model</span><span class=\"o\">$</span><span class=\"n\">cptable</span><span class=\"p\">[</span><span class=\"n\">opt</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s2\">\"CP\"</span><span class=\"p\">]</span><span class=\"w\">\n</span><span class=\"c1\"># prune tree</span><span class=\"w\">\n</span><span class=\"n\">breastCancerData.pruned.model</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">prune</span><span class=\"p\">(</span><span class=\"n\">breastCancerData.model</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">cp</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">cp</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"c1\"># plot tree</span><span class=\"w\">\n</span><span class=\"n\">rpart.plot</span><span class=\"p\">(</span><span class=\"n\">breastCancerData.pruned.model</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">table</span><span class=\"p\">(</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">breastCancerData.pruned.model</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">type</span><span class=\"o\">=</span><span class=\"s2\">\"class\"</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">breastCancerData.train</span><span class=\"o\">$</span><span class=\"n\">Diagnosis</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>The output now is the following Confusion Matrix and pruned tree:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>    B    M\nB  245  34\nM   9   109\n</code></pre></div>      </div>\n\n      <figure id=\"figure-13\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/decisionTreePruned.png\" alt=\"Pruned decision tree. \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/decisionTreePruned.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 13</strong>:</span> Pruned decision tree</figcaption></figure>\n\n      <blockquote class=\"question\">\n        <question-title></question-title>\n\n        <p>What does the above “Confusion Matrix” tells you?</p>\n\n        <blockquote class=\"solution\">\n          <solution-title></solution-title>\n\n          <p>TODO</p>\n\n        </blockquote>\n      </blockquote>\n    </li>\n    <li>\n      <p>Now that we have a model, we should check how the prediction works in our test dataset.</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">## make prediction</span><span class=\"w\">\n</span><span class=\"n\">BreastCancer_pred</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">breastCancerData.pruned.model</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">newdata</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">breastCancerData.test</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">type</span><span class=\"o\">=</span><span class=\"s2\">\"class\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">BreastCancer_pred</span><span class=\"w\"> </span><span class=\"o\">~</span><span class=\"w\"> </span><span class=\"n\">Diagnosis</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">data</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">breastCancerData.test</span><span class=\"p\">,</span><span class=\"w\">\n     </span><span class=\"n\">xlab</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Observed\"</span><span class=\"p\">,</span><span class=\"w\">\n     </span><span class=\"n\">ylab</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Prediction\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">table</span><span class=\"p\">(</span><span class=\"n\">BreastCancer_pred</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">breastCancerData.test</span><span class=\"o\">$</span><span class=\"n\">Diagnosis</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>The new Confusion Matrix is the following:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>BreastCancer_pred   B   M\n                B 102  16\n                M   1  53\n</code></pre></div>      </div>\n\n      <figure id=\"figure-14\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/predictionPlot.png\" alt=\"Prediction Plot. \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/predictionPlot.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 14</strong>:</span> Prediction Plot</figcaption></figure>\n    </li>\n  </ol>\n</blockquote>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <ol>\n    <li>Can we improve the above model? What are the key parameters that have the most impact?</li>\n    <li>We have been using only some of the variables in our model. What is the impact of using all variables / features for our prediction? Is this a good or a bad plan?</li>\n  </ol>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <p>TODO</p>\n\n  </blockquote>\n</blockquote>\n\n<h3 id=\"random-forests\">Random Forests</h3>\n\n<p>Random Forests is an ensemble learning technique, which essentially constructs multiple decision trees. Each tree is trained with a random sample of the training dataset and on a randomly chosen subspace. The final prediction result is derived from the predictions of all individual trees, with mean (for regression) or majority voting (for classification). The advantage is that it has better performance and is less likely to overfit than a single decision tree; however it has lower interpretability.</p>\n\n<p>There are two main libraries in R that provide the functionality for Random Forest creation; the <code class=\"language-plaintext highlighter-rouge\">randomForest</code> and the <code class=\"language-plaintext highlighter-rouge\">party: cforest()</code>.</p>\n\n<p>Package <code class=\"language-plaintext highlighter-rouge\">randomForest</code></p>\n<ul>\n  <li>very fast</li>\n  <li>cannot handle data with missing values</li>\n  <li>a limit of 32 to the maximum number of levels of each categorical attribute</li>\n  <li>extensions: extendedForest, gradientForest</li>\n</ul>\n\n<p>Package <code class=\"language-plaintext highlighter-rouge\">party: cforest()</code></p>\n<ul>\n  <li>not limited to the above maximum levels</li>\n  <li>slow</li>\n  <li>needs more memory</li>\n</ul>\n\n<p>In this exercise, we will be using the <code class=\"language-plaintext highlighter-rouge\">randomForest</code>.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Random Forests</hands-on-title>\n  <ol>\n    <li>\n      <p>First, let’s train the model:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">library</span><span class=\"p\">(</span><span class=\"n\">randomForest</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">set.seed</span><span class=\"p\">(</span><span class=\"m\">1000</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">rf</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">randomForest</span><span class=\"p\">(</span><span class=\"n\">Diagnosis</span><span class=\"w\"> </span><span class=\"o\">~</span><span class=\"w\"> </span><span class=\"n\">.</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">data</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">breastCancerData.train</span><span class=\"p\">,</span><span class=\"w\">\n                   </span><span class=\"n\">ntree</span><span class=\"o\">=</span><span class=\"m\">100</span><span class=\"p\">,</span><span class=\"w\">\n                   </span><span class=\"n\">proximity</span><span class=\"o\">=</span><span class=\"nb\">T</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">table</span><span class=\"p\">(</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">rf</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">breastCancerData.train</span><span class=\"o\">$</span><span class=\"n\">Diagnosis</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>The output is the following:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>   B   M\nB 249  12\nM   5 131\n</code></pre></div>      </div>\n    </li>\n    <li>\n      <p>We can also investigate the content of the model:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">print</span><span class=\"p\">(</span><span class=\"n\">rf</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>The output shows the individual components and internal parameters of the Random Forest model.</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Call:\n randomForest(formula = Diagnosis ~ ., data = breastCancerData.train,      ntree = 100, proximity = T)\n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 5\n\n        OOB estimate of  error rate: 4.28%\nConfusion matrix:\n    B   M class.error\nB 249   5  0.01968504\nM  12 131  0.08391608\n</code></pre></div>      </div>\n    </li>\n    <li>\n      <p>We can view the overall performance of the model here:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">rf</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">main</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"\"</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <figure id=\"figure-15\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/error-rate-rf.png\" alt=\"Error rate plot for the Random Forest model. \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/error-rate-rf.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 15</strong>:</span> Error rate plot for the Random Forest model</figcaption></figure>\n    </li>\n    <li>\n      <p>We can also review which of the variables has the highest “importance” (i.e. impact to the performance of the model):</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">importance</span><span class=\"p\">(</span><span class=\"n\">rf</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">varImpPlot</span><span class=\"p\">(</span><span class=\"n\">rf</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>The output is the table and the figure below:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>ID                             1.0244803\nRadius.Mean                    7.8983552\nTexture.Mean                   1.9614134\nPerimeter.Mean                 9.3502914\nArea.Mean                      7.3438007\nSmoothness.Mean                0.7228277\nCompactness.Mean               2.6595043\nConcavity.Mean                11.2341661\nConcave.Points.Mean           18.5940046\nSymmetry.Mean                  0.8989458\nFractal.Dimension.Mean         0.7465322\nRadius.SE                      3.1941672\nTexture.SE                     0.6363906\nPerimeter.SE                   2.4672730\nArea.SE                        5.3446273\nSmoothness.SE                  0.6089522\nCompactness.SE                 0.7785777\nConcavity.SE                   0.5576146\nConcave.Points.SE              1.0314107\nSymmetry.SE                    0.8839428\nFractal.Dimension.SE           0.6475348\nRadius.Worst                  18.2035365\nTexture.Worst                  3.2765864\nPerimeter.Worst               25.3605679\nArea.Worst                    17.1063000\nSmoothness.Worst               2.1677456\nCompactness.Worst              2.9489506\nConcavity.Worst                6.0009637\nConcave.Points.Worst          25.6081497\nSymmetry.Worst                 2.1507714\nFractal.Dimension.Worst        1.1498020\n</code></pre></div>      </div>\n\n      <figure id=\"figure-16\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/importance-variables.png\" alt=\"Importance of the individual variables. \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/importance-variables.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 16</strong>:</span> Importance of the individual variables</figcaption></figure>\n    </li>\n    <li>\n      <p>Let’s try to do a prediction of the <code class=\"language-plaintext highlighter-rouge\">Diagnosis</code> for the test set, using the new model. The margin of a data point is as the proportion of votes for the correct class minus maximum proportion of votes for other classes. Positive margin means correct classification.</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">BreastCancer_pred_RD</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">rf</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">newdata</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">breastCancerData.test</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">table</span><span class=\"p\">(</span><span class=\"n\">BreastCancer_pred_RD</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">breastCancerData.test</span><span class=\"o\">$</span><span class=\"n\">Diagnosis</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">margin</span><span class=\"p\">(</span><span class=\"n\">rf</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">breastCancerData.test</span><span class=\"o\">$</span><span class=\"n\">Diagnosis</span><span class=\"p\">))</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>The output is the table and figure below:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>BreastCancer_pred_RD   B   M\n                   B 101   6\n                   M   2  63\n</code></pre></div>      </div>\n\n      <figure id=\"figure-17\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/margin-rf.png\" alt=\"Margin plot for the Random Forest. \" width=\"1024\" height=\"677\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/margin-rf.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 17</strong>:</span> Margin plot for the Random Forest</figcaption></figure>\n    </li>\n    <li>\n      <p>Feature selection: We can evaluate the prediction performance of models with reduced numbers of variables that are ranked by their importance.</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">result</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">rfcv</span><span class=\"p\">(</span><span class=\"n\">breastCancerData.train</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">breastCancerData.train</span><span class=\"o\">$</span><span class=\"n\">Diagnosis</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">cv.fold</span><span class=\"o\">=</span><span class=\"m\">3</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">with</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">n.var</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">error.cv</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">log</span><span class=\"o\">=</span><span class=\"s2\">\"x\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">type</span><span class=\"o\">=</span><span class=\"s2\">\"o\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">lwd</span><span class=\"o\">=</span><span class=\"m\">2</span><span class=\"p\">))</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <figure id=\"figure-18\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/rfcv.png\" alt=\"Random Forest Cross-Valdidation for feature selection. \" width=\"483\" height=\"384\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/rfcv.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 18</strong>:</span> Random Forest Cross-Valdidation for feature selection</figcaption></figure>\n    </li>\n  </ol>\n\n</blockquote>\n\n<h2 id=\"supervised-learning-ii-regression\">Supervised Learning II: regression</h2>\n\n<h3 id=\"linear-regression\">Linear regression</h3>\n\n<p>Linear regression is to predict response with a linear function of predictors. The most common function in R for this is <code class=\"language-plaintext highlighter-rouge\">lm</code>. In our dataset, let’s try to investigate the relationship between <code class=\"language-plaintext highlighter-rouge\">Radius.Mean</code>, <code class=\"language-plaintext highlighter-rouge\">Concave.Points.Mean</code> and <code class=\"language-plaintext highlighter-rouge\">Area.Mean</code>.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Linear Regression</hands-on-title>\n  <ol>\n    <li>\n      <p>We can get a first impression by looking at the correlation of these variables:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">## correlation between Radius.Mean and Concave.Points.Mean / Area.Mean</span><span class=\"w\">\n</span><span class=\"n\">cor</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"o\">$</span><span class=\"n\">Radius.Mean</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">breastCancerData</span><span class=\"o\">$</span><span class=\"n\">Concave.Points.Mean</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"c1\">## [1] 0.8225285</span><span class=\"w\">\n</span><span class=\"n\">cor</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"o\">$</span><span class=\"n\">Concave.Points.Mean</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">breastCancerData</span><span class=\"o\">$</span><span class=\"n\">Area.Mean</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"c1\">## [1] 0.8232689</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n    <li>Lets create a short version of our data\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">bc</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"n\">breastCancerData</span><span class=\"p\">,</span><span class=\"n\">Radius.Mean</span><span class=\"p\">,</span><span class=\"n\">Concave.Points.Mean</span><span class=\"p\">,</span><span class=\"n\">Area.Mean</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n    <li>\n      <p>Let’s build now a linear regression model with function <code class=\"language-plaintext highlighter-rouge\">lm()</code> on the whole dataset:</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">bc_model_full</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">lm</span><span class=\"p\">(</span><span class=\"n\">Radius.Mean</span><span class=\"w\"> </span><span class=\"o\">~</span><span class=\"w\"> </span><span class=\"n\">Concave.Points.Mean</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">Area.Mean</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">bc</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">bc_model_full</span><span class=\"w\">\n</span></code></pre></div>      </div>\n\n      <p>The output is the following:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Call:\nlm(formula = Radius.Mean ~ ., data = bc)\n\nCoefficients:\n        (Intercept)  Concave.Points.Mean            Area.Mean\n            7.68087              2.72493              0.00964\n</code></pre></div>      </div>\n\n      <p>This tells us what are the coefficients of <code class=\"language-plaintext highlighter-rouge\">Concave.Points.Mean</code> and <code class=\"language-plaintext highlighter-rouge\">Area.Mean</code>, in the linear equation that connects them to <code class=\"language-plaintext highlighter-rouge\">Radius.Mean</code>. Let’s see if we can predict now the mean radius of a new sample, with <code class=\"language-plaintext highlighter-rouge\">Concave.Points.Mean</code> = 2.724931 and <code class=\"language-plaintext highlighter-rouge\">Area.Mean</code> = 0.00964.</p>\n    </li>\n    <li>Let’s make predictions on our training dataset and visualize\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">preds</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">bc_model_full</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">preds</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">bc</span><span class=\"o\">$</span><span class=\"n\">Radius.Mean</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">xlab</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Prediction\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">ylab</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"Observed\"</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">abline</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">0</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">b</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"m\">1</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n      <figure id=\"figure-19\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/lm_full_dataset.png\" alt=\"Prediction Plot GLM. \" width=\"630\" height=\"397\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/lm_full_dataset.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 19</strong>:</span> Prediction Plot GLM</figcaption></figure>\n    </li>\n    <li>\n      <p>We can also have a better look at what the model contains with <code class=\"language-plaintext highlighter-rouge\">summary(bc_model_full)</code>:</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Call:\nlm(formula = Radius.Mean ~ ., data = bc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max\n-4.8307 -0.1827  0.1497  0.3608  0.7411\n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)         7.6808702  0.0505533 151.936   &lt;2e-16 ***\nConcave.Points.Mean 2.7249328  1.0598070   2.571   0.0104 *\nArea.Mean           0.0096400  0.0001169  82.494   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.5563 on 566 degrees of freedom\nMultiple R-squared:  0.9752,\tAdjusted R-squared:  0.9751\nF-statistic: 1.111e+04 on 2 and 566 DF,  p-value: &lt; 2.2e-16\n</code></pre></div>      </div>\n    </li>\n    <li>\n      <p>But his only provides the evaluation on the whole dataset that we sued for training. we don’t know how it will perform on unknown dataset. So, let’s split our dataset into training and test set, create the model on training set and visualize the predictions</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">set.seed</span><span class=\"p\">(</span><span class=\"m\">123</span><span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"n\">ind</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">sample</span><span class=\"p\">(</span><span class=\"m\">2</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">nrow</span><span class=\"p\">(</span><span class=\"n\">bc</span><span class=\"p\">),</span><span class=\"w\"> </span><span class=\"n\">replace</span><span class=\"o\">=</span><span class=\"kc\">TRUE</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">prob</span><span class=\"o\">=</span><span class=\"nf\">c</span><span class=\"p\">(</span><span class=\"m\">0.75</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"m\">0.25</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"n\">bc_train</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">bc</span><span class=\"p\">[</span><span class=\"n\">ind</span><span class=\"o\">==</span><span class=\"m\">1</span><span class=\"p\">,]</span><span class=\"w\">\n</span><span class=\"n\">bc_test</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">bc</span><span class=\"p\">[</span><span class=\"n\">ind</span><span class=\"o\">==</span><span class=\"m\">2</span><span class=\"p\">,]</span><span class=\"w\">\n\n\n</span><span class=\"c1\">#Let's build now a linear regression model using the training data and print it:</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"n\">bc_model</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">lm</span><span class=\"p\">(</span><span class=\"n\">Radius.Mean</span><span class=\"w\"> </span><span class=\"o\">~</span><span class=\"w\"> </span><span class=\"n\">Concave.Points.Mean</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">Area.Mean</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">bc_train</span><span class=\"p\">))</span><span class=\"w\">\n\n</span><span class=\"c1\">#We can also view the model's summary</span><span class=\"w\">\n</span><span class=\"n\">summary</span><span class=\"p\">(</span><span class=\"n\">bc_model</span><span class=\"p\">)</span><span class=\"w\">\n\n\n</span><span class=\"c1\">######Evaluating graphically</span><span class=\"w\">\n</span><span class=\"c1\">#Let's make predictions on our training dataset and store the predictions as a new column</span><span class=\"w\">\n</span><span class=\"n\">bc_train</span><span class=\"o\">$</span><span class=\"n\">pred</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">bc_model</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"c1\"># plot the ground truths vs predictions for training set</span><span class=\"w\">\n</span><span class=\"n\">ggplot</span><span class=\"p\">(</span><span class=\"n\">bc_train</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">aes</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">pred</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">Radius.Mean</span><span class=\"p\">))</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">geom_point</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">geom_abline</span><span class=\"p\">(</span><span class=\"n\">color</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"blue\"</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n      <figure id=\"figure-20\" style=\"max-width: 90%;\"><img src=\"../../images/intro-to-ml-with-r/lm_train_dataset.png\" alt=\"Prediction Plot GLM. \" width=\"630\" height=\"397\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/intro-to-ml-with-r/lm_train_dataset.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 20</strong>:</span> Prediction Plot GLM</figcaption></figure>\n\n      <p>You will note that it is quite similar to when using whole dataset</p>\n    </li>\n    <li>Let’s predict using test data\n      <div class=\"language-R highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">bc_test</span><span class=\"o\">$</span><span class=\"n\">pred</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">bc_model</span><span class=\"w\"> </span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">newdata</span><span class=\"o\">=</span><span class=\"n\">bc_test</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n    <li>\n      <p>and plot</p>\n\n      <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># plot the ground truths vs predictions for test set and examine the plot. Does it look as good with the predictions on the training set?</span><span class=\"w\">\n</span><span class=\"n\">ggplot</span><span class=\"p\">(</span><span class=\"n\">bc_test</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">aes</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">pred</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">y</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">Radius.Mean</span><span class=\"p\">))</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">geom_point</span><span class=\"p\">()</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\">\n  </span><span class=\"n\">geom_abline</span><span class=\"p\">(</span><span class=\"n\">color</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"s2\">\"blue\"</span><span class=\"p\">)</span><span class=\"w\">\n</span></code></pre></div>      </div>\n    </li>\n  </ol>\n\n</blockquote>\n\n<p>Now let’s use the RMSE and the R_square metrics to evaluate our model on the training and test set. R_square measures how much of variability in dependent variable can be explained by the model. It is defined as the square of the correlation coefficient (<code class=\"language-plaintext highlighter-rouge\">R</code>), and that is why it is called “R Square” (more info <a href=\"https://en.wikipedia.org/wiki/Coefficient_of_determination\">on the Wikipedia page for CoD</a>).</p>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <p>Try evaluating model using RMSE, but on the training set this time</p>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">##### Answer to exercise 1.</span><span class=\"w\">\n</span><span class=\"c1\">#Calculate residuals</span><span class=\"w\">\n</span><span class=\"n\">res</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">bc_train</span><span class=\"o\">$</span><span class=\"n\">Radius.Mean</span><span class=\"o\">-</span><span class=\"n\">bc_train</span><span class=\"o\">$</span><span class=\"n\">pred</span><span class=\"w\">\n</span><span class=\"c1\">#For training data we can also obtain the residuals using the bc_model$residuals</span><span class=\"w\">\n\n</span><span class=\"c1\"># Calculate RMSE, assign it to the variable rmse and print it</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"n\">rmse</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">sqrt</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">res</span><span class=\"o\">^</span><span class=\"m\">2</span><span class=\"p\">)))</span><span class=\"w\">\n</span><span class=\"p\">[</span><span class=\"m\">1</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"m\">0.5624438</span><span class=\"w\">\n\n</span><span class=\"c1\"># Calculate the standard deviation of actual outcome and print it</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"n\">sd_bc_train</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">sd</span><span class=\"p\">(</span><span class=\"n\">bc_train</span><span class=\"o\">$</span><span class=\"n\">Radius.Mean</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"p\">[</span><span class=\"m\">1</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"m\">3.494182</span><span class=\"w\">\n</span></code></pre></div>    </div>\n\n    <p>So we can see that our RMSE is very small compared to SD, hence it is a good model</p>\n\n  </blockquote>\n</blockquote>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <ol>\n    <li>Calculate RMSE for the test data and check if the model is not overfit.</li>\n    <li>Evaluating model using R_square - on training set.</li>\n    <li>Calculate R_square for the test data and check if the model is not overfit.</li>\n  </ol>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <p>TODO</p>\n\n  </blockquote>\n</blockquote>\n\n<div class=\"language-r highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># Calculate mean of outcome: bc_mean.</span><span class=\"w\">\n</span><span class=\"n\">bc_mean</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">bc_train</span><span class=\"o\">$</span><span class=\"n\">Radius.Mean</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"c1\"># Calculate total sum of squares: tss.</span><span class=\"w\">\n</span><span class=\"n\">tss</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">sum</span><span class=\"p\">((</span><span class=\"n\">bc_train</span><span class=\"o\">$</span><span class=\"n\">Radius.Mean</span><span class=\"w\"> </span><span class=\"o\">-</span><span class=\"w\"> </span><span class=\"n\">bc_mean</span><span class=\"p\">)</span><span class=\"o\">^</span><span class=\"m\">2</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"c1\"># Calculate residual sum of squares: rss.</span><span class=\"w\">\n</span><span class=\"n\">err</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"n\">bc_train</span><span class=\"o\">$</span><span class=\"n\">Radius.Mean</span><span class=\"o\">-</span><span class=\"n\">bc_train</span><span class=\"o\">$</span><span class=\"n\">pred</span><span class=\"w\">\n</span><span class=\"n\">rss</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"nf\">sum</span><span class=\"p\">(</span><span class=\"n\">err</span><span class=\"o\">^</span><span class=\"m\">2</span><span class=\"p\">)</span><span class=\"w\">\n\n</span><span class=\"c1\"># Calculate R-squared: rsq. Print it. Is it a good fit?</span><span class=\"w\">\n</span><span class=\"p\">(</span><span class=\"n\">rsq</span><span class=\"w\"> </span><span class=\"o\">&lt;-</span><span class=\"w\"> </span><span class=\"m\">1</span><span class=\"o\">-</span><span class=\"p\">(</span><span class=\"n\">rss</span><span class=\"o\">/</span><span class=\"n\">tss</span><span class=\"p\">))</span><span class=\"w\">\n</span><span class=\"p\">[</span><span class=\"m\">1</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"m\">0.974028</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>This again confirms that our model is very good as the R_square value is very close to 1</p>\n\n<h1 id=\"conclusion\">Conclusion</h1>\n\n<p>With the rise in high-throughput sequencing technologies, the volume of omics data has grown exponentially in recent times and a major issue is to mine useful knowledge from these data which are also heterogeneous in nature. Machine learning (ML) is a discipline in which computers perform automated learning without being programmed explicitly and assist humans to make sense of large and complex data sets. The analysis of complex high-volume data is not trivial and classical tools cannot be used to explore their full potential. Machine learning can thus be very useful in mining large omics datasets to uncover new insights that can advance the field of bioinformatics.</p>\n\n<p>This tutorial was only a first introductory step into the main concepts and approaches in machine learning. We looked at some of the common methods being used to analyse a representative dataset, by providing a practical context through the use of basic but widely used R libraries. Hopefully, at this point, you will have acquired a first understanding of the standard ML processes, as well as the practical skills in applying them on familiar problems and publicly available real-world data sets.</p>\n"],"ref_slides":[],"hands_on":true,"slides":false,"mod_date":"2024-05-29 14:28:52 +0000","pub_date":"2021-05-21 15:04:43 +0000","version":20,"api":"https://training.galaxyproject.org/training-material/api/topics/statistics/tutorials/intro-to-ml-with-r/tutorial.json","tools":["interactive_tool_rstudio"],"supported_servers":{"exact":[{"url":"https://usegalaxy.eu","name":"UseGalaxy.eu","usegalaxy":true},{"url":"https://usegalaxy.fr/","name":"UseGalaxy.fr","usegalaxy":false},{"url":"https://usegalaxy.org","name":"UseGalaxy.org (Main)","usegalaxy":true},{"url":"https://usegalaxy.org.au","name":"UseGalaxy.org.au","usegalaxy":true}],"inexact":[]},"topic_name_human":"Statistics and machine learning","admin_install":{"install_tool_dependencies":true,"install_repository_dependencies":true,"install_resolver_dependencies":true,"tools":[]},"admin_install_yaml":"---\ninstall_tool_dependencies: true\ninstall_repository_dependencies: true\ninstall_resolver_dependencies: true\ntools: []\n","tours":false,"video":false,"slides_recordings":false,"translations":{"tutorial":[],"slides":[],"video":false},"license":"CC-BY-4.0","type":"tutorial"}
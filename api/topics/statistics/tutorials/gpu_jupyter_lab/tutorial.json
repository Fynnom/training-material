{"layout":"tutorial_hands_on","title":"A Docker-based interactive Jupyterlab powered by GPU for artificial intelligence in Galaxy","zenodo_link":"https://zenodo.org/record/6091361","questions":["How to use Jupyterlab and it several features?","How to use it for creating input datasets and writing artificial intelligence (AI) algorithms?"],"objectives":["Learn to use Jupyterlab - an online Python editor designed for developing AI algorithms","Explore several of its features such as Git, workflow of jupyter notebook, integration to Galaxy","Develop AI algorithms using Tensorflow","Send long-running jobs to Galaxy's cluster and save results in its history","Reproduce results from recent scientific publications - COVID CT scan segmentation and 3D protein structure prediction"],"requirements":[{"type":"internal","topic_name":"galaxy-interface","tutorials":["jupyterlab"]},{"type":"internal","topic_name":"statistics","tutorials":["intro_deep_learning"]}],"time_estimation":"1H","tags":["interactive-tools","machine-learning","deep-learning","jupyter-lab","image-segmentation","protein-3D-structure"],"contributors":[{"name":"Anup Kumar","email":"anup.rulez@gmail.com","twitter":"musafirtweetsz","joined":"2018-08","elixir_node":"de","affiliations":["uni-freiburg","eurosciencegateway","elixir-europe"],"id":"anuprulez","url":"https://training.galaxyproject.org/training-material/api/contributors/anuprulez.json","page":"https://training.galaxyproject.org/training-material/hall-of-fame/anuprulez/"}],"js_requirements":{"mathjax":null,"mermaid":false},"short_id":"T00266","url":"/topics/statistics/tutorials/gpu_jupyter_lab/tutorial.html","topic_name":"statistics","tutorial_name":"gpu_jupyter_lab","dir":"topics/statistics/tutorials/gpu_jupyter_lab","symlink":null,"id":"statistics/gpu_jupyter_lab","ref_tutorials":["<p><a href=\"https://jupyterlab.readthedocs.io/en/stable/\">Jupyterlab</a> is a popular integrated development environment (IDE) for a variety of tasks in data science such as prototyping analyses, creating meaningful plots, data manipulation and preprocessing. Python is one of the most used languages in such an environment. Given the usefulness of Jupyterlab, more importantly in online platforms, a robust <a href=\"https://usegalaxy.eu/root?tool_id=interactive_tool_ml_jupyter_notebook\">Jupyterlab notebook application</a> has been developed that is powered by GPU acceleration and contains numerous packages such as Pandas, Numpy, Scipy, <a href=\"https://scikit-learn.org/\">Scikit-learn</a>, <a href=\"https://www.tensorflow.org/\">Tensorflow</a>, <a href=\"https://onnx.ai/\">ONNX</a> to support modern data science projects. It has been developed as an interactive Galaxy tool that runs on an isolated <a href=\"https://github.com/anuprulez/ml-jupyter-notebook\">docker container</a>. The docker container has been built using <a href=\"https://hub.docker.com/layers/anupkumar/docker-ml-jupyterlab/galaxy-integration-0.2/images/sha256-e2d7e28a2f975523db0f5ac29c2e2ce3c7a35b061072098ad388d5b42ee86fba?context=repo\">nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04</a> as the base container. Moreover, a Galaxy <a href=\"https://github.com/bgruening/galaxytools/pull/1157\">tool</a> ( <code class=\"language-plaintext highlighter-rouge\">run_jupyter_job</code>) can be executed using <a href=\"https://bioblend.readthedocs.io/\">Bioblend</a> which uses Galaxy’s remote job handling for long-running machine learning and deep learning training. The training happens remotely on a Galaxy cluster and the outcome datasets such as the trained models, tabular files and so on are saved in a Galaxy history for further use.</p>\n\n<blockquote class=\"agenda\">\n  <agenda-title></agenda-title>\n\n  <p>In this tutorial, we will cover:</p>\n\n<ol id=\"markdown-toc\">\n  <li><a href=\"#features-of-jupyterlab\" id=\"markdown-toc-features-of-jupyterlab\">Features of Jupyterlab</a></li>\n  <li><a href=\"#image-segmentation\" id=\"markdown-toc-image-segmentation\">Image segmentation</a></li>\n  <li><a href=\"#covid-ct-scan\" id=\"markdown-toc-covid-ct-scan\">COVID CT scan</a></li>\n  <li><a href=\"#ct-scans-and-masks\" id=\"markdown-toc-ct-scans-and-masks\">CT scans and masks</a></li>\n  <li><a href=\"#protein-3d-structure\" id=\"markdown-toc-protein-3d-structure\">Protein 3D structure</a></li>\n  <li><a href=\"#reproduce-results-from-recent-publications\" id=\"markdown-toc-reproduce-results-from-recent-publications\">Reproduce results from recent publications</a>    <ol>\n      <li><a href=\"#open-jupyterlab\" id=\"markdown-toc-open-jupyterlab\">Open Jupyterlab</a></li>\n      <li><a href=\"#clone-github-repository\" id=\"markdown-toc-clone-github-repository\">Clone Github repository</a></li>\n      <li><a href=\"#use-case-1-run-image-segmentation-analysis\" id=\"markdown-toc-use-case-1-run-image-segmentation-analysis\">Use-case 1: Run image segmentation analysis</a></li>\n      <li><a href=\"#use-case-2-run-colabfold-to-predict-3d-structure-of-protein\" id=\"markdown-toc-use-case-2-run-colabfold-to-predict-3d-structure-of-protein\">Use-case 2: Run ColabFold to predict 3D structure of protein</a></li>\n      <li><a href=\"#dockers-security\" id=\"markdown-toc-dockers-security\">Docker’s security</a></li>\n    </ol>\n  </li>\n  <li><a href=\"#gpu-juyterlab-tool-in-a-galaxy-workflow\" id=\"markdown-toc-gpu-juyterlab-tool-in-a-galaxy-workflow\">GPU Juyterlab tool in a Galaxy workflow</a>    <ol>\n      <li><a href=\"#introduction\" id=\"markdown-toc-introduction\">Introduction</a></li>\n      <li><a href=\"#run-as-a-tool-in-workflow\" id=\"markdown-toc-run-as-a-tool-in-workflow\">Run as a tool in workflow</a></li>\n    </ol>\n  </li>\n  <li><a href=\"#conclusion\" id=\"markdown-toc-conclusion\">Conclusion</a></li>\n</ol>\n\n</blockquote>\n\n<h2 id=\"features-of-jupyterlab\">Features of Jupyterlab</h2>\n<p><a href=\"https://jupyterlab.readthedocs.io/en/stable/\">Jupyterlab</a> notebook has been augmented with several useful features and together they make it ready-to-use for quick prototyping and end-to-end artificial intelligence (AI) projects. Being able to <strong>serve online</strong> makes it convenient to share it with other researchers and users. Its features can be broadly classified into two categories - features that have been added solely as Python packages and those that have been added also as Python packages but have their respective user interfaces. We will briefly discuss these features and later, we will use some of those for building and using AI models.</p>\n\n<h4 id=\"features-python-packages\">Features (Python packages)</h4>\n<p>Accessibility to many features has been made possible by adding several compatible Python packages, the most prominent ones are Bioblend, <a href=\"https://developer.nvidia.com/cuda-toolkit\">CUDA</a> and ONNX. Using Bioblend the notebook can be <strong>connected to Galaxy</strong> and its histories, tools and workflows can be accessed and operated using various commands from the notebook. GPUs have accelerated AI research, especially deep learning. Therefore, the backend of the Jupyterlab is <strong>powered by GPU</strong> to make long-running AI training programs finish faster by parallelizing matrix multiplications. CUDA acts as a bridge between GPU and AI algorithms. In addition, a standard AI model format, <strong>open neural network exchange (ONNX)</strong>, has been supported in the notebook to transform Scikit-learn and Tensorflow models to <code class=\"language-plaintext highlighter-rouge\">onnx</code> files. These model files can be conveniently shared and used for inference. Galaxy also supports <code class=\"language-plaintext highlighter-rouge\">onnx</code> file format which makes it easier to interoperate such models from a notebook to Galaxy and vice-versa. Galaxy tool for <strong>remote training</strong> can also run on GPU. There are several additional packages, suited for performing machine learning tasks such as Scikit-learn and Tensorflow for developing AI algorithms, <a href=\"https://github.com/opencv/opencv\">Open-CV</a> and <a href=\"https://scikit-image.org/\">Scikit-Image</a> for image processing, <a href=\"https://nipy.org/nibabel/\">NiBabel</a> package for processing images files have been made available to researchers.</p>\n\n<h4 id=\"features-python-packages-with-user-interfaces\">Features (Python packages with user interfaces)</h4>\n<p>There are other features, added as Python packages, that have user interface components. These include <strong><a href=\"https://github.com/rapidsai/jupyterlab-nvdashboard\">GPU utilization dashboards</a></strong> for monitoring the GPU usage and system memory utilization, <strong><a href=\"https://github.com/voila-dashboards/voila\">Voila</a></strong> for rendering output cells of a notebook in a separate tab hiding all code cells, interactive <strong><a href=\"https://github.com/bqplot/bqplot\">Bqplots</a></strong> for plotting, <strong><a href=\"https://github.com/jupyterlab/jupyterlab-git\">Git</a></strong> version control to clone, commit and maintain codebase directly from the notebook and <strong><a href=\"https://elyra.readthedocs.io/en/stable/\">Elyra AI</a></strong> pipelines for knitting together multiple notebooks to create a workflow. Many notebooks are created for processing datasets in different ways. These notebooks can be knit together to form one pipeline where each notebook transforms datasets taking a different form of data from its previous notebook and passing on the transformed datasets to its next notebook. An example workflow created using Elyra AI can be found by the name “METABRIC_ML.pipeline” in the “elyra” folder of a typical Jupyterlab instance. These pipelines can also be executed remotely on multiple “runtimes” supported by docker containers. An example pipeline created in the notebook can be seen in Figure 1. These features, integrated into the Jupyterlab notebook, are useful for the efficient execution and management of data science projects.</p>\n\n<figure id=\"figure-1\" style=\"max-width: 90%;\"><img src=\"../../images/elyra_ai_pipeline.png\" alt=\"Elyra AI pipeline. \" width=\"966\" height=\"482\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/elyra_ai_pipeline.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 1</strong>:</span> An example ML pipeline created using Elyra AI in a notebook. Each connected block of the pipeline is a different notebook</figcaption></figure>\n\n<p>In the following sections, we will look at two examples of machine learning analyses - image segmentation and prediction of 3D structure of protein sequences. We will go through each step of how to perform such analyses in this Jupyterlab notebook.</p>\n\n<h2 id=\"image-segmentation\">Image segmentation</h2>\n<p>In an image classification task, a label is assigned to each image. For example, an image containing a cat gets a label as “cat” (“cat” is given an integer to be used in a machine learning program). But, in an image segmentation task, each pixel of an image gets a label. For example, in an image containing a cat, all the pixels occupied by the cat in the image are denoted by violet color (see Figure 2). More information can be found in the <a href=\"https://www.tensorflow.org/tutorials/images/segmentation\">image segmentation</a> tutorial. Image segmentation is a widely used technique in many fields such as medical imaging to find out abnormal regions in images of tissues, self-driving cars to detect different objects on highways and many more.</p>\n\n<figure id=\"figure-2\" style=\"max-width: 90%;\"><img src=\"../../images/cat_segmentation.png\" alt=\"Image segmentation for a cat. \" width=\"1095\" height=\"370\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/cat_segmentation.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 2</strong>:</span> Image segmentation for cat. The violet region of the middle plot denotes the cat and the outline of the violet section denoted by the yellow color is the cat's mask</figcaption></figure>\n\n<h2 id=\"covid-ct-scan\">COVID CT scan</h2>\n<p>Segmenting lung CT scans task to locate infected regions has been actively proposed to augment the RT-PCR testing for initial screening of COVID infection in humans. Deep learning has been used to predict these regions with high accuracy in studies such as <span class=\"citation\"><a href=\"#InfNet\">Fan 2020</a></span>, <span class=\"citation\"><a href=\"#JCSWu2021\">Wu 2021</a></span> and many more. In Figure 3, the differences between the CT scans of a normal person and a person suffering from COVID-19 can be seen. The regions of the lungs marked by white patches are infected as discussed in <span class=\"citation\"><a href=\"#SaeedizadehTVUnet\">Saeedizadeh 2021</a></span>.</p>\n\n<figure id=\"figure-3\" style=\"max-width: 90%;\"><img src=\"../../images/normal_covid_ct_scans.jpg\" alt=\"Normal and Covid CT scans. \" width=\"508\" height=\"266\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/normal_covid_ct_scans.jpg\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 3</strong>:</span> (Left) Lungs CT scan for a normal person and (right) lungs CT scan for a person having COVID-19</figcaption></figure>\n\n<h2 id=\"ct-scans-and-masks\">CT scans and masks</h2>\n<p>Figure 4 shows CT scans of infected lungs (top row) and borders around the infected regions have been drawn with red color (middle row). In the last row respective masks, from the CT scans, have been taken out denoting the infected regions. These masks are the “segmented” regions from the corresponding CT scans. While creating a dataset for training the deep learning model, Unet (briefly discussed in the following section), the CT scans are the data points and its “known” respective masks of infected regions become their labels (<span class=\"citation\"><a href=\"#SaeedizadehTVUnet\">Saeedizadeh 2021</a></span>).</p>\n\n<figure id=\"figure-4\" style=\"max-width: 90%;\"><img src=\"../../images/covid_ct_scan_masks.png\" alt=\"Masks. \" width=\"446\" height=\"681\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/covid_ct_scan_masks.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 4</strong>:</span> Picture shows CT scans and their masks of infected regions that are drawn with red color in middle row plots</figcaption></figure>\n\n<h3 id=\"unet-neural-network\">Unet neural network</h3>\n<p>Unet neural network is widely used for segmentation tasks in images (<span class=\"citation\"><a href=\"#Ronnebergerunet\">Ronneberger 2015</a></span>). The name “Unet” resembles the shape of its architecture as “U” (see Figure 5). It has two parts - encoder and decoder. The left half of the “U” shape is the encoder that learns the feature representation (downsampling) at a lower level. The right half is the decoder that maps the low-resolution feature representation on the higher resolution (upsampling) pixel space.</p>\n\n<figure id=\"figure-5\" style=\"max-width: 90%;\"><img src=\"../../images/Unet.jpg\" alt=\"Unet architecture. \" width=\"809\" height=\"537\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/Unet.jpg\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 5</strong>:</span> Architecture of Unet neural network for image segmentation. The name \"Unet\" resembles the shape of its architecture as 'U'</figcaption></figure>\n\n<p>In this tutorial, we will use the dataset of CTs scans and their respective masks to train an Unet neural network model. The model learns to map the infected regions in the CT scans to their masks. For prediction, the trained model is given unseen CT scans and for each CT scan, it predicts infected regions or masks. For this experiment, we will use Jupyterlab for cloning the notebooks from Github that contain scripts for downloading data, creating and training Unet model and prediction tasks. Data (CT scans and also the trained model) required for the notebooks can be downloaded from Zenodo (<span class=\"citation\"><a href=\"#anupkumar20226091361\">Kumar 2022</a></span>). The model can either be trained in Jupyterlab or can be sent to Galaxy’s cluster for remote processing. After remote processing, the created datasets such as the trained model become available in new Galaxy history.</p>\n\n<h2 id=\"protein-3d-structure\">Protein 3D structure</h2>\n<p>Understanding the structure of proteins provides insights into their functions. But, only a few proteins have known structures out of billions of known proteins. To advance in the direction of predicting their 3D structures only from their sequences, AlphaFold2 (<span class=\"citation\"><a href=\"#AlphaFold2\">Jumper 2021</a></span>) has made a breakthrough to predict their structures with high accuracy. However, the databases that it uses are large, approximately 2 terabytes (TB), and are hard to store and manage. Therefore, to make it more accessible, a few approaches such as ColabFold (<span class=\"citation\"><a href=\"#Mirdita2021\">Mirdita 2022</a></span>), have been developed that consume significantly less memory, much faster and produce 3D structures with similar accuracy. We will look at, in later sections, how we can use ColabFold to predict 3D structure of a protein sequence using the Jupyterlab infrastructure in Galaxy.</p>\n\n<h1 id=\"reproduce-results-from-recent-publications\">Reproduce results from recent publications</h1>\n<p>First, we will discuss a few features of Jupyterlab to create and train a Unet deep learning model and then, predict segmented regions in COVID CT scans using the trained Unet model (<span class=\"citation\"><a href=\"#SaeedizadehTVUnet\">Saeedizadeh 2021</a></span>). In addition, we will predict 3D structure of protein sequence using ColabFold (<span class=\"citation\"><a href=\"#Mirdita2021\">Mirdita 2022</a></span>), a faster implementation of AlphaFold2. First, let’s start the Jupyterlab instance on Galaxy Europe. The interactive tool can be found by searching “GPU enabled Interactive Jupyter Notebook for Machine Learning” name in the tool search.</p>\n\n<h2 id=\"open-jupyterlab\">Open Jupyterlab</h2>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>GPU enabled Interactive Jupyter Notebook for Machine Learning</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"interactive_tool_ml_jupyter_notebook\" title=\"GPU enabled Interactive Jupyter Notebook for Machine Learning tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>GPU enabled Interactive Jupyter Notebook for Machine Learning</strong></span>\n      <ul>\n        <li><em>“Do you already have a notebook?”</em>: <code class=\"language-plaintext highlighter-rouge\">Start with default notebooks</code></li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n\n</blockquote>\n\n<p>Now, we should wait for a few minutes until Galaxy creates the required compute environment for opening a new Jupyterlab. Usually, this task takes around 10-15 minutes. The progress can be checked by clicking on the “User&gt;Active Interactive tools”. On “Active Interactive Tools” page, there is a list of all open interactive tools. When the job info shows “running” on the “Active Interactive Tools” page, then the name of the interactive tool gets associated with a URL to the running Jupyterlab. On clicking this URL, the running Jupyterlab can be opened. Several features of Jupyterlab running in Galaxy can be learned by going through the “home_page.ipynb” notebook. The folder “notebooks” contain several notebooks that show multiple use-cases of features and packages. Now, let’s download the necessary notebooks from Github for performing image segmentation and predicting 3D structure of protein sequences.</p>\n\n<h2 id=\"clone-github-repository\">Clone Github repository</h2>\n<p>To use Git version control for cloning any codebase from GitHub, the following steps should be performed. Alternatively, the notebooks that showcase different usecases are also available by default at <code class=\"language-plaintext highlighter-rouge\">&lt;&lt;root&gt;&gt;/usecases/</code>.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Pull code</hands-on-title>\n\n  <ol>\n    <li>Create a new folder named <code class=\"language-plaintext highlighter-rouge\">covid_ct_segmentation</code> alongside other folders such as “data”, “outputs”, “elyra” or you can use your favourite folder name.</li>\n    <li>Inside the created folder, clone a code repository by clicking on “Git” icon as shown in Figure 6.</li>\n    <li>In the shown popup, provide the repository path as shown below and then, click on “clone”:\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>https://github.com/anuprulez/gpu_jupyterlab_ct_image_segmentation\n</code></pre></div>      </div>\n    </li>\n    <li>The repository “anuprulez/gpu_jupyterlab_ct_image_segmentation” gets immediately cloned.</li>\n    <li>Move inside the created folder <code class=\"language-plaintext highlighter-rouge\">gpu_jupyterlab_ct_image_segmentation</code>. A few notebooks can be found inside that are numbered.\n      <figure id=\"figure-6\" style=\"max-width: 90%;\"><img src=\"../../images/git_clone.png\" alt=\"Clone repository. \" width=\"461\" height=\"254\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/git_clone.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 6</strong>:</span> Clone a code repository using Git</figcaption></figure>\n    </li>\n  </ol>\n\n</blockquote>\n\n<p>Now, we have all the notebooks available for performing image segmentation and predicting 3D structures of protein sequences. The entire analysis of image segmentation can be completed in two ways - first by running 3 notebooks (numbered as 1, 2 and 3) in the Jupyterlab itself and second by using 4, 5 and 6 notebooks that start remote training on Galaxy cluster invoking another Galaxy tool from one of the notebooks. First, let’s look at how we can run this analysis in the notebook itself.</p>\n\n<h2 id=\"use-case-1-run-image-segmentation-analysis\">Use-case 1: Run image segmentation analysis</h2>\n\n<h3 id=\"run-notebooks-in-jupyterlab\">Run notebooks in Jupyterlab</h3>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Run notebooks in Jupyterlab</hands-on-title>\n\n  <ol>\n    <li>\n      <p>Download and save datasets in the notebook using <strong>“1_fetch_datasets.ipynb”</strong> notebook. It creates all the necessary folders and then, downloads two datasets - one “h5” file containing many matrices as sub-datasets belonging to training data, training labels, validation data, validation labels, test data and test labels.</p>\n\n      <blockquote class=\"comment\">\n        <comment-title></comment-title>\n        <p>Sub-datasets are stored as different variables after reading the original <code class=\"language-plaintext highlighter-rouge\">h5</code> file once. For training, we only need these datasets/matrices - training data, training labels, validation data and validation labels. The matrices, test data and test labels, are used only for prediction. We use <code class=\"language-plaintext highlighter-rouge\">h5</code> format for storing and retrieving datasets as all AI algorithms need input datasets in the form of matrices. Since, in the field of AI, there are many different types of datasets such as images, sequences, real numbers and so on, therefore, to converge all these different forms of datasets into one format, we use <code class=\"language-plaintext highlighter-rouge\">h5</code> to store matrices. In any AI analysis, different forms of datasets can be stored as <code class=\"language-plaintext highlighter-rouge\">h5</code> files. Therefore, for the image segmentation task, all the input datasets/matrices to the deep learning model are saved as sub-datasets in one <code class=\"language-plaintext highlighter-rouge\">h5</code> file so that they can be easily created, stored, downloaded and used. This step may take a few minutes as it downloads around 450 MB of data from Zenodo (<span class=\"citation\"><a href=\"#anupkumar20226091361\">Kumar 2022</a></span>). Once these datasets are downloaded to the notebook, we can move to the next step to create a deep learning model and start training it.</p>\n\n      </blockquote>\n    </li>\n    <li>\n      <p>Create and train a deep learning Unet model using <strong>“2_create_model_and_train.ipynb”</strong> notebook. This will read the input <code class=\"language-plaintext highlighter-rouge\">h5</code> dataset containing all images and train the model after creating deep learning model architecture.</p>\n\n      <blockquote class=\"comment\">\n        <comment-title></comment-title>\n        <p>This notebook first creates a deep learning architecture based on Unet including custom loss functions such as total variation and binary cross-entropy losses. After creating the deep learning architecture, all training datasets such as training data, training labels, validation data, validation labels are loaded from the combined <code class=\"language-plaintext highlighter-rouge\">h5</code> file. In the next step, all the datasets and deep learning architecture are compiled together and training starts for 10 epochs (10 iterations over the entire training dataset). The training is fast as it runs on GPU and finishes in a few minutes and creates a trained model. In the last step, the trained model containing several files is converted to one <code class=\"language-plaintext highlighter-rouge\">onnx</code> file. Once a trained model is ready, we can move to the next step to make predictions on unseen CT scan masks.</p>\n\n      </blockquote>\n    </li>\n    <li>\n      <p>Predict unseen masks using the trained model in <strong>“3_predict_masks.ipynb”</strong> notebook.</p>\n\n      <blockquote class=\"comment\">\n        <comment-title></comment-title>\n        <p>First, it reads test datasets from the combined “h5” file and then, loads the <code class=\"language-plaintext highlighter-rouge\">onnx</code> model. Using this model, it predicts masks of unseen CT scans and then plots the ground truth and predicted masks in one plot (see Figure 7)</p>\n\n      </blockquote>\n    </li>\n  </ol>\n\n  <figure id=\"figure-7\" style=\"max-width: 90%;\"><img src=\"../../images/true_pred_masks.png\" alt=\"True and predicted masks of CTs scans. \" width=\"3600\" height=\"3600\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/true_pred_masks.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 7</strong>:</span> Ground truth and predicted masks of COVID CT scans. Predicted masks in the third and fourth columns (computed using slightly different loss functions) are very close to the corresponding original masks in the second column</figcaption></figure>\n\n  <p>Figure 7 shows original lungs CT scans in the first column. The second column shows the corresponding true infected regions in white. The third and fourth columns show the infected regions predicted with different loss functions, one is the binary cross-entropy loss and another is the combination of binary cross-entropy loss and total variation loss. Binary cross-entropy calculates loss between two corresponding pixels of true and predicted images. It measures how close the two corresponding pixels are. But, total variation loss measures the amount of noise in predicted images as noisy regions usually show large variations in their neighbourhood. The noise in the predicted images gets minimized and the connectivity of predicted masks improves as well when minimizing this loss.</p>\n\n</blockquote>\n\n<h3 id=\"run-jupyterlab-notebooks-remotely-on-a-galaxy-cluster\">Run Jupyterlab notebooks remotely on a Galaxy cluster</h3>\n\n<p>The training task completed in the notebook above can also be sent to a Galaxy cluster by executing a Galaxy tool in the notebook itself. Using <a href=\"https://bioblend.readthedocs.io/\">Bioblend</a> APIs, datasets in the form of <code class=\"language-plaintext highlighter-rouge\">h5</code> files and notebook are uploaded to a Galaxy history and then a Galaxy tool <code class=\"language-plaintext highlighter-rouge\">run_jupyter_job</code> executes the notebook using the uploaded dataset on a Galaxy cluster and creates a trained model in the Galaxy history. Let’s look at how to execute a notebook remotely on a Galaxy cluster.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Run Jupyterlab notebooks remotely on a Galaxy cluster</hands-on-title>\n\n  <ol>\n    <li>Download and save datasets in the notebook using <strong>“1_fetch_datasets.ipynb”</strong> notebook in the same way as before. Ignore this step if this notebook has already been executed.</li>\n    <li>\n      <p>Execute <strong>“5_run_remote_training.ipynb”</strong> notebook to dynamically execute code inside <strong>“4_create_model_and_train_remote.ipynb”</strong> notebook on a remote cluster using a different Galaxy tool (<code class=\"language-plaintext highlighter-rouge\">run_jupyter_job</code>). The notebook <strong>“5_run_remote_training.ipynb”</strong> provides the path of a notebook to be executed remotely along with the datasets to the Galaxy tool by calling a custom function <code class=\"language-plaintext highlighter-rouge\">run_script_job</code> which is part of the Jupyterlab notebook.</p>\n\n      <blockquote class=\"comment\">\n        <comment-title></comment-title>\n        <p>Executing <strong>“5_run_remote_training.ipynb”</strong> uploads datasets and dynamic Python script, extracted from the <strong>“4_create_model_and_train_remote.ipynb”</strong> notebook, to a newly created Galaxy history (Figure 8). When the task of uploading dataset and dynamic code is finished, the Galaxy tool (<code class=\"language-plaintext highlighter-rouge\">run_jupyter_job</code>) executes the dynamically uploaded script with the uploaded dataset on a remote Galaxy cluster which is similar to running any other Galaxy tool. When the Galaxy tool (<code class=\"language-plaintext highlighter-rouge\">run_jupyter_job</code>) finishes its execution, the resulting models and other datasets appear in the created Galaxy history. While the job is running on the Galaxy cluster, the Jupyter notebook can be closed as the model training task gets decoupled from the notebook and is entirely transferred to the Galaxy cluster. Specific history used for this job can be accessed in Galaxy. The Jupyterlab method <code class=\"language-plaintext highlighter-rouge\">run_script_job</code> has multiple input parameters - <code class=\"language-plaintext highlighter-rouge\">script_path</code>: relative path to the script that is to be executed remotely, <code class=\"language-plaintext highlighter-rouge\">data_dict</code>: list of input datasets as <code class=\"language-plaintext highlighter-rouge\">h5</code> files, <code class=\"language-plaintext highlighter-rouge\">server</code>: Galaxy server URL, <code class=\"language-plaintext highlighter-rouge\">key</code>: Galaxy API key and <code class=\"language-plaintext highlighter-rouge\">new_history_name</code>: the name of the Galaxy history that is created).</p>\n\n      </blockquote>\n\n      <figure id=\"figure-8\" style=\"max-width: 90%;\"><img src=\"../../images/finished_history_remote_ai.png\" alt=\"Galaxy history. \" width=\"340\" height=\"766\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/finished_history_remote_ai.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 8</strong>:</span> Galaxy history showing finished datasets after remote training on a Galaxy cluster</figcaption></figure>\n    </li>\n  </ol>\n\n  <p><strong>Note</strong>: The training may take longer depending on how busy Galaxy’s queueing is as it sends the training task to be done on a Galaxy cluster. Therefore, this feature should be used when the training task is expected to run for several hours. The training time is higher because a large Docker container is downloaded on the assigned cluster and only then, the training task can proceed.</p>\n\n</blockquote>\n\n<p>When the Galaxy job finishes, it creates a history having a few datasets such as input <code class=\"language-plaintext highlighter-rouge\">h5</code> dataset, dynamic Python script extracted from the notebook, a trained model saved as <code class=\"language-plaintext highlighter-rouge\">onnx</code> file, all saved lists and NumPy arrays and a zipped file containing all the intermediate files created and saved in the dynamic script. One such example of a Galaxy history is <a href=\"https://usegalaxy.eu/u/kumara/h/ctsegmentationmarch18\">available</a>. Datasets from this history can be downloaded into the Jupyterlab notebook and further analyses can be performed. Using the notebook <strong>“6_predict_masks_remote_model.ipynb”</strong>, the trained model can be downloaded from the newly created Galaxy history to Jupyterlab notebook and then, it can be used to make predictions of unseen CT scan masks in the same way as shown in <strong>“3_predict_masks.ipynb”</strong> notebook.</p>\n\n<h2 id=\"use-case-2-run-colabfold-to-predict-3d-structure-of-protein\">Use-case 2: Run ColabFold to predict 3D structure of protein</h2>\n\n<p>Google Deepmind’s AlphaFold2 has made a breakthrough in predicting the 3D structure of proteins with outstanding accuracy. However, due to their large database size (a few TB), it is not easily accessible to researchers. Therefore, a few approaches have been developed that replace the time-consuming steps of AlphaFold2 with slightly different steps to create input features and predict the 3D structure with comparable accuracy as AlphaFold2. One such approach is ColabFold which replaces the large database search in AlphaFold2 for finding homologous sequences by a significantly faster <a href=\"https://github.com/soedinglab/MMseqs2\">MMseqs2 API</a> call to generate input features based on the query protein sequence. ColabFold is approximately 16 times faster than AlphaFold2 in predicting 3D structures of protein sequences. To make it accessible in this Jupyterlab, it has been integrated into the Docker container by having two additional packages - ColabFold and the GPU enabled <a href=\"https://github.com/google/jax\">JAX</a> which is just-in-time compiler from Google used for mathematical transformations. In the following hands-on section, we will run a notebook and predict a 3D structure of a protein sequence.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Predict 3D structure of protein using ColabFold</hands-on-title>\n\n  <ol>\n    <li>Clone a Github repository by following the steps described in the previous hands-on section <code class=\"language-plaintext highlighter-rouge\">Clone Github repository</code>. Ignore this step if already done.</li>\n    <li>Open <strong>7_ColabFold_MMseq2.ipynb</strong> notebook and execute it.</li>\n    <li>It needs a few minutes (around 5-10 minutes) to download AlphaFold2 weights (of size approximately 4 GB) and uses these weights for prediction. An example prediction is shown in figure 9.</li>\n  </ol>\n  <figure id=\"figure-9\" style=\"max-width: 90%;\"><img src=\"../../images/3D_300_L_protein.png\" alt=\"&quot;3D structure of a protein sequence of length 300 with side-chains predicted by ColabFold&quot;. \" width=\"588\" height=\"409\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/3D_300_L_protein.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 9</strong>:</span> 3D structure of a protein sequence of length 300 with side-chains predicted by ColabFold</figcaption></figure>\n\n</blockquote>\n\n<h2 id=\"dockers-security\">Docker’s security</h2>\n<p>For remote training, a dynamic Python script is sent to a Galaxy tool for execution on a Galaxy cluster that may pose security risks of containing malicious code. To minimize the security risks that come along with executing dynamic Python scripts, the Galaxy tool <code class=\"language-plaintext highlighter-rouge\">run_jupyter_job</code> is executed inside a <a href=\"https://github.com/anuprulez/ml-jupyter-notebook\">Docker container</a>. It is the same container inside which the Jupyterlab notebook runs. All the packages available inside the notebook are also available for the Galaxy tool. Programs run in isolated environments inside Docker containers and have their connections limited to necessary libraries, and network connections. Due to these restrictions, they have a reduced number of interactions with the host operating system.</p>\n\n<h1 id=\"gpu-juyterlab-tool-in-a-galaxy-workflow\">GPU Juyterlab tool in a Galaxy workflow</h1>\n\n<h2 id=\"introduction\">Introduction</h2>\n\n<p>In addition to an interactive mode that opens a Jupyterlab in Galaxy, the GPU Jupyterlab tool can also be used as a tool in a Galaxy workflow. In this mode, it takes input datasets from a different tool along with an <code class=\"language-plaintext highlighter-rouge\">ipynb</code> notebook file. It executes the <code class=\"language-plaintext highlighter-rouge\">ipynb</code> file along with the input datasets and produces output datasets inside a dataset collection. This output dataset collection can then be used with other Galaxy tools. This feature enables it to be used in a Galaxy workflow like other Galaxy tools.</p>\n\n<h3 id=\"execute-an-existing-notebook\">Execute an existing notebook</h3>\n<p>In this mode, the GPU Jupyterlab tool executes the input <code class=\"language-plaintext highlighter-rouge\">ipynb</code> file and produces output datasets (in a collection) if created in the notebook file. Input datasets can also be attached to the notebook which become available at <code class=\"language-plaintext highlighter-rouge\">/galaxy_inputs/&lt;&lt;Name for parameter&gt;&gt;</code> folder. The parameter <code class=\"language-plaintext highlighter-rouge\">Name for parameter</code> is set for each input dataset that is used as the folder name inside <code class=\"language-plaintext highlighter-rouge\">/galaxy_inputs/</code>. More than one input datasets can be attached to the notebook. Another feature <code class=\"language-plaintext highlighter-rouge\">Execute notebook and return a new one</code> controls whether webfrontend becomes available showing Jupyterlab or a notebook is just executed. When this parameter is set to <code class=\"language-plaintext highlighter-rouge\">yes</code>, then the attached notebook is executed and output datasets if any become available in an output dataset collection. When this parameter is set to <code class=\"language-plaintext highlighter-rouge\">no</code> (by default), then Jupyterlab infrastructure as a webfrontend is opened along with the attached notebook and input datasets if attached.</p>\n\n<h2 id=\"run-as-a-tool-in-workflow\">Run as a tool in workflow</h2>\n<p>When the parameter <code class=\"language-plaintext highlighter-rouge\">Execute notebook and return a new one</code> is set to <code class=\"language-plaintext highlighter-rouge\">yes</code>, the GPU Jupyterlab tool can be used as a part of any workflow. In this mode, it requires an <code class=\"language-plaintext highlighter-rouge\">ipynb</code> file/notebook that gets executed in Galaxy and output datasets if any become available in the Galaxy history. Along with a notebook, multiple input datasets can also be attached that become automatically available inside the notebook. They can be accessed inside the notebook and processed to produce desired output datasets. These output datasets can further be used with other Galaxy tools. The following image shows a sample workflow for illustration purposes. Similarly, high-quality workflows to analyse scientific datasets can be created.</p>\n\n<figure id=\"figure-10\" style=\"max-width: 90%;\"><img src=\"../../images/workflow_gpu_jupyterlab.png\" alt=\"&quot;A sample Galaxy workflow that uses GPU Jupyterlab as a tool&quot;. \" width=\"832\" height=\"501\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/workflow_gpu_jupyterlab.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 10</strong>:</span> A sample Galaxy workflow that uses GPU Jupyterlab as a tool which takes input datasets from one tool, trains a machine learning model to predict classes and then the predicted datasets is used as input to another Galaxy tool.</figcaption></figure>\n\n<p>Let’s look at how can this workflow be created in a step-wise manner. There are 3 steps - first, the training dataset is filtered using the <code class=\"language-plaintext highlighter-rouge\">Filter</code> tool. The output of this tool along with 2 other datasets (<code class=\"language-plaintext highlighter-rouge\">test_rows</code> and <code class=\"language-plaintext highlighter-rouge\">test_rows_labels</code>), a sample IPython notebook is executed by the GPU Jupyterlab tool. The sample IPython notebook trains a simple machine learning model using the train dataset and creates a classification model using <code class=\"language-plaintext highlighter-rouge\">RandomForestClassifier</code>. The trained model is then used to predict classes using the test dataset. The predicted classes is produced as a file in an output collection by the GPU Jupyterlab tool. As a last step, <code class=\"language-plaintext highlighter-rouge\">Cut</code> tool is used to extract the first column of the output collection. Together, these steps showcase how the GPU Jupyterlab tool is used with other Galaxy tools in a workflow.</p>\n\n<h3 id=\"get-data\">Get data</h3>\n\n<p>Datasets such as train, test and sample IPython (<code class=\"language-plaintext highlighter-rouge\">ipynb</code>) notebook files are downloaded from <a href=\"https://zenodo.org/record/7534061\">Zenodo</a> to import them into a Galaxy history.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Data upload</hands-on-title>\n\n  <ol>\n    <li>\n      <p>Make sure you have an empty analysis history.</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-creating-a-new-history\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-creating-a-new-history\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Creating a new history</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <p>Click the <i class=\"fas fa-plus\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">new-history</span> icon at the top of the history panel:</p>   <p><img src=\"/training-material/shared/images/history_create_new.svg\" alt=\"UI for creating new history\" /></p>   <!-- the original drawing can be found here https://docs.google.com/drawings/d/1cCBrLAo4kDGic5QyB70rRiWJAKTenTU8STsKDaLcVU8/edit?usp=sharing --> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p><strong>Rename your history</strong> to make it easy to recognize</p>\n\n      <blockquote class=\"tip\">\n        <tip-title>Rename a history</tip-title>\n\n        <ul>\n          <li>\n            <p>Click on the title of the history (by default the title is <code class=\"language-plaintext highlighter-rouge\">Unnamed history</code>)</p>\n\n            <p><a href=\"../../../../shared/images/rename_history.png\" rel=\"noopener noreferrer\"><img src=\"../../../../shared/images/rename_history.png\" alt=\"Renaming history. \" width=\"270\" height=\"320\" loading=\"lazy\" /></a></p>\n          </li>\n          <li>Type <code class=\"language-plaintext highlighter-rouge\">Galaxy Introduction</code> as the name</li>\n          <li>Press <kbd>Enter</kbd></li>\n        </ul>\n\n      </blockquote>\n    </li>\n    <li>\n      <p>Import the files from <a href=\"https://zenodo.org/record/7534061\">Zenodo</a></p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>https://zenodo.org/record/7534061/files/sample_notebook.ipynb\nhttps://zenodo.org/record/7534061/files/test_rows.csv\nhttps://zenodo.org/record/7534061/files/train_rows.csv\nhttps://zenodo.org/record/7534061/files/test_rows_labels.csv\n</code></pre></div>      </div>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-importing-via-links\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-importing-via-links\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Importing via links</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Copy the link location</li>   <li>     <p>Click <i class=\"fas fa-upload\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-upload</span> <strong>Upload Data</strong> at the top of the tool panel</p>   </li>   <li>Select <i class=\"fa fa-edit\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-wf-edit</span> <strong>Paste/Fetch Data</strong></li>   <li>     <p>Paste the link(s) into the text field</p>   </li>   <li>     <p>Press <strong>Start</strong></p>   </li>   <li><strong>Close</strong> the window</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p>Rename the datasets as <code class=\"language-plaintext highlighter-rouge\">test_rows</code>, <code class=\"language-plaintext highlighter-rouge\">train_rows</code> and <code class=\"language-plaintext highlighter-rouge\">test_rows_labels</code> respectively.</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-renaming-a-dataset\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-renaming-a-dataset\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Renaming a dataset</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Click on the <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, change the <strong>Name</strong> field</li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p>Check that the datatype of all three tabular datasets is <code class=\"language-plaintext highlighter-rouge\">csv</code>.</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-changing-the-datatype\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-changing-the-datatype\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Changing the datatype</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Click on the <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, click <i class=\"fas fa-database\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-chart-select-data</span> <strong>Datatypes</strong> tab on the top</li>   <li>In the <i class=\"fas fa-database\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-chart-select-data</span> <strong>Assign Datatype</strong>, select <code class=\"language-plaintext highlighter-rouge\">datatypes</code> from “<em>New type</em>” dropdown     <ul>       <li>Tip: you can start typing the datatype into the field to filter the dropdown menu</li>     </ul>   </li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n  </ol>\n\n</blockquote>\n\n<h3 id=\"filter-training-dataset\">Filter training dataset</h3>\n<p>This tool showcases how a tool prior to using the GPU Jupyterlab tool (in a workflow) can be used and how its output can be used as input to the GPU Jupyterlab tool.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Filter</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"Filter1\" title=\"Filter tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Filter</strong></span>\n      <ul>\n        <li><em>“Filter *”</em> : Select <code class=\"language-plaintext highlighter-rouge\">train_rows</code></li>\n        <li><em>“With following condition”</em> :<code class=\"language-plaintext highlighter-rouge\">c3==0</code></li>\n        <li><em>“Number of header lines to skip”</em>: <code class=\"language-plaintext highlighter-rouge\">1</code></li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n\n</blockquote>\n\n<p>Rename the filtered dataset as <code class=\"language-plaintext highlighter-rouge\">new_train_rows</code> which will be used as one of the inputs to the GPU Jupyterlab tool in the next step. The datatype should be <code class=\"language-plaintext highlighter-rouge\">tabular</code>.</p>\n\n<h3 id=\"execute-ipython-notebook-using-gpu-jupyterlab-tool\">Execute IPython notebook using GPU Jupyterlab tool</h3>\n<p>This tool showcases GPU Jupyterlab tool that executes a notebook using input datasets produced by different tools. Galaxy tool like features for this interactive tool has been adapted from another interactive Galaxy tool <strong>Interactive JupyTool and notebook</strong>.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>GPU Jupyterlab tool</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"interactive_tool_ml_jupyter_notebook\" title=\"GPU Jupyterlab tool tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>GPU Jupyterlab tool</strong></span>\n      <ul>\n        <li><em>“Do you already have a notebook?”</em> : Select <code class=\"language-plaintext highlighter-rouge\">Load an existing notebook</code></li>\n        <li><em>“IPython Notebook”</em> : Select <code class=\"language-plaintext highlighter-rouge\">sample_notebook.ipynb</code></li>\n        <li><em>“Execute notebook and return a new one”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n        <li>In <em>“User inputs”</em>:\n          <ul>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“1: User inputs”</em>\n              <ul>\n                <li><em>“Name for parameter”</em>: <code class=\"language-plaintext highlighter-rouge\">train</code></li>\n                <li><em>“Choose the input type”</em>: <code class=\"language-plaintext highlighter-rouge\">Dataset</code></li>\n                <li><em>“Select value”</em>: Select <code class=\"language-plaintext highlighter-rouge\">new_train_rows</code></li>\n              </ul>\n            </li>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“2: User inputs”</em>\n              <ul>\n                <li><em>“Name for parameter”</em>: <code class=\"language-plaintext highlighter-rouge\">test</code></li>\n                <li><em>“Choose the input type”</em>: <code class=\"language-plaintext highlighter-rouge\">Dataset</code></li>\n                <li><em>“Select value”</em>: Select <code class=\"language-plaintext highlighter-rouge\">test_rows</code></li>\n              </ul>\n            </li>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“3: User inputs”</em>\n              <ul>\n                <li><em>“Name for parameter”</em>: <code class=\"language-plaintext highlighter-rouge\">testlabels</code></li>\n                <li><em>“Choose the input type”</em>: <code class=\"language-plaintext highlighter-rouge\">Dataset</code></li>\n                <li><em>“Select value”</em>: Select <code class=\"language-plaintext highlighter-rouge\">test_rows_labels</code></li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n  </ul>\n\n</blockquote>\n\n<h3 id=\"cut-a-column-from-the-output-collection\">Cut a column from the output collection</h3>\n<p>This tool showcases how the output dataset collection produced by the GPU Jupyterlab tool can be used by a different Galaxy tool.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Cut</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"Cut1\" title=\"Cut tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Cut</strong></span>\n      <ul>\n        <li><em>“Cut columns”</em> : <code class=\"language-plaintext highlighter-rouge\">c1</code></li>\n        <li><em>“Delimited by”</em> : <code class=\"language-plaintext highlighter-rouge\">Comma</code></li>\n        <li><em>“From”</em>: Choose dataset collection <code class=\"language-plaintext highlighter-rouge\">GPU JupyterLab notebook output collection</code></li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n\n</blockquote>\n\n<p>The above tool extracts the first column from dataset residing in the output dataset collection. All three steps explained above show how the GPU Jupyterlab tool can be used with other Galaxy tools to create a workflow.</p>\n\n<h1 id=\"conclusion\">Conclusion</h1>\n<p>In this tutorial, we have discussed several features of the Jupyterlab notebook in Galaxy and used a few of them to perform an image segmentation task on COVID CT scans by training Unet, a deep learning model. In addition, we have used it for predicting 3D structure of a protein sequence which is a memory-intensive and time-consuming task. The ready-to-use infrastructure that we employed here for developing an AI algorithm and training a model shows that it is convenient for creating quick prototypes and end-to-end projects, accelerated by GPU, in the data science field without caring much about the storage and compute resources.</p>\n\n<p>The possibility of remote execution of long-running training tasks obviates keeping the notebook or computer open till the training finishes and at the same time, the datasets and models stay safe in Galaxy history. The usage of <code class=\"language-plaintext highlighter-rouge\">h5</code> files for input datasets to AI algorithms solves the problem of varying data formats across multiple fields that are used extensively for predictive tasks. Using <code class=\"language-plaintext highlighter-rouge\">onnx</code> for creating, storing and sharing an AI model makes it easier to handle such it as one compact file. Git integration makes it easy to maintain an entire code repository directly in the notebook. The entire environment can be shared with other researchers only via sharing the URL of the Jupyterlab. Bioblend connects this infrastructure to Galaxy which makes its histories, workflows and tools accessible directly via the notebook. Overall, this infrastructure would prove useful for researchers having little access to large compute resources to develop and manage quick prototypes and end-to-end data science projects.</p>\n\n<p>Feature to use GPU Jupyterlab as a regular Galaxy tool provides an ability to execute IPython notebooks in a workflow. The executed notebook is also available in the Galaxy history with the associated cell outputs.</p>\n"],"ref_slides":[],"video_library":{"tutorial":null,"slides":null,"demo":null,"both":null,"session":null},"hands_on":true,"slides":false,"mod_date":"2023-11-03 14:30:27 +0000","pub_date":"2022-04-06 17:18:13 +0000","version":16,"workflows":[{"workflow":"gpu_jupyterlab_as_jupytool.ga","tests":false,"url":"https://training.galaxyproject.org/training-material/topics/statistics/tutorials/gpu_jupyter_lab/workflows/gpu_jupyterlab_as_jupytool.ga","path":"topics/statistics/tutorials/gpu_jupyter_lab/workflows/gpu_jupyterlab_as_jupytool.ga","wfid":"statistics-gpu_jupyter_lab","wfname":"gpu_jupyterlab_as_jupytool","trs_endpoint":"https://training.galaxyproject.org/training-material/api/ga4gh/trs/v2/tools/statistics-gpu_jupyter_lab/versions/gpu_jupyterlab_as_jupytool","license":null,"creators":[],"name":"gpu_jupytool","title":"gpu_jupytool","test_results":null,"modified":"2024-06-13 19:24:30 +0000","mermaid":"flowchart TD\n  0[\"ℹ️ Input Dataset\\nIPynb notebook\"];\n  style 0 stroke:#2c3143,stroke-width:4px;\n  1[\"train_rows\"];\n  2[\"ℹ️ Input Dataset\\ntest_rows\"];\n  style 2 stroke:#2c3143,stroke-width:4px;\n  3[\"ℹ️ Input Dataset\\ntest_rows_labels\"];\n  style 3 stroke:#2c3143,stroke-width:4px;\n  4[\"GPU enabled Interactive Jupyter Notebook for Machine Learning\"];\n  0 -->|output| 4;\n  1 -->|out_file1| 4;\n  2 -->|output| 4;\n  3 -->|output| 4;\n  5[\"Cut\"];\n  4 -->|output_collection| 5;"}],"api":"https://training.galaxyproject.org/training-material/api/topics/statistics/tutorials/gpu_jupyter_lab/tutorial.json","tools":["Cut1","Filter1","interactive_tool_ml_jupyter_notebook"],"supported_servers":{"exact":[{"url":"https://usegalaxy.eu","name":"UseGalaxy.eu","usegalaxy":true}],"inexact":[]},"topic_name_human":"Statistics and machine learning","admin_install":{"install_tool_dependencies":true,"install_repository_dependencies":true,"install_resolver_dependencies":true,"tools":[]},"admin_install_yaml":"---\ninstall_tool_dependencies: true\ninstall_repository_dependencies: true\ninstall_resolver_dependencies: true\ntools: []\n","tours":false,"video":false,"translations":{"tutorial":[],"slides":[],"video":false},"license":"CC-BY-4.0","type":"tutorial"}
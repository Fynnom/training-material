{"layout":"tutorial_hands_on","title":"Introduction to deep learning","zenodo_link":"https://zenodo.org/record/3706539","questions":["What are deep learning and neural networks?","Why is it useful?","How to create a neural network architecture for classification?"],"objectives":["Learn basic principles of deep learning","Learn about how to create an end-to-end neural network architecture","Learn about Galaxy deep learning tools","Learn how to interpret predictions"],"key_points":["Multiple tools to constitute a neural network architecture","Interpretation of predictions using visualisation tools"],"time_estimation":"1H","contributors":[{"name":"Anup Kumar","email":"anup.rulez@gmail.com","twitter":"musafirtweetsz","joined":"2018-08","elixir_node":"de","affiliations":["uni-freiburg","eurosciencegateway","elixir-europe"],"id":"anuprulez","url":"https://training.galaxyproject.org/training-material/api/contributors/anuprulez.json","page":"https://training.galaxyproject.org/training-material/hall-of-fame/anuprulez/"},{"name":"Alireza Khanteymoori","email":"khanteymoori@gmail.com","orcid":"0000-0001-6811-9196","joined":"2019-07","former_affiliations":["uni-freiburg","elixir-europe"],"id":"khanteymoori","url":"https://training.galaxyproject.org/training-material/api/contributors/khanteymoori.json","page":"https://training.galaxyproject.org/training-material/hall-of-fame/khanteymoori/"}],"js_requirements":{"mathjax":null,"mermaid":false},"short_id":"T00268","url":"/topics/statistics/tutorials/intro_deep_learning/tutorial.html","topic_name":"statistics","tutorial_name":"intro_deep_learning","dir":"topics/statistics/tutorials/intro_deep_learning","symlink":null,"id":"statistics/intro_deep_learning","ref_tutorials":["<h2 id=\"introduction\">Introduction</h2>\n\n<h3 id=\"deep-learning-and-neural-networks\">Deep learning and neural networks</h3>\n<p><a href=\"https://en.wikipedia.org/wiki/Deep_learning\">Deep learning</a>, a branch of artificial intelligence, provides a collection of learning methods to model data with complex architectures to perform different non-linear transformations of data. Using these transformations, patterns are recognised in large volumes of data and new data can be categorised using these patterns extracted on existing data. These patterns are learned by computational models devised using different architectures of neural networks. In the recent years, the neural network architectures such as convolutional, long short-term memory networks, deep belief networks have become increasingly popular as machine learning tools in the fields of computer vision, image analysis, bioinformatics, speech recognition, natural language processing and so on achieving state-of-the-art performance, sometimes exceeding human performance. The availability of greater computational resources, more data, new algorithms for training deep models and easy to use libraries for implementation and training of neural networks are the drivers of this development. Deep learning works by approximating the mathematical function which maps data to its output and it has been shown that it can <a href=\"https://arxiv.org/pdf/1910.03344.pdf\">approximate</a> any function making it widely popular across multiple fields to analyse data. A neural network is a web of artificial neurons which are also called processing units. The idea of a neural network is inspired by <a href=\"https://en.wikipedia.org/wiki/Neural_circuit\">biological neural networks</a> where neuronal circuits are used to process information and learn. An artificial neural network is structured into multiple layers where each layer contains several neurons. The neurons from adjacent layers are interconnected (<a href=\"https://en.wikipedia.org/wiki/Feedforward_neural_network\">feed-forward neural network</a>) allowing the exchange of information between layers of neurons.</p>\n\n<figure id=\"figure-1\" style=\"max-width: 90%;\"><div style=\"overflow-x: auto\"><object data=\"../../images/neuron.svg\" type=\"image/svg+xml\" alt=\"data. \">data.</object></div><a target=\"_blank\" href=\"../../images/neuron.svg\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 1</strong>:</span> Structure of an artificial neuron. The input x (x1, x2, ..., xn) and its corresponding weight w (w1, w2, ..., wn) are vectors. The input and its weight are transformed to produce an output y</figcaption></figure>\n\n<p>An artificial neuron is shown in Figure 1. The neuron, shown in orange, takes input <code class=\"language-plaintext highlighter-rouge\">x</code> (only <code class=\"language-plaintext highlighter-rouge\">x1</code> and <code class=\"language-plaintext highlighter-rouge\">x2</code> are shown for simplicity) and computes output <code class=\"language-plaintext highlighter-rouge\">y</code>. The entities <code class=\"language-plaintext highlighter-rouge\">w1</code>, <code class=\"language-plaintext highlighter-rouge\">w2</code> are the weights of the connections (between inputs and neuron). The weights and inputs are combined following the basic principles of mathematics to produce output <code class=\"language-plaintext highlighter-rouge\">y</code> (shown in Figures 2, 3 and 4).</p>\n\n<figure id=\"figure-2\" style=\"max-width: 90%;\"><img src=\"../../images/eq1.png\" alt=\"data. \" width=\"155\" height=\"20\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/eq1.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 2</strong>:</span> Transformation of a component (x1) of the input vector (x).</figcaption></figure>\n\n<figure id=\"figure-3\" style=\"max-width: 90%;\"><img src=\"../../images/eq2.png\" alt=\"data. \" width=\"156\" height=\"20\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/eq2.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 3</strong>:</span> Transformation of a component (x2) of the input vector (x).</figcaption></figure>\n\n<p>Weights denote the significance of a particular input to produce the observed output. When it is large, the input is significant and when small, the input is less significant to produce the output. These weights can be initialised randomly and they are modified throughout the learning by a neural network. Using the updated inputs (as shown in the above equations), the output is computed:</p>\n\n<figure id=\"figure-4\" style=\"max-width: 90%;\"><img src=\"../../images/eq3.png\" alt=\"data. \" width=\"180\" height=\"29\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/eq3.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 4</strong>:</span> Computation of output y using input x, weight w and activation function f.</figcaption></figure>\n\n<p>where <em>f</em> is an activation function. An <a href=\"https://keras.io/activations/\">activation function</a> is a mathematical function which translates the combination of inputs to an output. The choices of these functions are many - sigmoid, linear, tanh, ReLU and so on. For example, sigmoid is:</p>\n\n<figure id=\"figure-5\" style=\"max-width: 90%;\"><img src=\"../../images/eq4.png\" alt=\"data. \" width=\"243\" height=\"63\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/eq4.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 5</strong>:</span> Sigmoid activation function.</figcaption></figure>\n\n<p>The above equation will return a real number between 0 and 1.</p>\n\n<p>Rectified exponential linear unit (ReLU) is given by:</p>\n\n<figure id=\"figure-6\" style=\"max-width: 90%;\"><img src=\"../../images/eq5.png\" alt=\"data. \" width=\"205\" height=\"29\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/eq5.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 6</strong>:</span> Rectified exponential linear unit (ReLU) activation function.</figcaption></figure>\n\n<p>As discussed earlier, neurons make the building blocks of a neural network and are arranged in several layers and a usual neural network looks like as shown in Figure 7.</p>\n\n<h4 id=\"input-layer\">Input layer</h4>\n<p>In the neural network (Figure 7), the input layer is shown in green. This layer receives input data and passes it on to the next layer. The number of neurons in this layer depends on the number of dimensions of input data. For example, if input data (matrix) is of size (500, 10), 500 rows (samples) and 10 columns (features), then the number of neurons in the input layer should be 10. Each neuron in input layer is connected to all neurons in the next layer. All these connections have a separate weight (denoted by <code class=\"language-plaintext highlighter-rouge\">w</code>).</p>\n\n<h4 id=\"hidden-layer\">Hidden layer</h4>\n<p>The next two layers after the input layer are called hidden layers. In the first hidden layer too, all the neurons are connected to all other neurons in the adjacent (hidden) layer. The number of hidden layers determines if the resulting neural network is deep (2 or more hidden layers) or shallow. When the number of hidden layers is 2 or more, the structure or architecture of the neural network is deep and overall learning is called deep learning. More the number of hidden layers, the more complex the architecture is. A complex architecture is beneficial for learning unique patterns from big data. But, complex architecture is prone to <a href=\"https://en.wikipedia.org/wiki/Overfitting\">overfitting</a> when a neural network starts memorising data without learning unique and general patterns.</p>\n\n<figure id=\"figure-7\" style=\"max-width: 90%;\"><div style=\"overflow-x: auto\"><object data=\"../../images/neural_network.svg\" type=\"image/svg+xml\" alt=\"data. \">data.</object></div><a target=\"_blank\" href=\"../../images/neural_network.svg\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 7</strong>:</span> A neural network consisting of 4 layers - 1 input, 2 hidden and 1 output. The neurons in each layer are connected to all neurons in the adjacent layer. Each connection between a pair of neurons contains a weight.</figcaption></figure>\n\n<p>The number of hidden layers and the size of each hidden layer is not fixed as it completely depends on the data. If the dataset is small (say only 1,000 samples), then it is sufficient to choose a less complex architecture (fewer hidden layers) to avoid the danger of overfitting. However, if the dataset is large (say &gt; 100,000 samples), more complex architecture can be chosen. In short, the architecture of the hidden layer is completely dependent on the nature and size of data.</p>\n\n<h4 id=\"output-layer\">Output layer</h4>\n<p>This layer collects output computed using input data and weights which are optimised during learning. An activation function is chosen to transform the combination of input and weight to an output. Some examples of activation functions have been discussed above.</p>\n\n<h4 id=\"optimisation\">Optimisation</h4>\n<p>Compted or predicted output, collected at the output layer, and the actual output are compared to find error (or loss). Neural network learning aims is to minimise this error so that the predicted output gets as close to the actual output as possible. This process of minimising error between predicted and actual output is called optimisation. There are several optimisers such as <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\">gradient descent</a>, root mean square propagation (<a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp\">RMSProp</a>), <a href=\"https://arxiv.org/pdf/1212.5701.pdf\">adadelta</a> and so on are available. These optimisers work by primarily adjusting the weights of connections so that the error is minimised. Once, a set of weights are achieved which provides the best accuracy or minimum error, the learning is terminated because the weights cannot be updated anymore which can further minimise the error.</p>\n\n<h4 id=\"neural-network-training\">Neural network training</h4>\n<p>Training is a process where input data is passed to a neural network at input layer and when finished, a trained model is created containing all the learned parameters such as weights of all connections in the neural network. Usually, a portion of data is extracted and saved as test data which is not used for training. It is used only for evaluating the trained model to get an unbiased estimate of learning and prediction strength. The partitioning of data into training and test can be set by deep learning practitioners. An example of partition can be - 70% training and 30% test data.</p>\n\n<h4 id=\"batch-and-epoch\">Batch and Epoch</h4>\n<p>While training a neural network, input data is passed in small batches. A batch is a subset of training data. An epoch is one iteration when all the training data is used for training in multiple batches. For example, if there is training data of size (500, 10) and batch size is fixed at 50, then there would be 10 batches (50 * 10 = 500) in each epoch. Each batch will have 50 samples and they are passed to the input layer of neural network. The loss computed at the output layer is propagated back and the weights are adjusted. The newly adjusted weights are used for the second batch of samples and so on. When all batches are finished, then one epoch of learning is done. The number of epochs and the size of a batch are parameters to be set by deep learning practitioners. These parameters depend on the size of data and should be tuned according to the data for optimum results.</p>\n\n<figure id=\"figure-8\" style=\"max-width: 90%;\"><img src=\"../../images/mse.png\" alt=\"data. \" width=\"1580\" height=\"218\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/mse.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 8</strong>:</span> Mean squared error loss function.</figcaption></figure>\n\n<h4 id=\"loss-function\">Loss function</h4>\n<p>The error between the computed and actual output is calculated using a loss function which is necessary to evaluate the strength of learning. Learning is good when loss decreases with training epochs otherwise, training should be stopped and the architecture should be carefully adjusted. There are several choices of loss functions too. Functions such as root mean squared error (RMSE) and absolute error (AE) are used for regression problems while cross-entropy error functions such as binary cross-entropy and categorical cross-entropy are used in classification problems. An example of loss function is shown in Figure 8.</p>\n\n<blockquote class=\"question\">\n  <question-title></question-title>\n\n  <ol>\n    <li>What do you understand by an architecture of a neural network?</li>\n    <li>How does a neural network learn?</li>\n  </ol>\n\n  <blockquote class=\"solution\">\n    <solution-title></solution-title>\n\n    <ol>\n      <li>Architecture of a neural network consists of multiple layers such as input, hidden, convolutional, output and their number of respective neurons, optimiser, loss and activation functions etc.</li>\n      <li>The learning happens by minimising the loss between the computed and actual output. The weights of all neuronal connections are adjusted (increased or decreased) to achieve the minimum loss. To ascertain the amount of change for weights, a technique known as backpropagation is used. Using this technique, the loss computed at the output layer is “propagated” back in the neural network (from output to input layer) and each neuronal connection is assigned a share of the total loss. In other words, how much each neuron is contributing to the total accumulated loss. For example, w1 is adjusted according to equation:</li>\n    </ol>\n    <figure id=\"figure-9\" style=\"max-width: 90%;\"><img src=\"../../images/partial_derivative.png\" alt=\"data. \" width=\"354\" height=\"88\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/partial_derivative.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 9</strong>:</span> Weight w1 is updated by computing a partial derivative of loss L with respect to weight. The derivative is multiplied with learning rate n.</figcaption></figure>\n    <p>In the above equation, <code class=\"language-plaintext highlighter-rouge\">L</code> is the total loss, <code class=\"language-plaintext highlighter-rouge\">w1</code> is the weight of a connection between an input neuron and a hidden neuron. Similarly, all weights are adjusted and in the subsequent iteration, the updated weights are used to compute loss at the output layer. Parameter <code class=\"language-plaintext highlighter-rouge\">n</code> is the learning rate which determines how small or big changes are needed in weights. It can either be a fixed quantity or a variable one. In case of a variable learning rate, it usually starts with a large number (say 1.0) and subsequently decays to a small number (say 0.001) as the training epochs proceed because initially a large learning rate helps to reach close to the minimum error quickly and then it is decayed to slow down the learning so that it stabilises at the minimum. More on backpropagation can be read <a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\">here</a>.</p>\n\n  </blockquote>\n\n</blockquote>\n\n<h3 id=\"relevance-of-deep-learning-in-bioinformatics\">Relevance of deep learning in Bioinformatics</h3>\n<p>Deep learning is an established tool in finding patterns in big data for multiple fields of research such as computer vision, image analysis, drug response prediction, protein structure prediction and so on. Different research areas use different architectures of neural network which are suitable to their respective data. For example - in computer vision and image analysis, convolutional neural network (CNN) is popular, graph convolutional neural network is often used for drug response prediction, recurrent neural network is useful for identifying motifs in protein sequences and so on. The table below shows more examples of neural networks which are popular with different fields of bioinformatics. These use-cases of deep learning in bioinformatics prove that it is essential to explore deep learning algorithms to find patterns in big data in biology. More details can be found in <a href=\"https://www.sciencedirect.com/science/article/pii/S1046202318303256\">Deep learning in bioinformatics: Introduction, application, and perspective in the big data era</a>.</p>\n\n<figure id=\"figure-10\" style=\"max-width: 90%;\"><img src=\"../../images/dl_bioinformatics.png\" alt=\"data. \" width=\"1050\" height=\"659\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/dl_bioinformatics.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 10</strong>:</span> Different architectures of neural networks for different fields of bioinformatics.</figcaption></figure>\n\n<h2 id=\"get-training-and-test-datasets\">Get training and test datasets</h2>\n<p>The datasets used for this tutorial contain gene expression profiles of humans suffering from two types of cancer - <a href=\"https://en.wikipedia.org/wiki/Acute_myeloid_leukemia\">acute myeloid leukemia (AML)</a> and <a href=\"https://en.wikipedia.org/wiki/Acute_lymphoblastic_leukemia\">acute lymphoblastic leukemia (ALL)</a>. The tutorial aims to differentiate between these two cancer types, predicting a cancer type for each patient, by learning unique patterns in gene expression profiles of patients. The data is divided into 2 parts - one for training and another for prediction. Each part contains two datasets - one has the gene expression profiles and another has labels (the types of cancer). The size of the training data (<code class=\"language-plaintext highlighter-rouge\">X_train</code>) is (38, 7129) where 38 is the number of patients and 7129 is the number of genes. The label dataset (<code class=\"language-plaintext highlighter-rouge\">y_train</code>) is of size (38, 1) and contains the information of the type of cancer for each patient (label encoding is 0 for ALL and 1 for AML). The test dataset (<code class=\"language-plaintext highlighter-rouge\">X_test</code>) is of size (34, 7129) and contains the same genes for 34 different patients. The label dataset for test is <code class=\"language-plaintext highlighter-rouge\">y_test</code> and is of size (34, 1). The neural network, which will be formulated in the remaining part of the tutorial, learns on the training data and its labels to create a trained model. The prediction ability of this model is evaluated on the test data (which is unseen during training to get an unbiased estimate of prediction ability). These datasets are uploaded to Galaxy by following the steps defined below:</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Data upload</hands-on-title>\n\n  <ol>\n    <li>\n      <p>Create a new history for this tutorial</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-creating-a-new-history\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-creating-a-new-history\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Creating a new history</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <p>Click the <i class=\"fas fa-plus\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">new-history</span> icon at the top of the history panel:</p>   <p><img src=\"/training-material/shared/images/history_create_new.svg\" alt=\"UI for creating new history\" /></p>   <!-- the original drawing can be found here https://docs.google.com/drawings/d/1cCBrLAo4kDGic5QyB70rRiWJAKTenTU8STsKDaLcVU8/edit?usp=sharing --> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p>Import the files from <a href=\"https://zenodo.org/record/3706539\">Zenodo</a></p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>https://zenodo.org/record/3706539/files/X_test.tsv\nhttps://zenodo.org/record/3706539/files/X_train.tsv\nhttps://zenodo.org/record/3706539/files/y_test.tsv\nhttps://zenodo.org/record/3706539/files/y_train.tsv\n</code></pre></div>      </div>\n    </li>\n    <li>\n      <p>Rename the datasets as <code class=\"language-plaintext highlighter-rouge\">X_test</code>, <code class=\"language-plaintext highlighter-rouge\">X_train</code>, <code class=\"language-plaintext highlighter-rouge\">y_test</code> and <code class=\"language-plaintext highlighter-rouge\">y_train</code> respectively.</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-renaming-a-dataset\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-renaming-a-dataset\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Renaming a dataset</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Click on the <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, change the <strong>Name</strong> field</li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-importing-via-links\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-importing-via-links\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Importing via links</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Copy the link location</li>   <li>     <p>Click <i class=\"fas fa-upload\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-upload</span> <strong>Upload Data</strong> at the top of the tool panel</p>   </li>   <li>Select <i class=\"fa fa-edit\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-wf-edit</span> <strong>Paste/Fetch Data</strong></li>   <li>     <p>Paste the link(s) into the text field</p>   </li>   <li>     <p>Press <strong>Start</strong></p>   </li>   <li><strong>Close</strong> the window</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p>Check that the datatype is <code class=\"language-plaintext highlighter-rouge\">tabular</code>.</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-changing-the-datatype\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-changing-the-datatype\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Changing the datatype</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Click on the <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, click <i class=\"fas fa-database\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-chart-select-data</span> <strong>Datatypes</strong> tab on the top</li>   <li>In the <i class=\"fas fa-database\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-chart-select-data</span> <strong>Assign Datatype</strong>, select <code class=\"language-plaintext highlighter-rouge\">datatypes</code> from “<em>New type</em>” dropdown     <ul>       <li>Tip: you can start typing the datatype into the field to filter the dropdown menu</li>     </ul>   </li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n  </ol>\n\n</blockquote>\n\n<h2 id=\"neural-network-architecture\">Neural network architecture</h2>\n<p>Defining a neural network architecture needs to ascertain the types and number of layers, the number of neurons for each layer, activation functions for all layers, type of optimiser and loss function. Choosing these parameters may require many experiments with data as there is no golden rule to choose the best combination of these parameters. The neural network used in this tutorial has an input layer, 2 hidden layers and one output layer. The input layer has a parameter <code class=\"language-plaintext highlighter-rouge\">input_shape</code> which is set according to the number of dimensions of data. It is set to (7129,) which is the number of genes present in data. The hidden layers have 16 neurons (units) each and the output layer has only one because a scalar output is expected (0 or 1). This partial architecture (having input shape, types and size of layers, and activation functions) of the neural network is defined as follows:</p>\n\n<h3 id=\"create-architecture-choose-layers\">Create architecture: Choose layers</h3>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Create a deep learning model architecture using Keras</hands-on-title>\n\n  <ol>\n    <li><strong>Create a deep learning model architecture using Keras</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following parameters:\n      <ul>\n        <li><em>“Select keras model type”</em>: <code class=\"language-plaintext highlighter-rouge\">Sequential</code></li>\n        <li>\n          <p><em>“input_shape”</em>: <code class=\"language-plaintext highlighter-rouge\">(7129, )</code></p>\n        </li>\n        <li>In <em>“LAYER”</em>:\n          <ul>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“1: LAYER”</em>:\n              <ul>\n                <li><em>“Choose the type of layer”</em>: <code class=\"language-plaintext highlighter-rouge\">Core -- Dense</code>\n                  <ul>\n                    <li><em>“units”</em>: <code class=\"language-plaintext highlighter-rouge\">16</code></li>\n                    <li><em>“Activation function”</em>: <code class=\"language-plaintext highlighter-rouge\">elu</code></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“2: LAYER”</em>:\n              <ul>\n                <li><em>“Choose the type of layer”</em>: <code class=\"language-plaintext highlighter-rouge\">Core -- Dense</code>\n                  <ul>\n                    <li><em>“units”</em>: <code class=\"language-plaintext highlighter-rouge\">16</code></li>\n                    <li><em>“Activation function”</em>: <code class=\"language-plaintext highlighter-rouge\">elu</code></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“3: LAYER”</em>:\n              <ul>\n                <li><em>“Choose the type of layer”</em>: <code class=\"language-plaintext highlighter-rouge\">Core -- Dense</code>\n                  <ul>\n                    <li><em>“units”</em>: <code class=\"language-plaintext highlighter-rouge\">1</code></li>\n                    <li><em>“Activation function”</em>: <code class=\"language-plaintext highlighter-rouge\">sigmoid</code></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n  </ol>\n\n</blockquote>\n\n<p>The tool returns a JSON output file containing data about the neural network layers and their attributes like their types, number of units they have and their activation functions. This file is used as an input to the next step where the architecture of the neural network is completed by adding optimiser, loss function, and training parameters such as the number of epochs and batch size. The loss function is chosen as <code class=\"language-plaintext highlighter-rouge\">binary_crossentropy</code> as the learning task is the classification of two labels (0 and 1).</p>\n\n<h3 id=\"create-architecture-add-training-parameters\">Create architecture: Add training parameters</h3>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Create deep learning model with an optimizer, loss function and fit parameters</hands-on-title>\n\n  <ol>\n    <li><strong>Create deep learning model with an optimizer, loss function and fit parameters</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following parameters:\n      <ul>\n        <li><em>“Choose a building mode”</em>: <code class=\"language-plaintext highlighter-rouge\">Build a training model</code></li>\n        <li><em>“Select the dataset containing model configurations (JSON)”</em>: <code class=\"language-plaintext highlighter-rouge\">Keras model config</code> (output of <strong>Create a deep learning model architecture using Keras</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span>)</li>\n        <li><em>“Do classification or regression?”</em>: <code class=\"language-plaintext highlighter-rouge\">KerasGClassifier</code></li>\n      </ul>\n\n      <p><code class=\"language-plaintext highlighter-rouge\">KerasGClassifier</code> is chosen because the learning task is classfication i.e. assigning each patient a type of cancer.</p>\n      <ul>\n        <li>In <em>“Compile Parameters”</em>:\n          <ul>\n            <li><em>“Select a loss function”</em>: <code class=\"language-plaintext highlighter-rouge\">binary_crossentropy</code></li>\n          </ul>\n\n          <p>The loss function is <code class=\"language-plaintext highlighter-rouge\">binary_crossentropy</code> because the labels are discrete and binary (0 and 1).</p>\n          <ul>\n            <li><em>“Select an optimizer”</em>: <code class=\"language-plaintext highlighter-rouge\">RMSprop - RMSProp optimizer</code></li>\n          </ul>\n        </li>\n        <li>In <em>“Fit Parameters”</em>:\n          <ul>\n            <li><em>“epochs”</em>: <code class=\"language-plaintext highlighter-rouge\">10</code></li>\n            <li><em>“batch_size”</em>: <code class=\"language-plaintext highlighter-rouge\">4</code></li>\n          </ul>\n\n          <p>The training data is small (only 38 patients). Therefore the number of epochs and batch size are also small.</p>\n        </li>\n      </ul>\n    </li>\n  </ol>\n\n</blockquote>\n\n<p>The tool returns a zipped file containing an object of the neural network architecture (define in the last two steps) which is used as a classifier to train it on data. Once the architecture is finalised, its associated object is used for training combining it with the training data as follows:</p>\n\n<h3 id=\"deep-learning-training\">Deep learning training</h3>\n<p>A neural network is trained on training data to learn hidden representations and mapping from features (genes) to both the types of cancer. As discussed earlier, the neural network minimises the error, which is given by the loss function, between actual and predicted labels while adjusting the weights of connections among neurons in multiple layers. Once the training is finished, the architecture and learned weights are saved. They are used to predict labels in test data. The deep learning training is set up as follows:</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Deep learning training and evaluation conduct deep training and evaluation either implicitly or explicitly</hands-on-title>\n\n  <ol>\n    <li><strong>Deep learning training and evaluation conduct deep training and evaluation</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following parameters:\n      <ul>\n        <li><em>“Select a scheme”</em>: <code class=\"language-plaintext highlighter-rouge\">Train and validate</code></li>\n        <li><em>“Choose the dataset containing pipeline/estimator object”</em>: <code class=\"language-plaintext highlighter-rouge\">Keras model builder</code> (output of <strong>Create deep learning model</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span>)</li>\n        <li><em>“Select input type”</em>: <code class=\"language-plaintext highlighter-rouge\">tabular data</code></li>\n        <li><em>“Training samples dataset”</em>: <code class=\"language-plaintext highlighter-rouge\">X_train</code>\n          <ul>\n            <li><em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n            <li><em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns</code></li>\n          </ul>\n        </li>\n        <li><em>“Dataset containing class labels or target values”</em>: <code class=\"language-plaintext highlighter-rouge\">y_train</code>\n          <ul>\n            <li><em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n            <li><em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns</code></li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n  </ol>\n\n</blockquote>\n\n<p>The tool gives 3 files as output - a tabular file containing output (accuracy of cross-validation) of training, a zipped file with the trained model (fitted estimator) and an H5 (HDF5) file containing the weights of neural network layers. The files containing the fitted estimator and weights are used to recreate the model and this recreated model is used to predict labels in test data.</p>\n\n<h3 id=\"prediction-on-test-data\">Prediction on test data</h3>\n<p>After training, the saved architecture (fitted estimator) and weights are used to predict labels for the test data. For each patient in the test data, a type of cancer is predicted using the trained model learned in the previous step.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Model Prediction predicts on new data using a preffited model</hands-on-title>\n\n  <ol>\n    <li><strong>Model Prediction predicts on new data using a preffited model</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following parameters:\n      <ul>\n        <li><em>“Choose the dataset containing pipeline/estimator object”</em>: <code class=\"language-plaintext highlighter-rouge\">Fitted estimator or estimator skeleton</code> (output of <strong>Deep learning training and evaluation</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span>)</li>\n        <li><em>“Choose the dataset containing weights for the estimator above”</em>: <code class=\"language-plaintext highlighter-rouge\">Weights trained</code> (output of <strong>Create deep learning model</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span>)</li>\n        <li><em>“Select invocation method”</em>: <code class=\"language-plaintext highlighter-rouge\">predict</code></li>\n        <li><em>“Select input data type for prediction”</em>: <code class=\"language-plaintext highlighter-rouge\">tabular data</code>\n          <ul>\n            <li><em>“Training samples dataset”</em>: <code class=\"language-plaintext highlighter-rouge\">X_test</code></li>\n            <li><em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n            <li><em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns</code></li>\n          </ul>\n        </li>\n      </ul>\n    </li>\n  </ol>\n\n</blockquote>\n\n<p>The tool returns the predicted labels (0 for ALL and 1 AML) for test data in a tabular format. The size of this data is (34,1) where 34 is the number of cancer patients in test data.</p>\n\n<h2 id=\"visualisation\">Visualisation</h2>\n<p>Visualising the results is important to ascertain the generalisation ability of the trained model on an unseen dataset. Using a dataset with the actual labels for the test data, the performance of the trained model is estimated by comparing the actual labels against the predicted labels using a confusion matrix plot.</p>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Machine Learning Visualization Extension includes several types of plotting for machine learning</hands-on-title>\n\n  <ol>\n    <li><strong>Machine Learning Visualization Extension includes several types of plotting for machine learning</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span> with the following parameters:\n      <ul>\n        <li><em>“Select a plotting type”</em>: <code class=\"language-plaintext highlighter-rouge\">Confusion matrix for classes</code></li>\n        <li><em>“Select dataset containing true labels”</em>: <code class=\"language-plaintext highlighter-rouge\">y_test</code></li>\n        <li><em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n        <li><em>“Choose how to select data by column”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns</code></li>\n        <li><em>“Select dataset containing predicted labels”</em>: <code class=\"language-plaintext highlighter-rouge\">Model prediction</code> (output of <strong>Model Prediction predicts on new data using a preffited model</strong> <i class=\"fas fa-wrench\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">tool</span>)</li>\n        <li><em>“Does the dataset contain header”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n      </ul>\n    </li>\n  </ol>\n\n</blockquote>\n\n<blockquote class=\"comment\">\n  <comment-title></comment-title>\n  <p>Please note that your predictions could be different from the plot shown in Figure 11 because the training data is small and the predictions may vary. Stability in predictions can be achieved if the deep learning model is trained on large data. But, for this tutorial, it is kept small to reduce the training time as the aim is to showcase how to create a pipeline for deep learning training. Generally, deep learning models are trained on large data and may keep running for a few hours to a few days.</p>\n</blockquote>\n\n<p>The image below shows <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">confusion matrix</a> which is a square matrix. It contains actual labels on the y-axis and predicted labels on the x-axis. Each cell in the matrix plot gives the number of cancer patients who got predicted correctly or incorrectly. For example, the number in the top-left cell (0, 0) denotes how many of these patients are predicted correctly for ALL (17/20). The higher the number in this cell, the better is the model for this cell. In the top-right cell, 3 patients who have ALL but they are predicted having AML. Similarly, the bottom-right cell denotes how many patients are predicted correctly for AML (10/14). In the bottom-left cell, 4 patients have AML but are predicted as ALL.</p>\n\n<figure id=\"figure-11\" style=\"max-width: 90%;\"><img src=\"../../images/confusion_matrix_dl.png\" alt=\"data. \" width=\"875\" height=\"875\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/confusion_matrix_dl.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 11</strong>:</span> Confusion matrix for true and predicted classes</figcaption></figure>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>The tutorial presents a case-study to predict labels (ALL and AML) of 34 new cancer patients after learning gene expression profiles of 38 cancer patients through multiple steps of a deep learning pipeline. All these steps show how to create a neural network architecture using Galaxy’s deep learning tools and analyse results using a confusion matrix visualisation. Similarly, multiple different architectures of neural networks can be created well-suited to datasets and aim of particular experiments. Moreover, it should be noted that one architecture of neural network giving promising results on a dataset may not work at all with another dataset. It is essential to perform multiple experiments with a dataset to formulate an optimal neural network architecture.</p>\n\n"],"ref_slides":[],"video_library":{"tutorial":null,"slides":null,"demo":null,"both":null,"session":null},"hands_on":true,"slides":false,"mod_date":"2024-03-05 11:50:50 +0000","pub_date":"2020-03-26 12:17:27 +0000","version":34,"workflows":[{"workflow":"Intro_To_Deep_Learning.ga","tests":false,"url":"https://training.galaxyproject.org/training-material/topics/statistics/tutorials/intro_deep_learning/workflows/Intro_To_Deep_Learning.ga","path":"topics/statistics/tutorials/intro_deep_learning/workflows/Intro_To_Deep_Learning.ga","wfid":"statistics-intro_deep_learning","wfname":"intro_to_deep_learning","trs_endpoint":"https://training.galaxyproject.org/training-material/api/ga4gh/trs/v2/tools/statistics-intro_deep_learning/versions/intro_to_deep_learning","license":null,"creators":[],"name":"Intro_To_Deep_Learning","title":"Intro_To_Deep_Learning","test_results":null,"modified":"2024-06-13 08:34:28 +0000","mermaid":"flowchart TD\n  0[\"ℹ️ Input Dataset\\nX_test\"];\n  style 0 stroke:#2c3143,stroke-width:4px;\n  1[\"ℹ️ Input Dataset\\nX_train\"];\n  style 1 stroke:#2c3143,stroke-width:4px;\n  2[\"ℹ️ Input Dataset\\ny_test\"];\n  style 2 stroke:#2c3143,stroke-width:4px;\n  3[\"ℹ️ Input Dataset\\ny_train\"];\n  style 3 stroke:#2c3143,stroke-width:4px;\n  4[\"Create a deep learning model architecture\"];\n  5[\"Create deep learning model\"];\n  4 -->|outfile| 5;\n  6[\"Deep learning training and evaluation\"];\n  5 -->|outfile| 6;\n  1 -->|output| 6;\n  3 -->|output| 6;\n  7[\"Model Prediction\"];\n  6 -->|outfile_object| 7;\n  6 -->|outfile_weights| 7;\n  0 -->|output| 7;\n  8[\"Machine Learning Visualization Extension\"];\n  7 -->|outfile_predict| 8;\n  2 -->|output| 8;"}],"api":"https://training.galaxyproject.org/training-material/api/topics/statistics/tutorials/intro_deep_learning/tutorial.json","tools":["toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_builder/keras_model_builder/0.5.0","toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_config/keras_model_config/0.5.0","toolshed.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/1.0.8.2","toolshed.g2.bx.psu.edu/repos/bgruening/ml_visualization_ex/ml_visualization_ex/1.0.8.2","toolshed.g2.bx.psu.edu/repos/bgruening/model_prediction/model_prediction/1.0.8.2"],"supported_servers":{"exact":[{"url":"https://usegalaxy.be/","name":"UseGalaxy.be","usegalaxy":false},{"url":"https://usegalaxy.cz/","name":"UseGalaxy.cz","usegalaxy":false},{"url":"https://usegalaxy.eu","name":"UseGalaxy.eu","usegalaxy":true},{"url":"https://usegalaxy.no/","name":"UseGalaxy.no","usegalaxy":false},{"url":"https://usegalaxy.org","name":"UseGalaxy.org (Main)","usegalaxy":true},{"url":"https://usegalaxy.org.au","name":"UseGalaxy.org.au","usegalaxy":true}],"inexact":[]},"topic_name_human":"Statistics and machine learning","admin_install":{"install_tool_dependencies":true,"install_repository_dependencies":true,"install_resolver_dependencies":true,"tools":[{"name":"keras_model_builder","owner":"bgruening","revisions":"772e0e89fc68","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"keras_model_config","owner":"bgruening","revisions":"8a794e6d3388","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"keras_train_and_eval","owner":"bgruening","revisions":"f1b9a42d6809","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"ml_visualization_ex","owner":"bgruening","revisions":"05143043ca13","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"model_prediction","owner":"bgruening","revisions":"83228baae3c5","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"}]},"admin_install_yaml":"---\ninstall_tool_dependencies: true\ninstall_repository_dependencies: true\ninstall_resolver_dependencies: true\ntools:\n- name: keras_model_builder\n  owner: bgruening\n  revisions: 772e0e89fc68\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: keras_model_config\n  owner: bgruening\n  revisions: 8a794e6d3388\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: keras_train_and_eval\n  owner: bgruening\n  revisions: f1b9a42d6809\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: ml_visualization_ex\n  owner: bgruening\n  revisions: 05143043ca13\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: model_prediction\n  owner: bgruening\n  revisions: 83228baae3c5\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n","tours":false,"video":false,"translations":{"tutorial":[],"slides":[],"video":false},"license":"CC-BY-4.0","type":"tutorial"}
{"layout":"tutorial_slides","title":"Convolutional neural networks (CNN) \n Deep Learning - Part 3","zenodo_link":"https://zenodo.org/record/4697906","questions":["What is a convolutional neural network (CNN)?","What are some applications of CNN?"],"objectives":["Understand the inspiration behind CNN and learn the CNN architecture","Learn the convolution operation and its parameters","Learn how to create a CNN using Galaxy’s deep learning tools","Solve an image classification problem on MNIST digit classification dataset using CNN in Galaxy"],"requirements":[{"type":"internal","topic_name":"statistics","tutorials":["intro_deep_learning","FNN","RNN"]}],"time_estimation":"2H","contributors":["kxk302"],"recordings":[{"captioners":["FilipposZ"],"date":"2021-02-15","galaxy_version":"21.01","length":"1H","youtube_id":"P1NVYOJrv_4","speakers":["kxk302"]}],"js_requirements":{"mathjax":null,"mermaid":false},"short_id":"S00088","url":"/topics/statistics/tutorials/CNN/slides.html","topic_name":"statistics","tutorial_name":"CNN","dir":"topics/statistics/tutorials/CNN","symlink":null,"id":"statistics/CNN","ref_tutorials":["<p>Artificial neural networks are a machine learning discipline that have been successfully applied to problems\nin pattern classification, clustering, regression, association, time series prediction, optimiztion, and control <span class=\"citation\"><a href=\"#JainEtAl\">Jain <i>et al.</i> 1996</a></span>.\nWith the increasing popularity of social media in the past decade, image and video processing tasks have become very\nimportant. The previous neural network architectures (e.g. feedforward neural networks) could not scale up to handle\nimage and video processing tasks. This gave way to the development of convolutional neural networks that are specifically\ntailored to image and video processing tasks. In this tutorial, we explain what convolutional neural networks are, discuss\ntheir architecture, and solve an image classification problem using MNIST digit classification dataset using a CNN in Galaxy.</p>\n\n<blockquote class=\"agenda\">\n  <agenda-title></agenda-title>\n\n  <p>In this tutorial, we will cover:</p>\n\n<ol id=\"markdown-toc\">\n  <li><a href=\"#limitations-of-feedforward-neural-networks-fnn-for-image-processing\" id=\"markdown-toc-limitations-of-feedforward-neural-networks-fnn-for-image-processing\">Limitations of feedforward neural networks (FNN) for image processing</a></li>\n  <li><a href=\"#inspiration-for-convolutional-neural-networks\" id=\"markdown-toc-inspiration-for-convolutional-neural-networks\">Inspiration for convolutional neural networks</a></li>\n  <li><a href=\"#architecture-of-cnn\" id=\"markdown-toc-architecture-of-cnn\">Architecture of CNN</a></li>\n  <li><a href=\"#mnist-dataset\" id=\"markdown-toc-mnist-dataset\">MNIST dataset</a></li>\n  <li><a href=\"#get-data\" id=\"markdown-toc-get-data\">Get data</a></li>\n  <li><a href=\"#classification-of-mnist-dataset-images-with-cnn\" id=\"markdown-toc-classification-of-mnist-dataset-images-with-cnn\">Classification of MNIST dataset images with CNN</a></li>\n  <li><a href=\"#conclusion\" id=\"markdown-toc-conclusion\">Conclusion</a></li>\n</ol>\n\n</blockquote>\n\n<h2 id=\"limitations-of-feedforward-neural-networks-fnn-for-image-processing\">Limitations of feedforward neural networks (FNN) for image processing</h2>\n\n<p>In a fully connected FNN (Figure 1), all the nodes in a layer are connected to all the nodes in the next layer. Each connection has a weight\n\\(w\\_{i,j}\\) that needs to be learned by the learning algorithm. Lets say our input is a 64 pixel by 64 pixel grayscale image. Each grayscale\npixel is represented by 1 value, usually between 0 to 255, where 0 represents black, 255 represents white, and the values in between represent\nvarious shades of gray. Since each grayscale pixel can be represented by 1 value, we say the <em>channel</em> size is 1. Such an image can be represented\nby 64 X 64 X 1 = 4,096 values (rows X columns X channels). Hence, the input layer of a FNN processing such an image has 4096 nodes.</p>\n\n<p>Lets assume the next layer has 500 nodes. Since all the nodes in subsequent layers are fully connected, we will have 4,096 X 500 = 2,048,000 weights\nbetween the input and the first hidden layer. For complex problems, we usually need multiple hidden layers in our FNN, as a simpler FNN may not be\nable to learn the model mapping the inputs to outputs in the training data. Having multiple hidden layers compounds the problem of having many weights\nin our FNN. Having many weights makes the learning process more difficult as the dimension of the search space is increased. It also makes the training\nmore time and resource consuming and increases the likelihood of overfitting. This problem is further compunded for color images. Unlike grayscale\nimages, each pixel in a color image is represented by 3 values, representing red, green, and blue colors (Called RGB color mode), where every color\ncan be represented by various combination of these primary colors. Since each color pixel can be represented by 3 values, we say the <strong>channel</strong> size\nis 3. Such an image can be represented by 64 X 64 X 3 = 12,288 values (rows X columns X channels). The number of weights between the input layer and\nthe first hidden layer with 500 nodes is now 12,288 X 500 = 6,144,000. It is clear that a FNN cannot scale to handle larger images\n(<span class=\"citation\"><a href=\"#OSheaEtAl\">O’Shea and Nash 2015</a></span>) and that we need a more scalable architecture.</p>\n\n<figure id=\"figure-1\" style=\"max-width: 90%;\"><img src=\"../../images/FFNN.png\" alt=\"Neurons forming the input, output, and hidden layers of a multi-layer feedforward neural network. \" width=\"361\" height=\"461\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/FFNN.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 1</strong>:</span> Feedforward neural network with a hidden layer. Biases to hidden/output layer neurons are omitted for clarity</figcaption></figure>\n\n<p>Another problem with using FNN for image processing is that a 2 dimensional image is represented as a 1 dimensional vector in the input layer,\nhence, any spatial relationship in the data is ignored. CNN, on the other hand, maintains the spatial structure of the data, and is better suited\nfor finding spatial relationships in the image data.</p>\n\n<h2 id=\"inspiration-for-convolutional-neural-networks\">Inspiration for convolutional neural networks</h2>\n\n<p>In 1959 Hubel and Wiesel conducted an experiment to understand how the visual cortex of the brain processes visual information (<span class=\"citation\"><a href=\"#HubelWiesel\">Hubel and Wiesel 1959</a></span>).\nThey recorded the activity of the neurons in the visual cortex of a cat while moving a bright line in front of the cat. They noticed that some cells fire\nwhen the bright line is shown at a particular angle and a particular location (They called these <strong>simple</strong> cells). Other neurons fired when the bright\nline was shown regardless of the angle/location and seemed to detect movement (They called these <strong>complex</strong> cells). It seemed complex cells receive\ninputs from multiple simple cells and have an hierarchical structure. Hubel and Wiesel won the Noble prize for their findings in 1981.</p>\n\n<p>In 1980, inspired by hierarchical structure of complex and simple cells, Fukushima proposed <em>Neocognitron</em> (<span class=\"citation\"><a href=\"#Fukishima\">Fukushima 1988</a></span>), a hierarchical neural\nnetwork used for handwritten Japanese character recognition. Neocognitron was the first CNN, and had its own training algorithm. In 1989, LeCun et. al.\n(<span class=\"citation\"><a href=\"#LeCunEtAl\">LeCun <i>et al.</i> 1989</a></span>) proposed a CNN that could be trained by backpropagation algorithm. CNN gained immense popularity when they outperformed other\nmodels at ILSVRC (ImageNet Large Scale Visual Recognition Challenge). ILSVRC is a competition in object classification and detection on hundreds of\nobject categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty\ninstitutions (<span class=\"citation\"><a href=\"#RussakovskyEtAl\">Russakovsky <i>et al.</i> 2015</a></span>). Notable CNN architectures that won ILSVRC are AlexNet in 2012 (<span class=\"citation\"><a href=\"#KrizhevskyEtAl\">Krizhevsky <i>et al.</i> 2012</a></span>), ZFNet in 2013 (\n<span class=\"citation\"><a href=\"#ZeilerEtAl\">Zeiler and Fergus 2013</a></span>), GoogLeNet and VGG in 2014 (<span class=\"citation\"><a href=\"#SzegedyEtAl\">Szegedy <i>et al.</i> 2014</a></span>, <span class=\"citation\"><a href=\"#SimonyanEtAl\">Simonyan and Zisserman 2015</a></span>), and ResNet in 2015 (<span class=\"citation\"><a href=\"#HeEtAl\">He <i>et al.</i> 2015</a></span>).</p>\n\n<h2 id=\"architecture-of-cnn\">Architecture of CNN</h2>\n\n<p>A typical CNN has the following 4 layers (<span class=\"citation\"><a href=\"#OSheaEtAl\">O’Shea and Nash 2015</a></span>)</p>\n\n<ol>\n  <li>Input layer</li>\n  <li>Convolution layer</li>\n  <li>Pooling layer</li>\n  <li>Fully connected layer</li>\n</ol>\n\n<p>Please note that we will explain a 2 dimensional (2D) CNN here. But the same concepts apply to a 1 (or 3) dimensional CNN as well.</p>\n\n<h3 id=\"input-layer\">Input layer</h3>\n\n<p>The input layer represents the input to the CNN. An example input, could be a 28 pixel by 28 pixel grayscale image. Unlike FNN, we do not\n“flatten” the input to a 1D vector, and the input is presented to the network in 2D as a 28 x 28 matrix. This makes capturing\nspatial relationships easier.</p>\n\n<h3 id=\"convolution-layer\">Convolution layer</h3>\n\n<p>The convolution layer is composed of multiple filters (also called kernels). Filters for a 2D image are also 2D. Suppose we have\na 28 pixel by 28 pixel grayscale image. Each pixel is represented by a number between 0 and 255, where 0 represents the color black,\n255 represents the color white, and the values in between represent different shades of gray. Suppose we have a 3 by 3 filter (9\nvalues in total), and the values are randomly set to 0 or 1. Convolution is the process of placing the 3 by 3 filter on the top left\ncorner of the image, multiplying filter values by the pixel values and adding the results, moving the filter to the right one pixel at\na time and repeating this process. When we get to the top right corner of the image, we simply move the filter down one pixel and\nrestart from the left. This process ends when we get to the bottom right corner of the image.</p>\n\n<figure id=\"figure-2\" style=\"max-width: 90%;\"><img src=\"../../images/Conv_no_padding_no_strides.gif\" alt=\"A 3 by 3 filter applied to a 4 by 4 image, resulting in a 2 by 2 image. \" width=\"244\" height=\"259\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/Conv_no_padding_no_strides.gif\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 2</strong>:</span> A 3 by 3 filter applied to a 4 by 4 image, resulting in a 2 by 2 image (<span class=\"citation\"><a href=\"#DumoulinVisin\">Dumoulin and Visin 2016</a></span>)</figcaption></figure>\n\n<p>Covolution operator has the following parameters:</p>\n\n<ol>\n  <li>Filter size</li>\n  <li>Padding</li>\n  <li>Stride</li>\n  <li>Dilation</li>\n  <li>Activation function</li>\n</ol>\n\n<p>Filter size can be 5 by 5, 3 by 3, and so on. Larger filter sizes should be avoided as the learning algorithm needs to learn filter values (weights),\nand larger filters increase the number of weights to be learned (more compute capacity, more training time, more chance of overfitting). Also, odd\nsized filters are preferred to even sized filters, due to the nice geometric property of all the input pixels being around the output pixel.</p>\n\n<p>If you look at Figure 2 you see that after applying a 3 by 3 filter to a 4 by 4 image, we end up with a 2 by 2 image – the size of the image has gone\ndown. If we want to keep the resultant image size the same, we can use <em>padding</em>. We pad the input in every direction with 0’s before applying the\nfilter. If the padding is 1 by 1, then we add 1 zero in evey direction. If its 2 by 2, then we add 2 zeros in every direction, and so on.</p>\n\n<figure id=\"figure-3\" style=\"max-width: 90%;\"><img src=\"../../images/Conv_same_padding_no_strides.gif\" alt=\"A 3 by 3 filter applied to a 5 by 5 image, with padding of 1, resulting in a 5 by 5 image. \" width=\"395\" height=\"449\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/Conv_same_padding_no_strides.gif\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 3</strong>:</span> A 3 by 3 filter applied to a 5 by 5 image, with padding of 1, resulting in a 5 by 5 image (<span class=\"citation\"><a href=\"#DumoulinVisin\">Dumoulin and Visin 2016</a></span>)</figcaption></figure>\n\n<p>As mentioned before, we start the convolution by placing the filter on the top left corner of the image, and after multiplying filter and image\nvalues (and adding them), we move the filter to the right and repeat the process. How many pixels we move to the right (or down) is the <em>stride</em>.\nIn figure 2 and 3, the stride of the filter is 1. We move the filter one pixel to the right (or down). But we could use a different stride. Figure 4\nshows an example of using stride of 2.</p>\n\n<figure id=\"figure-4\" style=\"max-width: 90%;\"><img src=\"../../images/Conv_no_padding_strides.gif\" alt=\"A 3 by 3 filter applied to a 5 by 5 image, with stride of 2, resulting in a 2 by 2 image. \" width=\"294\" height=\"288\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/Conv_no_padding_strides.gif\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 4</strong>:</span> A 3 by 3 filter applied to a 5 by 5 image, with stride of 2, resulting in a 2 by 2 image (<span class=\"citation\"><a href=\"#DumoulinVisin\">Dumoulin and Visin 2016</a></span>)</figcaption></figure>\n\n<p>When we apply a, say 3 by 3, filter to an image, our filter’s output is affected by pixels in a 3 by 3 subset of the image. If we like to have a\nlarger <em>receptive field</em> (portion of the image that affect our filter’s output), we could use <em>dilation</em>. If we set the dilation to 2 (Figure 5),\ninstead of a contiguous 3 by 3 subset of the image, every other pixel of a 5 by 5 subset of the image affects the filter’s output.</p>\n\n<figure id=\"figure-5\" style=\"max-width: 90%;\"><img src=\"../../images/Conv_dilation.gif\" alt=\"A 3 by 3 filter applied to a 7 by 7 image, with dilation of 2, resulting in a 3 by 3 image. \" width=\"395\" height=\"381\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/Conv_dilation.gif\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 5</strong>:</span> A 3 by 3 filter applied to a 7 by 7 image, with dilation of 2, resulting in a 3 by 3 image (<span class=\"citation\"><a href=\"#DumoulinVisin\">Dumoulin and Visin 2016</a></span>)</figcaption></figure>\n\n<p>After the filter scans the whole image, we apply an activation function to filter output to introduce non-linearlity. The preferred activation function\nused in CNN is ReLU or one its variants like Leaky ReLU (<span class=\"citation\"><a href=\"#NwankpaEtAl\">Nwankpa <i>et al.</i> 2018</a></span>). ReLU leaves pixels with positive values in filter output as is, and\nreplacing negative values with 0 (or a small number in case of Leaky ReLU). Figure 6 shows the results of applying ReLU activation function to a filter\noutput.</p>\n\n<figure id=\"figure-6\" style=\"max-width: 90%;\"><img src=\"../../images/Conv_ReLU.png\" alt=\"Two matrices representing filter output before and after ReLU activation function is applied. \" width=\"406\" height=\"159\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/Conv_ReLU.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 6</strong>:</span> Applying ReLU activation function to filter output</figcaption></figure>\n\n<p>Given the input size, filter size, padding, stride and dilation you can calculate the output size of the convolution operation as below.</p>\n\n\\[\\frac{(\\text{input size} - \\text{(filter size + (filter size -1)*(dilation - 1)})) + (2*padding)}{stride} + 1\\]\n\n<figure id=\"figure-7\" style=\"max-width: 90%;\"><img src=\"../../images/Conv_single_input_channel.png\" alt=\"One matrix representing an input vector and another matrix representing a filter, along with calculation for single input channel two dimensional convolution operation. \" width=\"564\" height=\"254\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/Conv_single_input_channel.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 7</strong>:</span> Illustration of single input channel two dimensional convolution</figcaption></figure>\n\n<p>Figure 7 illustrates the calculations for a convolution operation, via a 3 by 3 filter on a single channel 5 by 5 input vector (5 x 5 x 1). Figure 8\nillustrates the calculations when the input vector has 3 channels (5 x 5 x 3). To show this in 2 dimensions, we are displaying each channel in input\nvector and filter separately. Figure 9 shows a sample multi-channel 2D convolution in 3 dimensions.</p>\n\n<figure id=\"figure-8\" style=\"max-width: 90%;\"><img src=\"../../images/Conv_multiple_input_channel.png\" alt=\"Three matrices representing an input vector and another three matrices representing a filter, along with calculation for multiple input channel two dimensional convolution operation . \" width=\"643\" height=\"600\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/Conv_multiple_input_channel.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 8</strong>:</span> Illustration of multiple input channel two dimensional convolution</figcaption></figure>\n\n<p>As Figures 8 and 9 show the output of a multi-channel 2 dimensional filter is a single channel 2 dimensional image. Applying <em>multiple</em> filters to the\ninput image results in a multi-channel 2 dimensional image for the output. For example, if the input image is 28 by 28 by 3 (rows x columns x channels),\nand we apply a 3 by 3 filter with 1 by 1 padding, we would get a 28 by 28 by 1 image. If we apply 15 filters to the input image, our output would be 28\nby 28 by 15. Hence, the number of filters in a convolution layer allows us to increase or decrease the channel size.</p>\n\n<figure id=\"figure-9\" style=\"max-width: 90%;\"><img src=\"../../images/Conv_multiple_channel_3d.gif\" alt=\"Multiple cubes representing input vector, filter, and output in a 3 channel 2 dimensional convolution operation. \" width=\"948\" height=\"548\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/Conv_multiple_channel_3d.gif\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 9</strong>:</span> Three dimensional illustration of multiple input channel two dimensional convolution (Source: https://thomelane.github.io/convolutions/2DConvRGB.html)</figcaption></figure>\n\n<h3 id=\"pooling-layer\">Pooling layer</h3>\n\n<p>The pooling layer performs down sampling to reduce the spatial dimensionality of the input. This decreases the number of parameters, which in turn\nreduces the learning time and computation, and the likelihood of overfitting. The most popular type of pooling is <em>max pooling</em>. Its usually a 2 by 2\nfilter with a stride of 2 that returns the maximum value as it slides over the input data (similar to convolution filters).</p>\n\n<h3 id=\"fully-connected-layer\">Fully connected layer</h3>\n\n<p>The last layer in a CNN is a fully connected layer. We connect all the nodes from the previous layer to this fully connected layer, which is responsible\nfor classification of the image.</p>\n\n<figure id=\"figure-10\" style=\"max-width: 90%;\"><img src=\"../../images/Conv_CNN.png\" alt=\"A convolutional neural network with 3 convolution layers followed by 3 pooling layers. \" width=\"842\" height=\"265\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/Conv_CNN.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 10</strong>:</span> A convolutional neural network with 3 convolution layers followed by 3 pooling layers (<span class=\"citation\"><a href=\"#OSheaEtAl\">O’Shea and Nash 2015</a></span>)</figcaption></figure>\n\n<p>As shown in Figure 10, a typical CNN usually has more than one convolution layer plus pooling layer. Each convolution plus pooling layer is responsible\nfor feature extraction at a different level of abstraction. For example, the filters in the first layer could detect horizental, vertical, and diagonal\nedges. The filters in the next layer could detect shapes, and the filters in the last layer could detect collection of shapes. Filter values are randomly\ninitialized and are learned by the learning algorithm. This makes CNN very powerful as they not only do classification, but can also automatically do\nfeature extraction. This distinguishes CNN from other classification techniques (like Support Vector Machines), which cannot do feature extraction.</p>\n\n<h2 id=\"mnist-dataset\">MNIST dataset</h2>\n\n<p>The MNIST database of handwritten digits (<span class=\"citation\"><a href=\"#LeCunMnist\">LeCun <i>et al.</i> 2010</a></span>) is composed of a training set of 60,000 images and a test set of 10,000 images. The digits\nhave been size-normalized and centered in a fixed-size image (28 by 28 pixels). Images are grayscale, where each pixel is represented by a number between\n0 and 255 (0 for black, 255 for white, and other values for different shades of gray). MNIST database is a standard image classification dataset and is used\nto compare various Machine Learning techniques.</p>\n\n<h2 id=\"get-data\">Get data</h2>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Data upload</hands-on-title>\n\n  <ol>\n    <li>\n      <p>Make sure you have an empty analysis history.</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-creating-a-new-history\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-creating-a-new-history\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Creating a new history</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <p>Click the <i class=\"fas fa-plus\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">new-history</span> icon at the top of the history panel:</p>   <p><img src=\"/training-material/shared/images/history_create_new.svg\" alt=\"UI for creating new history\" /></p>   <!-- the original drawing can be found here https://docs.google.com/drawings/d/1cCBrLAo4kDGic5QyB70rRiWJAKTenTU8STsKDaLcVU8/edit?usp=sharing --> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p><strong>Rename your history</strong> to make it easy to recognize</p>\n\n      <blockquote class=\"tip\">\n        <tip-title>Rename a history</tip-title>\n\n        <ul>\n          <li>\n            <p>Click on the title of the history (by default the title is <code class=\"language-plaintext highlighter-rouge\">Unnamed history</code>)</p>\n\n            <p><a href=\"../../../../shared/images/rename_history.png\" rel=\"noopener noreferrer\"><img src=\"../../../../shared/images/rename_history.png\" alt=\"Renaming history. \" width=\"270\" height=\"320\" loading=\"lazy\" /></a></p>\n          </li>\n          <li>Type <code class=\"language-plaintext highlighter-rouge\">Galaxy Introduction</code> as the name</li>\n          <li>Press <kbd>Enter</kbd></li>\n        </ul>\n\n      </blockquote>\n    </li>\n    <li>\n      <p>Import the files from <a href=\"https://zenodo.org/record/4697906\">Zenodo</a></p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>https://zenodo.org/record/4697906/files/X_train.tsv\nhttps://zenodo.org/record/4697906/files/y_train.tsv\nhttps://zenodo.org/record/4697906/files/X_test.tsv\nhttps://zenodo.org/record/4697906/files/y_test.tsv\n</code></pre></div>      </div>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-importing-via-links\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-importing-via-links\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Importing via links</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Copy the link location</li>   <li>     <p>Click <i class=\"fas fa-upload\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-upload</span> <strong>Upload Data</strong> at the top of the tool panel</p>   </li>   <li>Select <i class=\"fa fa-edit\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-wf-edit</span> <strong>Paste/Fetch Data</strong></li>   <li>     <p>Paste the link(s) into the text field</p>   </li>   <li>     <p>Press <strong>Start</strong></p>   </li>   <li><strong>Close</strong> the window</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p>Rename the datasets as <code class=\"language-plaintext highlighter-rouge\">X_train</code>, <code class=\"language-plaintext highlighter-rouge\">y_train</code>, <code class=\"language-plaintext highlighter-rouge\">X_test</code>, and <code class=\"language-plaintext highlighter-rouge\">y_test</code> respectively.</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-renaming-a-dataset\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-renaming-a-dataset\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Renaming a dataset</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Click on the <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, change the <strong>Name</strong> field</li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p>Check that the datatype of all four datasets is <code class=\"language-plaintext highlighter-rouge\">tabular</code>.</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-changing-the-datatype\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-changing-the-datatype\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Changing the datatype</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Click on the <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, click <i class=\"fas fa-database\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-chart-select-data</span> <strong>Datatypes</strong> tab on the top</li>   <li>In the <i class=\"fas fa-database\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-chart-select-data</span> <strong>Assign Datatype</strong>, select <code class=\"language-plaintext highlighter-rouge\">datatypes</code> from “<em>New type</em>” dropdown     <ul>       <li>Tip: you can start typing the datatype into the field to filter the dropdown menu</li>     </ul>   </li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n  </ol>\n\n</blockquote>\n\n<h2 id=\"classification-of-mnist-dataset-images-with-cnn\">Classification of MNIST dataset images with CNN</h2>\n\n<p>In this section, we define a CNN and train it using MNIST dataset training data. The goal is to learn a model such that given an image\nof a digit we can predict whether the digit (0 to 9). We then evaluate the trained CNN on the test dataset and plot the confusion matrix.</p>\n\n<p>In order to train the CNN, we must have the One-Hot Encoding (OHE) representation of the training\nlabels. This is needed to calculate the categorical cross entropy loss function. OHE encodes labels\nas a <strong>one-hot</strong> numeric array, where only one element is 1 and the rest are 0’s. For example, if\nwe had 3 fruits (apple, orange, banana) and their labels were 1, 2, and 3, the OHE\nrepresntation of apple would be (1,0,0), the OHE representation of orange would be (0,1,0), and the\nOHE representation of banana would be (0,0,1). For apple with label 1, the first element of array\nis 1 (and the rest are 0’s); For Orange with label 2, the second element of the array is 1 (and the\nrest are 0’s); And for Banana with label 3, the third element of the array is 1 (and the rest are 0’s).\nWe have 10 digits in our dataset and we would just have an array of size 10, where only one\nelement is 1, corresponding to the digit, and the rest are 0’s.</p>\n\n<h3 id=\"create-one-hot-encoding-ohe-representation-of-training-labels\">Create One-Hot Encoding (OHE) representation of training labels</h3>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>One-Hot Encoding</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_to_categorical/sklearn_to_categorical/1.0.10.0\" title=\"To categorical tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>To categorical</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.10.0)</span>\n      <ul>\n        <li><em>“Input file”</em> : Select <code class=\"language-plaintext highlighter-rouge\">y_train</code></li>\n        <li><em>“Does the dataset contain header?”</em> : Select <code class=\"language-plaintext highlighter-rouge\">No</code></li>\n        <li><em>“Total number of classes”</em>: Select <code class=\"language-plaintext highlighter-rouge\">10</code></li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n\n</blockquote>\n\n<h3 id=\"create-a-deep-learning-model-architecture\">Create a deep learning model architecture</h3>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Model config</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_config/keras_model_config/1.0.10.0\" title=\"Create a deep learning model architecture tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Create a deep learning model architecture</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.10.0)</span>\n      <ul>\n        <li><em>“Select keras model type”</em>: <code class=\"language-plaintext highlighter-rouge\">sequential</code></li>\n        <li><em>“input_shape”</em>: <code class=\"language-plaintext highlighter-rouge\">(784,)</code></li>\n        <li>In <em>“LAYER”</em>:\n          <ul>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“1: LAYER”</em>:\n              <ul>\n                <li><em>“Choose the type of layer”</em>: <code class=\"language-plaintext highlighter-rouge\">Core -- Reshape</code>\n                  <ul>\n                    <li><em>“target_shape”</em>: <code class=\"language-plaintext highlighter-rouge\">(28,28,1)</code></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“2: LAYER”</em>:\n              <ul>\n                <li><em>“Choose the type of layer”</em>: <code class=\"language-plaintext highlighter-rouge\">Convolutional -- Conv2D</code>\n                  <ul>\n                    <li><em>“filters”</em>: <code class=\"language-plaintext highlighter-rouge\">64</code></li>\n                    <li><em>“kernel_size”</em>: <code class=\"language-plaintext highlighter-rouge\">3</code></li>\n                    <li><em>“Activation function”</em>: <code class=\"language-plaintext highlighter-rouge\">relu</code></li>\n                    <li><em>“Type in key words arguments if different from the default”</em>: <code class=\"language-plaintext highlighter-rouge\">padding='same'</code></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“3: LAYER”</em>:\n              <ul>\n                <li><em>“Choose the type of layer”</em>: <code class=\"language-plaintext highlighter-rouge\">Pooling -- MaxPooling2D</code>\n                  <ul>\n                    <li><em>“pool_size”</em>: <code class=\"language-plaintext highlighter-rouge\">(2,2)</code></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“4: LAYER”</em>:\n              <ul>\n                <li><em>“Choose the type of layer”</em>: <code class=\"language-plaintext highlighter-rouge\">Convolutional -- Conv2D</code>\n                  <ul>\n                    <li><em>“filters”</em>: <code class=\"language-plaintext highlighter-rouge\">32</code></li>\n                    <li><em>“kernel_size”</em>: <code class=\"language-plaintext highlighter-rouge\">3</code></li>\n                    <li><em>“Activation function”</em>: <code class=\"language-plaintext highlighter-rouge\">relu</code></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“5: LAYER”</em>:\n              <ul>\n                <li><em>“Choose the type of layer”</em>: <code class=\"language-plaintext highlighter-rouge\">Pooling -- MaxPooling2D</code>\n                  <ul>\n                    <li><em>“pool_size”</em>: <code class=\"language-plaintext highlighter-rouge\">(2,2)</code></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“6: LAYER”</em>:\n              <ul>\n                <li><em>“Choose the type of layer”</em>: <code class=\"language-plaintext highlighter-rouge\">Core -- Flatten</code></li>\n              </ul>\n            </li>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“7: LAYER”</em>:\n              <ul>\n                <li><em>“Choose the type of layer”</em>: <code class=\"language-plaintext highlighter-rouge\">Core -- Dense</code>\n                  <ul>\n                    <li><em>“units”</em>”: <code class=\"language-plaintext highlighter-rouge\">10</code></li>\n                    <li><em>“Activation function”</em>: <code class=\"language-plaintext highlighter-rouge\">softmax</code></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n</blockquote>\n\n<p>Each image is passed in as a 784 dimensional vector (28 x 28 = 784). The reshape layer reshapes it into (28, 28, 1) dimensions – 28 rows (image height), 28 columns (image width), and\n1 channel. Channel is 1 since the image is grayscale and each pixel can be represented by one integer. Color images are represented by 3 integers (RGB\nvalues) and have channel size 3. Our CNN then has 2 convolution + pooling layers. First convolution layer has 64 filters (output would be 64 dimensional),\nand filter size is 3 x 3. Second convolutional layer has 32 filters (output would be 32 dimensional), and filter size is 3 x 3. Both pooling layers are\nMaxPool layers with pool size of 2 by 2. Afterwards, we flatten the previous layers output (every row/colum/channel would be an individual node). Finally,\nwe add a fully connected layer with 10 nodes and use a softmax activation function to get the probability of each digit. Digit with the highest\nprobability is predicted by CNN. The model config can be downloaded as a JSON file.</p>\n\n<h3 id=\"create-a-deep-learning-model\">Create a deep learning model</h3>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Model builder (Optimizer, loss function, and fit parameters)</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_builder/keras_model_builder/1.0.10.0\" title=\"Create deep learning model tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Create deep learning model</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.10.0)</span>\n      <ul>\n        <li><em>“Choose a building mode”</em>: <code class=\"language-plaintext highlighter-rouge\">Build a training model</code></li>\n        <li><em>“Select the dataset containing model configuration”</em>: Select the <em>Keras Model Config</em> from the previous step.</li>\n        <li><em>“Do classification or regression?”</em>: <code class=\"language-plaintext highlighter-rouge\">KerasGClassifier</code></li>\n        <li>In <em>“Compile Parameters”</em>:\n          <ul>\n            <li><em>“Select a loss function”</em>: <code class=\"language-plaintext highlighter-rouge\">categorical_crossentropy</code></li>\n            <li><em>“Select an optimizer”</em>: <code class=\"language-plaintext highlighter-rouge\">Adam - Adam optimizer </code></li>\n            <li><em>“Select metrics”</em>: <code class=\"language-plaintext highlighter-rouge\">acc/accuracy</code></li>\n          </ul>\n        </li>\n        <li>In <em>“Fit Parameters”</em>:\n          <ul>\n            <li><em>“epochs”</em>: <code class=\"language-plaintext highlighter-rouge\">2</code></li>\n            <li><em>“batch_size”</em>: <code class=\"language-plaintext highlighter-rouge\">500</code></li>\n          </ul>\n        </li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n</blockquote>\n\n<p>A loss function measures how different the predicted output is versus the expected output. For multi-class classification problems, we use\n<em>categorical cross entropy</em> as loss function. Epochs is the number of times the whole training data is used to train the model. Setting <em>epochs</em> to 2\nmeans each training example in our dataset is used twice to train our model. If we update network weights/biases after all the training data is\nfeed to the network, the training will be very slow (as we have 60000 training examples in our dataset). To speed up the training, we present\nonly a subset of the training examples to the network, after which we update the weights/biases. <em>batch_size</em> decides the size of this subset.\nThe model builder can be downloaded as a zip file.</p>\n\n<h3 id=\"deep-learning-training-and-evaluation\">Deep learning training and evaluation</h3>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Training the model</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/1.0.11.0\" title=\"Deep learning training and evaluation tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Deep learning training and evaluation</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.11.0)</span>\n      <ul>\n        <li><em>“Select a scheme”</em>: <code class=\"language-plaintext highlighter-rouge\">Train and Validate</code></li>\n        <li><em>“Choose the dataset containing pipeline/estimator object”</em>: Select the <em>Keras Model Builder</em> from the previous step.</li>\n        <li><em>“Select input type:”</em>: <code class=\"language-plaintext highlighter-rouge\">tabular data</code>\n          <ul>\n            <li><em>“Training samples dataset”</em>: Select <code class=\"language-plaintext highlighter-rouge\">X_train</code> dataset</li>\n            <li><em>“Choose how to select data by column:”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns</code></li>\n            <li><em>“Dataset containing class labels or target values”</em>: Select the OHE representation of <code class=\"language-plaintext highlighter-rouge\">y_train</code> dataset</li>\n            <li><em>“Choose how to select data by column:”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns</code></li>\n          </ul>\n        </li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n\n</blockquote>\n\n<p>The training step generates 3 datasets. 1) accuracy of the trained model, 2) the trained model, downloadable as a zip file, and 3) the trained\nmodel weights, downloadable as an hdf5 file. These files are needed for prediction in the next step.</p>\n\n<h3 id=\"model-prediction\">Model Prediction</h3>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Testing the model</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/model_prediction/model_prediction/1.0.11.0\" title=\"Model Prediction tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Model Prediction</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.11.0)</span>\n      <ul>\n        <li><em>“Choose the dataset containing pipeline/estimator object”</em> : Select the trained model from the previous step.</li>\n        <li><em>“Choose the dataset containing weights for the estimator above”</em> : Select the trained model weights from the previous step.</li>\n        <li><em>“Select invocation method”</em>: <code class=\"language-plaintext highlighter-rouge\">predict</code></li>\n        <li><em>“Select input data type for prediction”</em>: <code class=\"language-plaintext highlighter-rouge\">tabular data</code></li>\n        <li><em>“Training samples dataset”</em>: Select <code class=\"language-plaintext highlighter-rouge\">X_test</code> dataset</li>\n        <li><em>“Choose how to select data by column:”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns</code></li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n\n</blockquote>\n\n<p>The prediction step generates 1 dataset. It’s a file that has predictions (0 to 9 for the predicted digits) for every image in the test dataset.</p>\n\n<h3 id=\"machine-learning-visualization-extension\">Machine Learning Visualization Extension</h3>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Creating the confusion matrix</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/ml_visualization_ex/ml_visualization_ex/1.0.11.0\" title=\"Machine Learning Visualization Extension tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Machine Learning Visualization Extension</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.11.0)</span>\n      <ul>\n        <li><em>“Select a plotting type”</em>: <code class=\"language-plaintext highlighter-rouge\">Confusion matrix for classes</code></li>\n        <li><em>“Select dataset containing the true labels”</em>”: <code class=\"language-plaintext highlighter-rouge\">y_test</code></li>\n        <li><em>“Choose how to select data by column:”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns</code></li>\n        <li><em>“Select dataset containing the predicted labels”</em>”: Select <code class=\"language-plaintext highlighter-rouge\">Model Prediction</code> from the previous step</li>\n        <li><em>“Does the dataset contain header:”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n\n</blockquote>\n\n<p><strong>Confusion Matrix</strong> is a table that describes the performance of a classification model. It lists the number of examples that were correctly\nclassified by the model, True positives (TP) and true negatives (TN). It also lists the number of examples that were classified as positive that\nwere actually negative (False positive, FP, or Type I error), and the number of examples that were classified as negative that were actually\npositive (False negative, FN, or Type 2 error). Given the confusion matrix, we can calculate <strong>precision</strong> and\n<strong>recall</strong> <span class=\"citation\"><a href=\"#TatbulEtAl\">Tatbul <i>et al.</i> 2018</a></span>. Precision is the fraction of predicted positives that are true positives (Precision = TP / (TP + FP)). Recall\nis the fraction of true positives that are predicted (Recall = TP / (TP + FN)). One way to describe the confusion matrix with just one value is\nto use the <strong>F score</strong>, which is the harmonic mean of precision and recall</p>\n\n\\[Precision = \\frac{\\text{True positives}}{\\text{True positives + False positives}}\\]\n\n\\[Recall = \\frac{\\text{True positives}}{\\text{True positives + False negatives}}\\]\n\n\\[F score = \\frac{2 * \\text{Precision * Recall}}{\\text{Precision + Recall}}\\]\n\n<figure id=\"figure-11\" style=\"max-width: 90%;\"><img src=\"../../images/Conv_confusion_matrix.png\" alt=\"Confusion matrix for MNIST image classification problem. \" width=\"875\" height=\"875\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/Conv_confusion_matrix.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 11</strong>:</span> Image classification confusion matrix</figcaption></figure>\n\n<p>Figure 11 is the resultant confusion matrix for our image problem. The first row in the table represents the <em>true</em> digit 0 class labels (we have\n967 + 2 + 1 + 8 + 2 = 980 digit 0 images). The second row represents the <em>true</em> digit 1 class labels (Again, we have 1,132 + 1 + 2 = 1,135 digit 1\nimages). Similarly, you can count the true class labels for digits 2 to 9 by adding up the numbers in the coresponding row. The first column from the\nleft represents the <em>predicted</em> digit 0 class labels (Our CNN predicted 967 + 4 + 2 + 8 +7 + 2 = 990 images as being digit 0). The second column from\nthe left represents the <em>predicted</em> digit 1 class labels (Our CNN predicted 1,132 + 7 + 3 + 12 + 3 + 5 = 1,162 images as being digit 1). Similarly,\nyou can count the predicted class labels for digits 2 to 9 by adding up the numbers in the corresponding column.</p>\n\n<p>For digit 0, looking at the top-left green cell, we see that our CNN has correctly predicted 967 images as digit 0 (True positives). Adding the numbers\nin the left most column besides the True positives (4 + 2 + 8 + 7 + 2 = 23), we see that our CNN has incorrectly predicted 23 images as being digit 0\n(False positives). Adding the numbers on the top row besides the True positives (2 + 1 + 8 + 2 = 13), we see that our CNN has incorrectly predicted\n13 digits 0 images as not being digit 0 (False negatives). Given these numbers we can calculate Precision, Recall, and the F score for digit 0 as\nfollows:</p>\n\n\\[Precision = \\frac{\\text{True positives}}{\\text{True positives + False positives}} = \\frac{967}{967 + 23} = 0.97\\]\n\n\\[Recall = \\frac{\\text{True positives}}{\\text{True positives + False negatives}} = \\frac{967}{967 + 13} = 0.98\\]\n\n\\[F score = \\frac{2 * \\text{Precision * Recall}}{\\text{Precision + Recall}} = \\frac{2 * 0.97 * 0.98}{0.97 + 0.98} = 0.97\\]\n\n<p>You can calculate the Precision, Recall, and F score for other digits in a similar manner.</p>\n\n<h1 id=\"conclusion\">Conclusion</h1>\n\n<p>In this tutorial, we explained the motivation for convolutional neural networks, explained their architecture, and discussed convolution\noperator and its parameters. We then used Galaxy to solve an image classification problem using CNN on MNIST dataset.</p>\n"],"ref_slides":["# What is a convolutional neural network (CNN)?\n\n???\n\nWhat is a convolutional neural network (CNN)?\n\n---\n\n# Convolutional Neural Network (CNN)\n\n- Increasing popularity of social media in past decade\n\t- Image and video processing tasks have become very important\n- FNN could not scale up to image and video processing tasks\n- CNN specifically tailored for image and video processing tasks\n\n---\n\n# Feedforward neural networks (FNN)\n\n- In FNN all nodes in a layer connected to all nodes in next layer\n\t- Each connection has a weight, must be learned by learning algorithm\n\n![Neurons forming the input, output, and hidden layers of a multi-layer feedforward neural network](/training-material/topics/statistics/images/FFNN.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Limitations of FNN\n\n- If input is 64 pixel by 64 pixel grayscale image\n\t- Each grayscale pixel represented by 1 value, usually between 0 to 255\n\t- Where 0 is black, 255 is white, and values in between are shades of gray\n- Since each grayscale pixel represented by 1 value, we say *channel* size is 1\n- Image represented by 64 x 64 x 1 = 4,096 values (rows x columns x channels)\n\t- Hence, input layer of FNN has 4096 nodes\n- Lets assume next layer has 500 nodes\n\t- Since FNN fully connected, we have 4,096 x 500 = 2,048,000 weights\n\n---\n\n# Limitations of FNN\n\n- For complex problems, we need multiple hidden layers in our FNN\n\t- Compunds the problem of having many weights\n- Having too many weights\n\t- Makes learning more difficult as dimension of search space is increased\n\t- Makes training more time/resource consuming\n\t- Increases the likelihood of overfitting\n\n- Problem is further compunded for color images\n\t- Each pixel in color image represented by 3 values (RGB color mode)\n\t- Since each pixel represented by 3 values, we say *channel* size is 3\n\t- Image represented by 64 x 64 x 3 = 12,288 values (rows x columns x channels)\n\t- Number of weights is now 12,288 x 500 = 6,144,000\n\n---\n\n# Limitations of FNN\n\n- Clear that FNN cannot scale to larger images (Too many weights)\n- Another problem with FNN\n\t- 2D image represented as 1D vector in input layer\n\t- Any spatial relationship in the data is ignored\n\n---\n\n# Inspiration for CNN\n\n- In 1959 Hubel & Wiesel did an experiment to understand how visual cortex of brain processes visual info\n\t- Recorded activity of neurons in visual cortex of a cat\n\t- While moving a bright line in front of the cat\n- Some cells fired when bright line is shown at a particular angle/location\n\t- Called these *simple* cells\n- Other cells fired when bright line was shown regardless of angle/location\n\t- Seemed to detect movement\n\t- Called these *complex* cells\n- Seemed complex cells receive inputs from multiple simple cells\n\t- Have an hierarchical structure\n- Hubel and Wiesel won Noble prize in 1981\n\n---\n\n# Inspiration for CNN\n\n- Inspired by complex/simple cells, Fukushima proposed *Neocognitron* (1980)\n\t- Hierarchical neural network used for handwritten Japanese character recognition\n\t- First CNN, had its own training algorithm\n- In 1989, LeCun proposed CNN that was trained by backpropagation\n- CNN got popular when outperformed other models at ImageNet Challenge\n\t- Competition in object classification/detection\n\t- On hundreds of object categories and millions of images\n\t- Run annually from 2010 to present\n- Notable CNN architectures that won ImageNet challenge\n\t- AlexNet (2012), ZFNet (2013), GoogLeNet & VGG (2014), ResNet (2015)\n\n---\n\n# Architecture of CNN\n\n- A typical CNN has 4 layers\n\t- Input layer\n\t- Convolution layer\n\t- Pooling layer\n\t- Fully connected layer\n\n- We will explain a 2D CNN here\n\t- Same concepts apply to a 1 (or 3) dimensional CNN\n\n---\n\n# Input layer\n\n- Example input a 28 pixel by 28 pixel grayscale image\n- Unlike FNN, we do not “flatten” the input to a 1D vector\n\t- Input is presented to network in 2D as 28 x 28 matrix\n\t- This makes capturing spatial relationships easier\n\n---\n\n# Convolution layer\n\n- Composed of multiple filters (kernels)\n- Filters for 2D image are also 2D\n- Suppose we have a 3 by 3 filter (9 values in total)\n\t- Values are randomly set to 0 or 1\n- Convolution: placing 3 by 3 filter on the top left corner of image\n\t- Multiply filter values by pixel values, add the results\n\t- Move filter to right one pixel at a time, and repeat this process\n\t- When at top right corner, move filter down one pixel and repeat process\n\t- Process ends when we get to bottom right corner of image\n\n---\n\n# 3 by 3 Filter\n\n![A 3 by 3 filter applied to a 4 by 4 image, resulting in a 2 by 2 image](/training-material/topics/statistics/images/Conv_no_padding_no_strides.gif) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Convolution operator parameters\n\n- Filter size\n- Padding\n- Stride\n- Dilation\n- Activation function\n\n---\n\n# Filter size\n\n- Filter size can be 5 by 5, 3 by 3, and so on\n- Larger filter sizes should be avoided\n\t- As learning algorithm needs to learn filter values (weights)\n- Odd sized filters are preferred to even sized filters\n\t- Nice geometric property of all input pixels being around output pixel\n\n---\n\n# Padding\n\n- After applying 3 by 3 filter to 4 by 4 image, we get a 2 by 2 image\n\t– Size of the image has gone down\n- If we want to keep image size the same, we can use padding\n\t- We pad input in every direction with 0’s before applying filter\n\t- If padding is 1 by 1, then we add 1 zero in evey direction\n\t- If padding is 2 by 2, then we add 2 zeros in every direction, and so on\n\n---\n\n# 3 by 3 filter with padding of 1\n\n![A 3 by 3 filter applied to a 5 by 5 image, with padding of 1, resulting in a 5 by 5 image](/training-material/topics/statistics/images/Conv_same_padding_no_strides.gif) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Stride\n\n- How many pixels we move filter to the right/down is stride\n- Stride 1: move filter one pixel to the right/down\n- Stride 2: move filter two pixels to the right/down\n\n---\n\n# 3 by 3 filter with stride of 2\n\n![A 3 by 3 filter applied to a 5 by 5 image, with stride of 2, resulting in a 2 by 2 image](/training-material/topics/statistics/images/Conv_no_padding_strides.gif) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Dilation\n\n- When we apply 3 by 3 filter, output affected by pixels in 3 by 3 subset of image\n- Dilation: To have a larger receptive field (portion of image affecting filter’s output)\n- If dilation set to 2, instead of contiguous 3 by 3 subset of image, every other pixel of a 5 by 5 subset of image affects output\n\n---\n\n# 3 by 3 filter with dilation of 2\n\n![A 3 by 3 filter applied to a 7 by 7 image, with dilation of 2, resulting in a 3 by 3 image](/training-material/topics/statistics/images/Conv_dilation.gif) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Activation function\n\n- After filter applied to whole image, apply activation function to output to introduce non-linearlity\n- Preferred activation function in CNN is ReLU\n- ReLU leaves outputs with positive values as is, replaces negative values with 0\n\n---\n\n# Relu activation function\n\n![Two matrices representing filter output before and after ReLU activation function is applied](/training-material/topics/statistics/images/Conv_ReLU.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Single channel 2D convolution\n\n![One matrix representing an input vector and another matrix representing a filter, along with calculation for single input channel two dimensional convolution operation](/training-material/topics/statistics/images/Conv_single_input_channel.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Triple channel 2D convolution\n\n![Three matrices representing an input vector and another three matrices representing a filter, along with calculation for multiple input channel two dimensional convolution operation](/training-material/topics/statistics/images/Conv_multiple_input_channel.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Triple channel 2D convolution in 3D\n\n![Multiple cubes representing input vector, filter, and output in a 3 channel 2 dimensional convolution operation](/training-material/topics/statistics/images/Conv_multiple_channel_3d.gif) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Change channel size\n\n- Output of a multi-channel 2D filter is a single channel 2D image\n- Applying *multiple* filters results in a multi-channel 2D image\n- E.g., if input image is 28 x 28 x 3 (rows x columns x channels)\n\t- We apply a 3 x 3 filter with 1 x 1 padding, we get a 28 x 28 x 1 image\n\t- If we apply 15 such filters, we get a 28 x 28 x 15\n- Number of filters allows us to increase or decrease channel size\n\n---\n\n# Pooling layer\n\n- Pooling layer performs down sampling to reduce spatial dimensionality of input\n- This decreases number of parameters\n\t- Reduces learning time/computation\n\t- Reduces likelihood of overfitting\n- Most popular type is *max* pooling\n\t- Usually a 2 x 2 filter with a stride of 2 \n\t- Returns maximum value as it slides over input data\n\n---\n\n# Fully connected layer\n\n- Last layer in a CNN\n- Connect all nodes from previous layer to this fully connected layer\n\t- Which is responsible for classification of the image\n\n---\n\n# An example CNN\n\n![A convolutional neural network with 3 convolution layers followed by 3 pooling layers](/training-material/topics/statistics/images/Conv_CNN.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# An example CNN\n\n- A typical CNN has several convolution plus pooling layers\n\t- Each responsible for feature extraction at different levels of abstraction\n\t- E.g., filters in first layer detect horizontal, vertical, and diagonal edges\n\t- Filters in the next layer detect shapes\n\t- Filters in the last layer detect collection of shapes\n- Filter values randomly initialized, learned by learning algorithm\n- CNN not only do classification, but can also automatically do feature extraction\n\t- Distinguishes CNN from other classification techniques (like Support Vector Machines)\n\n---\n\n# MNIST dataset\n\n- MNIST dataset of handwritten digits\n\t- Composed of training set of 60,000 and test set of 10,000 images\n- Digits have been size-normalized/centered in a fixed-size image (28 by 28 pixels)\n- Images are grayscale\n\t- Each pixel is represented by a number between 0 and 255\n\t- 0 for black, 255 for white, and other values for shades of gray\n- MNIST dataset is a standard image classification dataset\n\t- Used to compare various Machine Learning techniques\n---\n\n# Classification of MNIST images with CNN\n\n- We define a CNN and train it using MNIST dataset training data\n- Goal is to learn a model such that given image of a digit we predict the digit (0 to 9)\n- We then evaluate the trained CNN on test dataset and plot the confusion matrix\n\n---\n\n# For references, please see tutorial's References section\n\n---\n\n- Galaxy Training Materials ([training.galaxyproject.org](https://training.galaxyproject.org))\n\n![Screenshot of the gtn stats page with 21 topics, 170 tutorials, 159 contributors, 16 scientific topics, and a growing community](/training-material/topics/introduction/images/gtn_stats.png)\n\n???\n\n- If you would like to learn more about Galaxy, there are a large number of tutorials available.\n- These tutorials cover a wide range of scientific domains.\n\n---\n\n# Getting Help\n\n- **Help Forum** ([help.galaxyproject.org](https://help.galaxyproject.org))\n\n\n  ![Galaxy Help](/training-material/topics/introduction/images/galaxy_help.png)\n\n\n- **Gitter Chat**\n    - [Main Chat](https://gitter.im/galaxyproject/Lobby)\n    - [Galaxy Training Chat](https://gitter.im/Galaxy-Training-Network/Lobby)\n    - Many more channels (scientific domains, developers, admins)\n\n???\n\n- If you get stuck, there are ways to get help.\n- You can ask your questions on the help forum.\n- Or you can chat with the community on Gitter.\n\n---\n\n# Join an event\n\n- Many Galaxy events across the globe\n- Event Horizon: [galaxyproject.org/events](https://galaxyproject.org/events)\n\n![Event schedule](/training-material/topics/introduction/images/event_horizon.png)\n\n???\n\n- There are frequent Galaxy events all around the world.\n- You can find upcoming events on the Galaxy Event Horizon.\n"],"hands_on":true,"slides":true,"mod_date":"2024-05-29 15:37:52 +0000","pub_date":"2021-04-19 17:54:38 +0000","version":39,"workflows":[{"workflow":"Intro_To_CNN_v1_0_11_0.ga","tests":true,"url":"https://training.galaxyproject.org/training-material/topics/statistics/tutorials/CNN/workflows/Intro_To_CNN_v1_0_11_0.ga","path":"topics/statistics/tutorials/CNN/workflows/Intro_To_CNN_v1_0_11_0.ga","wfid":"statistics-CNN","wfname":"intro_to_cnn_v1_0_11_0","trs_endpoint":"https://training.galaxyproject.org/training-material/api/ga4gh/trs/v2/tools/statistics-CNN/versions/intro_to_cnn_v1_0_11_0","license":"CC-BY-4.0","creators":[{"class":"Person","identifier":"0000-0001-6585-3619","name":"Kaivan Kamali"}],"name":"Intro_To_CNN_v1.0.11.0","title":"Intro_To_CNN_v1.0.11.0","test_results":null,"modified":"2024-06-24 07:46:43 +0000","mermaid":"flowchart TD\n  0[\"ℹ️ Input Dataset\\nX_test\"];\n  style 0 stroke:#2c3143,stroke-width:4px;\n  1[\"ℹ️ Input Dataset\\nX_train\"];\n  style 1 stroke:#2c3143,stroke-width:4px;\n  2[\"ℹ️ Input Dataset\\ny_test\"];\n  style 2 stroke:#2c3143,stroke-width:4px;\n  3[\"ℹ️ Input Dataset\\ny_train\"];\n  style 3 stroke:#2c3143,stroke-width:4px;\n  4[\"Create a deep learning model architecture\"];\n  5[\"To categorical\"];\n  3 -->|output| 5;\n  6[\"Create deep learning model\"];\n  4 -->|outfile| 6;\n  7[\"Deep learning training and evaluation\"];\n  6 -->|outfile| 7;\n  1 -->|output| 7;\n  5 -->|outfile| 7;\n  8[\"Model Prediction\"];\n  7 -->|outfile_object| 8;\n  0 -->|output| 8;\n  9[\"Machine Learning Visualization Extension\"];\n  8 -->|outfile_predict| 9;\n  2 -->|output| 9;"}],"api":"https://training.galaxyproject.org/training-material/api/topics/statistics/tutorials/CNN/tutorial.json","tools":["toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_builder/keras_model_builder/1.0.10.0","toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_config/keras_model_config/1.0.10.0","toolshed.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/1.0.11.0","toolshed.g2.bx.psu.edu/repos/bgruening/ml_visualization_ex/ml_visualization_ex/1.0.11.0","toolshed.g2.bx.psu.edu/repos/bgruening/model_prediction/model_prediction/1.0.11.0","toolshed.g2.bx.psu.edu/repos/bgruening/sklearn_to_categorical/sklearn_to_categorical/1.0.10.0"],"supported_servers":{"exact":[{"url":"https://usegalaxy.eu","name":"UseGalaxy.eu","usegalaxy":true},{"url":"https://usegalaxy.org","name":"UseGalaxy.org (Main)","usegalaxy":true},{"url":"https://usegalaxy.org.au","name":"UseGalaxy.org.au","usegalaxy":true}],"inexact":[{"url":"https://usegalaxy.cz/","name":"UseGalaxy.cz","usegalaxy":false}]},"topic_name_human":"Statistics and machine learning","admin_install":{"install_tool_dependencies":true,"install_repository_dependencies":true,"install_resolver_dependencies":true,"tools":[{"name":"keras_model_builder","owner":"bgruening","revisions":"66d7efc06000","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"keras_model_config","owner":"bgruening","revisions":"f22a9297440f","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"keras_train_and_eval","owner":"bgruening","revisions":"2af1346e68c9","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"ml_visualization_ex","owner":"bgruening","revisions":"8334dc98c48e","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"model_prediction","owner":"bgruening","revisions":"25b46af53232","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"sklearn_to_categorical","owner":"bgruening","revisions":"61634a87efc7","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"}]},"admin_install_yaml":"---\ninstall_tool_dependencies: true\ninstall_repository_dependencies: true\ninstall_resolver_dependencies: true\ntools:\n- name: keras_model_builder\n  owner: bgruening\n  revisions: 66d7efc06000\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: keras_model_config\n  owner: bgruening\n  revisions: f22a9297440f\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: keras_train_and_eval\n  owner: bgruening\n  revisions: 2af1346e68c9\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: ml_visualization_ex\n  owner: bgruening\n  revisions: 8334dc98c48e\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: model_prediction\n  owner: bgruening\n  revisions: 25b46af53232\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: sklearn_to_categorical\n  owner: bgruening\n  revisions: 61634a87efc7\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n","tours":false,"video":false,"slides_recordings":false,"translations":{"tutorial":[],"slides":[],"video":false},"license":"CC-BY-4.0","type":"tutorial","logo":"GTN","key_points":null,"redirect_from":["/short/statistics/CNN/slides","/short/S00088"]}
{"layout":"tutorial_hands_on","title":"Deep Learning (Part 1) - Feedforward neural networks (FNN)","zenodo_link":"https://zenodo.org/record/4660497","questions":["What is a feedforward neural network (FNN)?","What are some applications of FNN?"],"objectives":["Understand the inspiration for neural networks","Learn various activation functions, and classification/regression problems solved by neural networks","Discuss various cost/loss functions and the backpropagation algorithm","Learn how to create a neural network using Galaxy's deep learning tools","Solve a simple regression problem, car purchase price prediction, via FNN in Galaxy"],"requirements":[{"type":"internal","topic_name":"statistics","tutorials":["intro_deep_learning"]}],"follow_up_training":[{"type":"internal","topic_name":"statistics","tutorials":["RNN"]}],"time_estimation":"2H","contributors":[{"name":"Kaivan Kamali","email":"kxk302@gmail.com","joined":"2021-02","id":"kxk302","url":"https://training.galaxyproject.org/training-material/api/contributors/kxk302.json","page":"https://training.galaxyproject.org/training-material/hall-of-fame/kxk302/"}],"recordings":[{"captioners":["kxk302"],"date":"2021-02-15","galaxy_version":"21.01","length":"1H10M","youtube_id":"VbzJDkyPL4A","speakers":["kxk302"]}],"js_requirements":{"mathjax":2165,"mermaid":false},"short_id":"T00258","url":"/topics/statistics/tutorials/FNN/tutorial.html","topic_name":"statistics","tutorial_name":"FNN","dir":"topics/statistics/tutorials/FNN","symlink":null,"id":"statistics/FNN","ref_tutorials":["<p>Artificial neural networks are a machine learning discipline roughly inspired by how neurons in a\nhuman brain work. In the past decade, there has been a huge resurgence of neural networks thanks\nto the vast availability of data and enormous increases in computing capacity (Successfully\ntraining complex neural networks in some domains requires lots of data and compute capacity). There\nare various types of neural networks (Feedforward, recurrent, etc.). In this tutorial, we discuss\nfeedforward neural networks (FNN), which have been successfully applied to pattern classification,\nclustering, regression, association, optimization, control, and forecasting (<span class=\"citation\"><a href=\"#JainEtAl\">Jain <i>et al.</i> 1996</a></span>).\nWe will discuss biological neurons that inspired artificial neural networks, review activation\nfunctions, classification/regression problems solved by neural networks, and the backpropagation\nlearning algorithm. Finally, we construct a FNN to solve a regression problem using car purchase\nprice prediction dataset.</p>\n\n<blockquote class=\"agenda\">\n  <agenda-title></agenda-title>\n\n  <p>In this tutorial, we will cover:</p>\n\n<ol id=\"markdown-toc\">\n  <li><a href=\"#inspiration-for-artificial-neural-networks\" id=\"markdown-toc-inspiration-for-artificial-neural-networks\">Inspiration for artificial neural networks</a></li>\n  <li><a href=\"#perceptron\" id=\"markdown-toc-perceptron\">Perceptron</a></li>\n  <li><a href=\"#activation-functions\" id=\"markdown-toc-activation-functions\">Activation functions</a></li>\n  <li><a href=\"#supervised-learning\" id=\"markdown-toc-supervised-learning\">Supervised learning</a></li>\n  <li><a href=\"#losscost-function\" id=\"markdown-toc-losscost-function\">Loss/Cost function</a></li>\n  <li><a href=\"#backpropagation-learning-algorithm\" id=\"markdown-toc-backpropagation-learning-algorithm\">Backpropagation Learning algorithm</a></li>\n  <li><a href=\"#get-data\" id=\"markdown-toc-get-data\">Get Data</a></li>\n  <li><a href=\"#solve-a-simple-regression-problem-using-car-purchase-price-prediction-dataset-via-fnn-in-galaxy\" id=\"markdown-toc-solve-a-simple-regression-problem-using-car-purchase-price-prediction-dataset-via-fnn-in-galaxy\">Solve a simple regression problem using car purchase price prediction dataset via FNN in Galaxy</a>    <ol>\n      <li><a href=\"#create-a-deep-learning-model-architecture\" id=\"markdown-toc-create-a-deep-learning-model-architecture\">Create a deep learning model architecture</a></li>\n      <li><a href=\"#create-a-deep-learning-model\" id=\"markdown-toc-create-a-deep-learning-model\">Create a deep learning model</a></li>\n      <li><a href=\"#deep-learning-training-and-evaluation\" id=\"markdown-toc-deep-learning-training-and-evaluation\">Deep learning training and evaluation</a></li>\n      <li><a href=\"#model-prediction\" id=\"markdown-toc-model-prediction\">Model Prediction</a></li>\n      <li><a href=\"#plot-actual-vs-predicted-curves-and-residual-plots\" id=\"markdown-toc-plot-actual-vs-predicted-curves-and-residual-plots\">Plot actual vs predicted curves and residual plots</a></li>\n    </ol>\n  </li>\n  <li><a href=\"#conclusion\" id=\"markdown-toc-conclusion\">Conclusion</a></li>\n</ol>\n\n</blockquote>\n\n<h1 id=\"inspiration-for-artificial-neural-networks\">Inspiration for artificial neural networks</h1>\n\n<p>A neuron is a special biological cell with information processing ability (<span class=\"citation\"><a href=\"#JainEtAl\">Jain <i>et al.</i> 1996</a></span>).\nFigure 1 shows a biological neuron. It has a cell body and two outreaching tree-like branches:\naxon and dendrites. A neuron receives signals from other neurons through its dendrites, and\ntransmits signals generated by its cell body to other neurons via its axon. A synapse is a place\nof contact between two neurons, an axon strand of one neuron and a dendrite strand of another\nneuron. A synapse can either enhance or inhibit the signal that passes through it. Learning occurs\nby changing the effectiveness of synapse. If the signals received exceeds a threshold, the neuron\n<em>fires</em>, i.e., it transmits a signal to other neurons. If not, it will not fire.</p>\n\n<figure id=\"figure-1\" style=\"max-width: 90%;\"><img src=\"../../images/FNN_bio_neuron.png\" alt=\"Sketch of a biological neuron and its components. \" width=\"559\" height=\"245\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/FNN_bio_neuron.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 1</strong>:</span> A biological neuron (<span class=\"citation\"><a href=\"#JainEtAl\">Jain <i>et al.</i> 1996</a></span>)</figcaption></figure>\n\n<p>Cerebral cortex, the outer most layer of the brain, is a sheet of neurons about 2 to 3 mm thick,\nwith a surface area of about 2,200 \\(cm^{2}\\). Cerebral cortex has about \\(10^{11}\\) neurons.\nEach neuron is connected to \\(10^{3} to 10^{4}\\) neurons. Hence, a human brain has around\n \\(10^{14} to 10^{15}\\) connections. Neurons communicate by a short train of signals, usually\nmilliseconds in duration. The frequency in which the signals are transmitted can be up to several\nhundred Hertz, which is millions of times slower than an electronic circuit. However, complex tasks,\nsuch as face recognition are made within a few hundred milliseconds. This implies that computation\ninvolved cannot take more than 100 serial steps, i.e., brain runs parallel programs that are about\n100 steps long for such complex perceptual tasks. The amount of information sent from one neuron to\nanother is also very small. This implies that critical information is not transmitted directly, but\nis captured by the interconnections. What enables slow computing elements in the brain to perform\ncomplex tasks so quickly is the distributed computation and representation nature of the brain (<span class=\"citation\"><a href=\"#JainEtAl\">Jain <i>et al.</i> 1996</a></span>).</p>\n\n<h1 id=\"perceptron\">Perceptron</h1>\n\n<p>Perceptron (<span class=\"citation\"><a href=\"#Rosenblatt\">Rosenblatt 1957</a></span>) is the oldest neural network still in use today. It’s a form of a\nfeedforward neural network, in which the connections between the nodes do not form a loop. It accepts\nmultiple inputs, each input is multiplied by a weight, and the products are added up. The weights\nsimulate the role of synapse in biological neurons (to enhance or inhibit a signal). A <em>bias</em> value\nis then added to the result before it is passed to an <em>activation function</em>. An activation function\nsimulates the neuron firing or not. For example, in a <em>binary step</em> activation function, if the sum of\nweighted inputs and bias is greater than zero, the neuron output is 1 (it fires). Else, the neuron\noutput is 0 (it does not fire). Bias allows us to shift the activation function.</p>\n\n\\[f(x) =\n\\left\\{\n\t\\begin{array}{ll}\n\t\t1  &amp; \\mbox{if } \\boldsymbol{x} \\cdot \\boldsymbol{w} + b \\geq 0 \\\\\n\t\t0  &amp; \\mbox{otherwise }\n\t\\end{array}\n\\right.\\]\n\n<p>Figure 2 shows a Perceptron, a single layer FNN, where the input is 3 dimensional (input layer has\n3 nodes), and output is 1 dimensional (output layer has 1 node).</p>\n\n<figure id=\"figure-2\" style=\"max-width: 90%;\"><img src=\"../../images/FFNN_no_hidden.png\" alt=\"Neurons forming the input and output layers of a single layer feedforward neural network. \" width=\"233\" height=\"463\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/FFNN_no_hidden.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 2</strong>:</span> A perceptron</figcaption></figure>\n\n<p>In supervised learning, we are given a set of input-output pairs, called the <em>training\nset</em>. Given the training set, the learning algorithm (iteratively) adjusts the model\nparameters (weights and biases), so that the model can accurately map inputs to outputs.\nThe learning algorithm for Perceptron is very simple and reduces the weights (via a\nsmall learning rate multiplier) if the predicted output is more than the expected output\nand increases them otherwise (<span class=\"citation\"><a href=\"#Rosenblatt\">Rosenblatt 1957</a></span>).</p>\n\n<p>Minsky and Papert showed that a single layer FNN cannot solve problems in which the data is not\nlinearly separable, such as the XOR problem (<span class=\"citation\"><a href=\"#Newell780\">Newell 1969</a></span>). Adding one (or more) hidden\nlayers to FNN enables it to solve problems in which data is non-linearly separable. Per Universal\nApproximation Theorem, a FNN with one hidden layer can represent any function (<span class=\"citation\"><a href=\"#Cybenko1989\">Cybenko 1989</a></span>),\nalthough in practice training such a model is very difficult (if not impossible), hence, we usually\nadd multiple hidden layers to solve complex problems.</p>\n\n<figure id=\"figure-3\" style=\"max-width: 90%;\"><img src=\"../../images/FFNN.png\" alt=\"Neurons forming the input, output, and hidden layers of a multi-layer feedforward neural network. \" width=\"361\" height=\"461\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/FFNN.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 3</strong>:</span> Feedforward neural network with a hidden layer. Biases to hidden/output layer neurons are omitted for clarity</figcaption></figure>\n\n<p>The problem with multi-layer FNN was lack of a learning algorithm, as the Perceptron’s learning\nalgorithm could not be extended to multi-layer FNN. This along with Minsky and Papert highlighting\nthe limitations of Perceptron resulted in sudden drop in interest in neural networks (referred to\nas <em>AI winter</em>). In the 80’s the backpropagation algorithm was proposed (<span class=\"citation\"><a href=\"#Rumelhart1986\">Rumelhart <i>et al.</i> 1986</a></span>),\nwhich enabled learning in multi-layer FNN and resulted in a renewed interest in the field.</p>\n\n<p>In a multi-layer neural network, we have an input layer, an output layer, and one or more hidden layers\n(between input and output layers). The input layer has as many neurons as the dimension of the input\ndata. The number of neurons in the output layer depends on the type of the problem the neural network\nis trying to solve (See Supervised learning section below). The more hidden layers that we have (and\nthe more neurons we have in each hidden layer), our neural network can estimate more complex functions.\nHowever, this comes at the cost of increased training time (due to increased number of parameters) and\nincreased likelihood of <em>overfitting</em>. Overfitting happens when a model captures the details of the\ntraining data, performs well on the training data, but is unable to perform well on data not used in\nthe training. The neural network, hence, cannot <em>generalize</em> to unseen data. There are regularization\ntechniques that can prevent that (<span class=\"citation\"><a href=\"#KukackaEtAl\">Kukacka <i>et al.</i> 2017</a></span>) but they are outside the scope of this tutorial.</p>\n\n<h1 id=\"activation-functions\">Activation functions</h1>\n\n<p>There are many activation functions besides the step function used in Perceptron (<span class=\"citation\"><a href=\"#nwankpaEtAl\">Nwankpa <i>et al.</i> 2018</a></span>). Figure 4 shows\nsome of the more common activation functions.</p>\n\n<figure id=\"figure-4\" style=\"max-width: 90%;\"><img src=\"../../images/FNN_activation_functions.png\" alt=\"Table showing the formula, graph, derivative, and range of common activation functions. \" width=\"1703\" height=\"836\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/FNN_activation_functions.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 4</strong>:</span> Common activation functions (Source: https://en.wikipedia.org/wiki/Activation_function)</figcaption></figure>\n\n<p>Linear activation function is used in the output layer of a network when we have a regression problem. It does\nnot make sense to use it in all layers, as such multi-layer network can be reduced to a single layer network.\nAlso, networks with linear activation functions cannot model non-linear relationships between input and output.</p>\n\n<p>Binary step activation function is used in Perceptron. It cannot be used in multi-layers networks as they use\nback propagation learning algorithm, which changes network weights/biases based on the derivative\nof the activation function, which is zero. Hence, there would no weights/biases updates in back propagation.</p>\n\n<p>Sigmoid activation function can be used both at the output layer and hidden layers of a multilayer network. They\nallow the network to model non-linear relationships between input and output. The problem with Sigmoid activation\nfunction is that the derivative values away from the origin are very small and quickly approach zero. In a multi\nlayer network, in order to calculate weight updates in layers closer to the input layer, we use the chain rule\nwhich requires multiplying multiple Sigmoid derivative values (formula given in Backpropagation learning algorithm\nsection below). Multiplying multiple small numbers results in a <em>very</em> small number, meaning that the weight updates\nwill be minimal and the learning algorithm will be very slow. This is known as the <em>vanishing gradient</em> problem. In\nnetworks with many hidden layers (so called <em>deep networks</em>), we generally avoid Sigmoid and use ReLU activation\nfunction.</p>\n\n<p>Hyperbolic tangent (or tanh), similar to Sigmoid function, is a soft step function. But its range is between -1\nand 1 (instead of 0 and 1). One benefit of tanh over Sigmoid is that its derivative values are larger, so it\nsuffers less from the vanishing gradient problem.</p>\n\n<p>Finally, ReLU (Rectified Linear Unit) is an activation function popular is deep neural networks. Since it\ndoes not suffer from vanishing gradient problem, it is preferred to Sigmoid or tanh. Sigmoid or tanh can\nstill be used in the output layer of deep networks.</p>\n\n<h1 id=\"supervised-learning\">Supervised learning</h1>\n\n<p>In supervised learning a <em>training set</em> is defined as\n\\({(\\boldsymbol{x^{(1)}}, \\boldsymbol{y^{(1)}}), ((\\boldsymbol{x^{(2)}}, \\boldsymbol{y^{(2)}}), ..., ((\\boldsymbol{x^{(m)}}, \\boldsymbol{y^{(m)}})}\\)\nand each pair \\((\\boldsymbol{x^{(i)}}, \\boldsymbol{y^{(i)}})\\) is called a <em>training example</em>.\n<em>m</em> is the number of training examples and \\(\\boldsymbol{x^{(i)}}\\) is called <em>feature vector</em>\nor <em>input vector</em>.  Each element of the vector is called a <em>feature</em>. Each \\(\\boldsymbol{x^{(i)}}\\)\ncorresponds to a label \\(\\boldsymbol{y^{(i)}}\\). We assume there is an unknown function\n\\(\\boldsymbol{y} = f(\\boldsymbol{x})\\) that maps the feature vectors to labels. The goal of\nsupervised learning is to use the training set to learn or estimate <em>f</em>. We call this estimated\nfunction \\(\\hat{f}(\\boldsymbol{x})\\). We want \\(\\hat{f}(\\boldsymbol{x})\\) to be close to\n\\(f(\\boldsymbol{x})\\) not only for training set, but for training examples not in training\nset (<span class=\"citation\"><a href=\"#Bagheri\">Bagheri 2020</a></span>).</p>\n\n<p>When the label is a numerical variable, we call the problem a <em>regression</em> problem, and when it’s a categorical variable,\nwe call it a <em>classification</em> problem. In classification problems, the label can be represented by the set\n\\(\\boldsymbol{y^{i}} \\in {1,2,...,c}\\), where each number is a class label and <em>c</em> is the number of classes.\nIf <em>c</em>=2, the class labels are mutually exclusive, we call it a <em>binary classification</em> problem. If <em>c</em> &gt; 2, and\nthe labels are mutually exclusive, we call <em>multiclass classification</em> problem. If labels are <em>not</em> mutually exclusive,\nwe call it a <em>multilabel classification</em> problem (<span class=\"citation\"><a href=\"#Bagheri\">Bagheri 2020</a></span>).</p>\n\n<p>We use a method called <em>one-hot encoding</em> to convert binary and multiclass classification class label numbers into\nbinary values. We convert the scalar label <em>y</em> into a vector \\(\\boldsymbol{y}\\) which has <em>c</em> elements. When y is\nequal to k, the k-th element of \\(\\boldsymbol{y}\\) is one and all other elements are zero. When labels are <em>not</em>\nmutually exclusive, we use another method called <em>multi-hot encoding</em>. Suppose we are doing a multilabel image\nclassification, where an image can have a dog, panda, or cat in it. We represent the label by a vector of 3, and if\ndog and cat are present in the image, first and third element of the vector are one and the second element is zero (<span class=\"citation\"><a href=\"#Bagheri\">Bagheri 2020</a></span>).</p>\n\n<figure id=\"figure-5\" style=\"max-width: 90%;\"><img src=\"../../images/FNN_output_encoding.png\" alt=\"Three images illustrating binary, multiclass, and multilabel classifications and their label representation. \" width=\"1279\" height=\"944\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/FNN_output_encoding.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 5</strong>:</span> Examples of binary, multiclass, and multilabel classifications and their label representation (<span class=\"citation\"><a href=\"#Bagheri\">Bagheri 2020</a></span>)</figcaption></figure>\n\n<p>Figure 5 shows examples of binary, multiclass, and multilabel classification problems and their associated one-hot\nencoded or multi-hot encoded labels. The output layer of a neural network for binary classification usually has a\nsingle neuron with Sigmoid activation function. If the neuron’s output is greater than 0.5, we assume the output is 1,\nand otherwise, we assume the output is 0. For multilabel classification problems, the output layer of the neural network\nusually has as many neurons as the number of classes and the neurons use Sigmoid activation function. Again, we use a\nthreshold of 0.5 to determine whether the output of each neuron is 1 or 0. For multiclass classification problems, the\noutput layer usually has as many neurons as the number of classes. However, instead of Sigmoid,  we use a <em>Softmax</em>\nactivation function, which takes the input to all the neurons in the output layer, and creates a probability distribution,\nso, the sum of outputs of all output layer neurons adds up to 1. The neuron with the highest probability denotes the predicted\nlabel.</p>\n\n<h1 id=\"losscost-function\">Loss/Cost function</h1>\n\n<p>During training, for each training example in the training set \\((\\boldsymbol{x^{(i)}}, \\boldsymbol{y^{(i)}})\\), we\npresent the feature vector \\(\\boldsymbol{x^{(i)}}\\) to the neural network, and compare the network’s predicted output\n\\(\\boldsymbol{\\hat{y}}\\) with the corresponding label \\(\\boldsymbol{y^{(1)}}\\). We need to define a <strong>loss function</strong>\nto objectively measure how much the network’s predicted output is different than the expected output (the corresponding\nlabel). We use the <strong>cross entropy</strong> loss function for classification problems, and <em>quadratic</em> loss function for\nregression problems.</p>\n\n<p>For multiclass classification problems, the cross entropy is calculated as below</p>\n\n\\[\\mathcal{L}(\\boldsymbol{\\hat{y}^{(j)}}, \\boldsymbol{y^{(j)}}) = - \\sum\\_{i=1}^{c} \\boldsymbol{y\\_{i}^{(j)}}ln(\\boldsymbol{\\hat{y}\\_{i}^{(j)}})\\]\n\n<p>You can find the cross entropy formula for binary and multilabel classifications in <span class=\"citation\"><a href=\"#Bagheri\">Bagheri 2020</a></span>. They are just special\ncases of multiclass cross entropy and are not give here for the sake of brevity.</p>\n\n<p>The loss function is calculated for each training example in the training set. The average of the calculated loss functions\nfor all training examples in the training set is the <strong>cost function</strong>. For multiclass classification problems, the cost function\nis calculated as below (again refer to <span class=\"citation\"><a href=\"#Bagheri\">Bagheri 2020</a></span> for binary classification and multilabel classification formulas).</p>\n\n\\[J(\\boldsymbol{W}, \\boldsymbol{b}) = - \\frac{1}{m} \\sum\\_{j=1}^{m} \\sum\\_{i=1}^{c} \\boldsymbol{y\\_{i}^{(j)}}ln(\\boldsymbol{\\hat{y}\\_{i}^{(j)}})\\]\n\n<p>For regression problems, the quadratic loss function is calculated as below:</p>\n\n\\[\\mathcal{L}(\\boldsymbol{\\hat{y}^{(j)}}, \\boldsymbol{y^{(j)}}) = \\frac{1}{2} \\| \\boldsymbol{y^{(j)}} - \\boldsymbol{\\hat{y}^{(j)}} \\| ^ 2\\]\n\n<p>Similarly, the <em>quadratic</em> cost function (or <em>Mean Squared Error (MSE)</em>) is the average of the calculated loss functions\nfor all training examples in the training set.</p>\n\n\\[J(\\boldsymbol{W}, \\boldsymbol{b}) = \\frac{1}{2m} \\sum\\_{j=1}^{m} \\| \\boldsymbol{y^{(j)}} - \\boldsymbol{\\hat{y}^{(j)}} \\| ^ 2\\]\n\n<h1 id=\"backpropagation-learning-algorithm\">Backpropagation Learning algorithm</h1>\n\n<p>The <strong>backpropagation</strong> algorithm <span class=\"citation\"><a href=\"#Rumelhart1986\">Rumelhart <i>et al.</i> 1986</a></span> is a gradient descent technique. Gradient descent aims to find\na local minimum of a function by iteratively moving in the opposite direction of the gradient (i.e., the slope) of the\nfunction at the current point. The goal of a learning in neural networks is to minimize the cost function given the\ntraining set. The cost function is a function of network weights and biases of all the neurons in all the layers.\nBackpropagation iteratively computes the gradient of cost function relative to each weight and bias, then updates\nthe weights and biases in the opposite direction of the gradient, to find a local minimum.</p>\n\n<p>In order to specify the formula for backpropagation, we need to define the error of the \\(i^{th}\\) neuron in \\(l^{th}\\)\nlayer of a network for the \\(j^{th}\\) training example as follows (where \\(z\\_{i}^{[l](j)}\\) is the weighted some of\ninput to the neuron, and \\(\\mathcal{L}\\) is the loss function):</p>\n\n\\[\\delta\\_{i}^{[l](j)} = \\frac{\\partial \\mathcal{L}(\\boldsymbol{\\hat{y}^{(j)}}, \\boldsymbol{y^{(j)}})}{\\partial z\\_{i}^{[l](j)}}\\]\n\n<p>Backpropagation formulas are expressed in terms of the error defined above. Full derivation of the formulas below is outside\nthe scope of this tutorial (Repeated use of chain rule is needed. Please refer to the excellent article by <span class=\"citation\"><a href=\"#Bagheri\">Bagheri 2020</a></span>\nfor details). Note that in formulas below L denotes the output layer, g the activation function, \\(\\nabla\\) the gradient,\n\\(W^{[l]^{T}}\\) layer l weights transposed, \\(b\\_{i}^{l}\\) bias of neuron i at layer l, \\(w\\_{ik}^{l}\\) weight to neuron\ni at layer l from neuron k from layer l-1, and \\(a\\_{k}^{[l-1](j)}\\) activation of neuron k at layer l-1 for training example j.</p>\n\n\\[\\boldsymbol{\\delta}^{[L](j)} = \\nabla\\_{\\boldsymbol{\\hat{y}^{(j)}}}\\mathcal{L} \\odot (g^{[L]})^{'} (\\boldsymbol{z}^{[L](j)}) = \\boldsymbol{\\hat{y}^{(j)}} - \\boldsymbol{y^{(j)}}\\]\n\n\\[\\boldsymbol{\\delta}^{[l](j)} = W^{[l+1]^{T}} \\boldsymbol{\\delta}^{[l+1](j)}  \\odot (g^{[l]})^{'} (\\boldsymbol{z}^{[l](j)})\\]\n\n\\[\\frac{\\partial L}{\\partial b\\_{i}^{[l]}} = \\boldsymbol{\\delta}\\_{i}^{[l](j)}\\]\n\n\\[\\frac{\\partial L}{\\partial w\\_{ik}^{[l]}} = \\boldsymbol{\\delta}\\_{i}^{[l](j)} a\\_{k}^{[l-1](j)}\\]\n\n<p>As you can see, we can calculate the error at the output layer for sample <em>j</em> using the first equation. Afterwards, we can calculate\nthe error in the layer right before the output layer for sample <em>j</em> using the second equation. The second equation is recursive,\nmeaning that we can calculate the error in any layer, given the error values for the next layer. This backward calculation of the\nerrors is the reason this algorithm is called <em>backpropagation</em>.</p>\n\n<p>After the error values for all the layers are calculated for sample <em>j</em>, we use the third and fourth equations to calculate the gradient\nof loss function relative to biases and weights for sample <em>j</em>. We can repeat these steps for all the samples, average the gradients of\nthe loss function relative to biases and weights and use the average value to update the biases and weights. This is called <em>batch\ngradient descent</em>. If we have too many samples, such calculations will take a long time. An alternative is to update the biases/weights\nbased on the gradient of each sample. This is called <em>stochastic gradient descent</em>. While this is much faster than batch gradient descent,\nthe gradient calculated based on a single sample is not a good estimate of the gradient calculated in the batch version of the algorithm.\nA middle ground solution is to calculate the gradient of a <em>batch</em>, and update the biases and weights based on the average of the gradients\nin the batch. This is called <em>mini-batch gradient descent</em> and is preferred to the other two variations of the algorithm.</p>\n\n<p>Also, note that in the second equation which is recursive, we have a term that is the derivative of the activation function for that\nlayer. The recursive nature of this equation means, calculating the error values in the layer prior to the output layer requires 1\nmultiplication by the derivative value; calculating the error values in two (or more) layers before the output layer requires 2 (or more)\nmultiplication by the derivative values. If these derivative values are small, as could be the case for the Sigmoid function, the product\nof multiple small values will result in a <em>very</em> small value (e.g., 0.001). Since these error values decide the updates for biases and weights, this\nmeans the update to biases and weights in layers closer to the input layer will be very small, slowing the learning algorithm to a halt.\nThis phenomenon is known as the <em>vanishing gradient</em> problem and is the reason Sigmoid function cannot be used in very deep networks (And\nwhy ReLU is so popular in deep networks).</p>\n\n<h1 id=\"get-data\">Get Data</h1>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Data upload</hands-on-title>\n\n  <ol>\n    <li>\n      <p>Create a new history for this tutorial</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-creating-a-new-history\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-creating-a-new-history\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Creating a new history</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <p>Click the <i class=\"fas fa-plus\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">new-history</span> icon at the top of the history panel:</p>   <p><img src=\"/training-material/shared/images/history_create_new.svg\" alt=\"UI for creating new history\" /></p>   <!-- the original drawing can be found here https://docs.google.com/drawings/d/1cCBrLAo4kDGic5QyB70rRiWJAKTenTU8STsKDaLcVU8/edit?usp=sharing --> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p>Import the files from <a href=\"https://zenodo.org/record/4660497\">Zenodo</a> or from the shared data library</p>\n\n      <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>https://zenodo.org/record/4660497/files/X_test.tsv\nhttps://zenodo.org/record/4660497/files/X_train.tsv\nhttps://zenodo.org/record/4660497/files/y_test.tsv\nhttps://zenodo.org/record/4660497/files/y_train.tsv\n</code></pre></div>      </div>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-importing-via-links\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-importing-via-links\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Importing via links</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Copy the link location</li>   <li>     <p>Click <i class=\"fas fa-upload\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-upload</span> <strong>Upload Data</strong> at the top of the tool panel</p>   </li>   <li>Select <i class=\"fa fa-edit\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-wf-edit</span> <strong>Paste/Fetch Data</strong></li>   <li>     <p>Paste the link(s) into the text field</p>   </li>   <li>     <p>Press <strong>Start</strong></p>   </li>   <li><strong>Close</strong> the window</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-importing-data-from-a-data-library\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-importing-data-from-a-data-library\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Importing data from a data library</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <p>As an alternative to uploading the data from a URL or your computer, the files may also have been made available from a <em>shared data library</em>:</p>   <ol>   <li>Go into <strong>Shared data</strong> (top panel) then <strong>Data libraries</strong></li>   <li>Navigate to  the correct folder as indicated by your instructor.     <ul>       <li>On most Galaxies tutorial data will be provided in a folder named <strong>GTN - Material –&gt; Topic Name -&gt; Tutorial Name</strong>.</li>     </ul>   </li>   <li>Select the desired files</li>   <li>Click on <strong>Add to History</strong> <i class=\"fas fa-caret-down\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-dropdown</span> near the top and select <strong>as Datasets</strong> from the dropdown menu</li>   <li>     <p>In the pop-up window, choose</p>     <ul>       <li><em>“Select history”</em>: the history you want to import the data to (or create a new one)</li>     </ul>   </li>   <li>Click on <strong>Import</strong></li> </ol> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p>Rename the datasets as <code class=\"language-plaintext highlighter-rouge\">X_test</code>, <code class=\"language-plaintext highlighter-rouge\">X_train</code>, <code class=\"language-plaintext highlighter-rouge\">y_test</code>, and <code class=\"language-plaintext highlighter-rouge\">y_train</code> respectively.</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-renaming-a-dataset\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-renaming-a-dataset\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Renaming a dataset</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Click on the <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, change the <strong>Name</strong> field</li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n    <li>\n      <p>Check that the datatype of all the four datasets is <code class=\"language-plaintext highlighter-rouge\">tabular</code>. If not, change the dataset’s datatype to tabular.</p>\n\n      <!--SNIPPET-->\n      <blockquote class=\"tip\">   <div class=\"box-title tip-title\" id=\"tip-changing-the-datatype\"><button class=\"gtn-boxify-button tip\" type=\"button\" aria-controls=\"tip-changing-the-datatype\" aria-expanded=\"true\"><i class=\"far fa-lightbulb\" aria-hidden=\"true\"></i> <span>Tip: Changing the datatype</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>   <ul>   <li>Click on the <i class=\"fas fa-pencil-alt\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-pencil</span> <strong>pencil icon</strong> for the dataset to edit its attributes</li>   <li>In the central panel, click <i class=\"fas fa-database\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-chart-select-data</span> <strong>Datatypes</strong> tab on the top</li>   <li>In the <i class=\"fas fa-database\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">galaxy-chart-select-data</span> <strong>Assign Datatype</strong>, select <code class=\"language-plaintext highlighter-rouge\">tabular</code> from “<em>New type</em>” dropdown     <ul>       <li>Tip: you can start typing the datatype into the field to filter the dropdown menu</li>     </ul>   </li>   <li>Click the <strong>Save</strong> button</li> </ul> </blockquote>\n      <p><!--END_SNIPPET--></p>\n    </li>\n  </ol>\n\n</blockquote>\n\n<h1 id=\"solve-a-simple-regression-problem-using-car-purchase-price-prediction-dataset-via-fnn-in-galaxy\">Solve a simple regression problem using car purchase price prediction dataset via FNN in Galaxy</h1>\n\n<p>In this section, we define a FNN (<span class=\"citation\"><a href=\"#Python\">Python and tutorials 2018</a></span>) and train it using car purchase price prediction dataset (<span class=\"citation\"><a href=\"#Grogan\">Grogan 2020</a></span>). Given 5 attributes\nabout an individual (age, gender, average miles driven per day, personal debt, and monthly income), and the money they spent on purchasing\na car, the goal is to learn a model such that given an individual’s attributes, we can accurately predict how much money they are will spend\npurchasing a car. We then evaluate the trained FNN on the test dataset and plot various graphs to assess the model’s performance. Our training\ndataset has 723 training examples, and our test dataset has 242 test examples. Input features have been scaled to be in 0 to 1 range.</p>\n\n<h2 id=\"create-a-deep-learning-model-architecture\">Create a deep learning model architecture</h2>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Model config</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_config/keras_model_config/1.0.10.0\" title=\"Create a deep learning model architecture tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Create a deep learning model architecture</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.10.0)</span>\n      <ul>\n        <li><em>“Select keras model type”</em>: <code class=\"language-plaintext highlighter-rouge\">sequential</code></li>\n        <li><em>“input_shape”</em>: <code class=\"language-plaintext highlighter-rouge\">(5,)</code></li>\n        <li>In <em>“LAYER”</em>:\n          <ul>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“1: LAYER”</em>:\n              <ul>\n                <li><em>“Choose the type of layer”</em>: <code class=\"language-plaintext highlighter-rouge\">Core -- Dense</code>\n                  <ul>\n                    <li><em>“units”</em>”: <code class=\"language-plaintext highlighter-rouge\">12</code></li>\n                    <li><em>“Activation function”</em>: <code class=\"language-plaintext highlighter-rouge\">relu</code></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“2: LAYER”</em>:\n              <ul>\n                <li><em>“Choose the type of layer”</em>: <code class=\"language-plaintext highlighter-rouge\">Core -- Dense</code>\n                  <ul>\n                    <li><em>“units”</em>”: <code class=\"language-plaintext highlighter-rouge\">8</code></li>\n                    <li><em>“Activation function”</em>: <code class=\"language-plaintext highlighter-rouge\">relu</code></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n            <li><i class=\"far fa-plus-square\" aria-hidden=\"true\"></i><span class=\"visually-hidden\">param-repeat</span> <em>“3: LAYER”</em>:\n              <ul>\n                <li><em>“Choose the type of layer”</em>: <code class=\"language-plaintext highlighter-rouge\">Core -- Dense</code>\n                  <ul>\n                    <li><em>“units”</em>”: <code class=\"language-plaintext highlighter-rouge\">1</code></li>\n                    <li><em>“Activation function”</em>: <code class=\"language-plaintext highlighter-rouge\">linear</code></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          </ul>\n        </li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n</blockquote>\n\n<p>Input has 5 attributes: age, gender, average miles driven per day, personal debt, and monthly income. Our neural network has 3 layers. All\nthree layers are fully connected. The last layer has a single neuron with a linear activation function, used in regression problems. Prior\nlayers use ReLU activation function. The model config can be downloaded as a JSON file.</p>\n\n<h2 id=\"create-a-deep-learning-model\">Create a deep learning model</h2>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Model builder (Optimizer, loss function, and fit parameters)</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_builder/keras_model_builder/1.0.10.0\" title=\"Create deep learning model tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Create deep learning model</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.10.0)</span>\n      <ul>\n        <li><em>“Choose a building mode”</em>: <code class=\"language-plaintext highlighter-rouge\">Build a training model</code></li>\n        <li><em>“Select the dataset containing model configuration”</em>: Select the <em>Keras Model Config</em> from the previous step.</li>\n        <li><em>“Do classification or regression?”</em>: <code class=\"language-plaintext highlighter-rouge\">KerasGRegressor</code></li>\n        <li>In <em>“Compile Parameters”</em>:\n          <ul>\n            <li><em>“Select a loss function”</em>: <code class=\"language-plaintext highlighter-rouge\">mse / MSE / mean_squared_error</code></li>\n            <li><em>“Select an optimizer”</em>: <code class=\"language-plaintext highlighter-rouge\">Adam - Adam optimizer </code></li>\n            <li><em>“Select metrics”</em>: <code class=\"language-plaintext highlighter-rouge\">mse / MSE / mean_squared_error</code></li>\n          </ul>\n        </li>\n        <li>In <em>“Fit Parameters”</em>:\n          <ul>\n            <li><em>“epochs”</em>: <code class=\"language-plaintext highlighter-rouge\">150</code></li>\n            <li><em>“batch_size”</em>: <code class=\"language-plaintext highlighter-rouge\">50</code></li>\n          </ul>\n        </li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n</blockquote>\n\n<p>A loss function measures how different the predicted output is from the expected output. For regression problems, we use\n<em>Mean Squared Error (MSE)</em> loss function, which averages the square of the difference between predicted and actual values for\nthe batch. Epochs is the number of times the whole training data is used to train the model. If we update network weights/biases\nafter all the training data is fed to the network, the training will be slow (as we have 723 training examples in our dataset).\nTo speed up the training, we present only a subset of the training examples to the network, after which we update the weights/biases.\nbatch_size decides the size of this subset (which we set to 50). The model builder can be downloaded as a zip file.</p>\n\n<h2 id=\"deep-learning-training-and-evaluation\">Deep learning training and evaluation</h2>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Training the model</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/1.0.10.0\" title=\"Deep learning training and evaluation tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Deep learning training and evaluation</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.10.0)</span>\n      <ul>\n        <li><em>“Select a scheme”</em>: <code class=\"language-plaintext highlighter-rouge\">Train and Validate</code></li>\n        <li><em>“Choose the dataset containing pipeline/estimator object”</em>: Select the <em>Keras Model Builder</em> from the previous step.</li>\n        <li><em>“Select input type:”</em>: <code class=\"language-plaintext highlighter-rouge\">tabular data</code>\n          <ul>\n            <li><em>“Training samples dataset”</em>: Select <code class=\"language-plaintext highlighter-rouge\">X_train</code> dataset</li>\n            <li><em>“Choose how to select data by column:”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns</code></li>\n            <li><em>“Does the dataset contain header:”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n            <li><em>“Dataset containing class labels or target values”</em>: Select <code class=\"language-plaintext highlighter-rouge\">y_train</code> dataset</li>\n            <li><em>“Choose how to select data by column:”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns</code></li>\n            <li><em>“Does the dataset contain header:”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n          </ul>\n        </li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n\n</blockquote>\n\n<p>The training step generates 2 datasets. 1) accuracy of the trained model, 2) the trained model, in h5mlm format. These files are needed for prediction in the next step.</p>\n\n<h2 id=\"model-prediction\">Model Prediction</h2>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Testing the model</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/model_prediction/model_prediction/1.0.10.0\" title=\"Model Prediction tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Model Prediction</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 1.0.10.0)</span>\n      <ul>\n        <li><em>“Choose the dataset containing pipeline/estimator object”</em> : Select the trained model from the previous step.</li>\n        <li><em>“Choose the dataset containing weights for the estimator above”</em> : Select the trained model weights from the previous step.</li>\n        <li><em>“Select invocation method”</em>: <code class=\"language-plaintext highlighter-rouge\">predict</code></li>\n        <li><em>“Select input data type for prediction”</em>: <code class=\"language-plaintext highlighter-rouge\">tabular data</code></li>\n        <li><em>“Training samples dataset”</em>: Select <code class=\"language-plaintext highlighter-rouge\">X_test</code> dataset</li>\n        <li><em>“Choose how to select data by column:”</em>: <code class=\"language-plaintext highlighter-rouge\">All columns</code></li>\n        <li><em>“Does the dataset contain header:”</em>: <code class=\"language-plaintext highlighter-rouge\">Yes</code></li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n\n</blockquote>\n\n<p>The prediction step generates 1 dataset. It’s a file that has the predicted car purchase price for every row in the test dataset.</p>\n\n<h2 id=\"plot-actual-vs-predicted-curves-and-residual-plots\">Plot actual vs predicted curves and residual plots</h2>\n\n<blockquote class=\"hands_on\">\n  <hands-on-title>Check and visualize the predictions</hands-on-title>\n\n  <ul>\n    <li><span class=\"tool\" data-tool=\"toolshed.g2.bx.psu.edu/repos/bgruening/plotly_regression_performance_plots/plotly_regression_performance_plots/0.1\" title=\"Plot actual vs predicted curves and residual plots tool\" aria-role=\"button\"><i class=\"fas fa-wrench\" aria-hidden=\"true\"></i> <strong>Plot actual vs predicted curves and residual plots</strong> (<i class=\"fas fa-cubes\" aria-hidden=\"true\"></i> Galaxy version 0.1)</span>\n      <ul>\n        <li><em>“Select input data file”</em>: <code class=\"language-plaintext highlighter-rouge\">y_test</code></li>\n        <li><em>“Select predicted data file”</em>”: Select <code class=\"language-plaintext highlighter-rouge\">Model Prediction</code> from the previous step</li>\n        <li>Click <em>“Run Tool”</em></li>\n      </ul>\n    </li>\n  </ul>\n\n</blockquote>\n\n<p>This step generates 3 graphs. The first graph (Figure 6) plots true vs predicted values. The closer the points are to each other,\nthe better our model’s performance at predicting. The second graph (Figure 7) is a scatter plot of true vs predicted values. If\nour model predicts all values correctly, we would get a diagonal line (going from bottom left to upper right). The more the\npredicted vs true points are off this diagonal line, the worse our model’s performance at predicting. The R2 (coefficient of\ndetermination) score for our model is 0.87 (out of the best possible score of 1.0). The RMSE (root mean squared error) is 0.11.\nThe best value for RMSE is obviously 0 for perfect prediction. The third graph (Figure 8) plots residual (predicted - true) vs\npredicted values. The better our model’s predictions, the closer the points to y=0 line.</p>\n\n<figure id=\"figure-6\" style=\"max-width: 90%;\"><img src=\"../../images/FNN_true_predicted_plot.png\" alt=\"True vs predicted values plot. \" width=\"1811\" height=\"960\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/FNN_true_predicted_plot.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 6</strong>:</span> True vs predicted values plot</figcaption></figure>\n\n<figure id=\"figure-7\" style=\"max-width: 90%;\"><img src=\"../../images/FNN_scatter_plot.png\" alt=\"Scatterplot of true vs predicted values plot. \" width=\"1870\" height=\"952\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/FNN_scatter_plot.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 7</strong>:</span> Scatterplot of true vs predicted values plot</figcaption></figure>\n\n<figure id=\"figure-8\" style=\"max-width: 90%;\"><img src=\"../../images/FNN_residual_plot.png\" alt=\"Residual vs predicted values plot. \" width=\"1874\" height=\"958\" loading=\"lazy\" /><a target=\"_blank\" href=\"../../images/FNN_residual_plot.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 8</strong>:</span> Residual vs predicted values plot</figcaption></figure>\n\n<h1 id=\"conclusion\">Conclusion</h1>\n\n<p>In this tutorial, we discussed the inspiration behind the neural networks, and explained Perceptron, one of the earliest neural\nnetworks still in use today. We then discussed different activation functions, what supervised learning is, what are loss/cost functions,\nand how backpropagation minimizes the cost function. Finally, we implemented a FNN in Galaxy to solve a regression problem on car purchase price prediction data.</p>\n"],"ref_slides":["# What is an artificial neural network?\n\n???\n\nWhat is an artificial neural network?\n\n---\n\n# Artificial Neural Networks\n\n- ML discipline roughly inspired by how neurons in a human brain work\n- Huge resurgence due to availability of data and computing capacity\n- Various types of neural networks (Feedforward, Recurrent, Convolutional)\n- FNN applied to classification, clustering, regression, and association\n\n---\n\n# Inspiration for neural networks\n\n- Neuron a special biological cell with information processing ability\n\t- Receives signals from other neurons through its *dendrites*\n\t- If the signals received exceeds a threshold, the neuron fires\n\t- Transmits signals to other neurons via its *axon*\n- *Synapse*: contact between axon of a neuron and denderite of another\n\t- Synapse either enhances/inhibits the signal that passes through it\n\t- Learning occurs by changing the effectiveness of synapse\n\n![Sketch of a biological neuron and its components](/training-material/topics/statistics/images/FNN_bio_neuron.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Celebral cortex\n\n- Outter most layer of brain, 2 to 3 mm thick, surface area of 2,200 sq. cm\n- Has about 10^11 neurons\n\t- Each neuron connected to 10^3 to 10^4 neurons\n\t- Human brain has 10^14 to 10^15 connections\n\n---\n\n# Celebral cortex\n\n- Neurons communicate by signals ms in duration\n\t- Signal transmission frequency up to several hundred Hertz\n\t- Millions of times slower than an electronic circuit\n\t- Complex tasks like face recognition done within a few hundred ms\n\t- Computation involved cannot take more than 100 serial steps\n- The information sent from one neuron to another is very small\n\t- Critical information not transmitted\n\t- But captured by the interconnections\n- Distributed computation/representation of the brain\n\t- Allows slow computing elements to perform complex tasks quickly\n\n---\n\n\n# Perceptron\n\n![Neurons forming the input and output layers of a single layer feedforward neural network](/training-material/topics/statistics/images/FFNN_no_hidden.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Learning in Perceptron\n\n- Given a set of input-output pairs (called *training set*)\n- Learning algorithm iteratively adjusts model parameters\n\t- Weights and biases\n- So the model can accurately map inputs to outputs\n- Perceptron learning algorithm\n\n---\n\n# Limitations of Perceptron\n\n- Single layer FNN cannot solve problems in which data is not linearly separable\n\t- E.g., the XOR problem\n- Adding one (or more) hidden layers enables FNN to represent any function\n\t- *Universal Approximation Theorem*\n- Perceptron learning algorithm could not extend to  multi-layer FNN\n\t- AI winter\n- *Backpropagation* algorithm in 80's enabled learning in multi-layer FNN\n\n---\n\n# Multi-layer FNN\n\n![Neurons forming the input, output, and hidden layers of a multi-layer feedforward neural network](/training-material/topics/statistics/images/FFNN.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n- More hidden layers (and more neurons in each hidden layer)\n\t- Can estimate more complex functions\n\t- More parameters increases training time\n\t- More likelihood of *overfitting*\n\n---\n\n# Activation functions\n\n![Table showing the formula, graph, derivative, and range of common activation functions](/training-material/topics/statistics/images/FNN_activation_functions.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Supervised learning\n\n\n- Training set of size *m*: { (x^1,y^1),(x^2,y^2),...,(x^m,y^m) }\n\t- Each pair (x^i,y^i) is called a *training example*\n\t- x^i is called *feature vector*\n\t- Each element of feature vector is called a *feature*\n\t- Each x^i corresponds to a *label* y^i\n- We assume an unknown function y=f(x) maps feature vectors to labels\n- The goal is to use the training set to learn or estimate f\n\t- We want the estimate to be close to f(x) not only for training set\n\t- But for training examples not in training set\n\n---\n\n# Classification problems\n\n![Three images illustrating binary, multiclass, and multilabel classifications and their label representation](/training-material/topics/statistics/images/FNN_output_encoding.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Output layer\n\n- Binary classification\n\t- Single neuron in output layer\n\t- Sigmoid activation function\n\t- Activation > 0.5, output 1\n\t- Activation <= 0.5, output 0\n- Multilabel classification\n\t- As many neurons in output layer as number of classes\n\t- Sigmoid activation function\n\t- Activation > 0.5, output 1\n\t- Activation <= 0.5, output 0\n---\n\n# Output layer (Continued)\n\n- Multiclass classification\n\t- As many neurons in output layer as number of classes\n\t- *Softmax* activation function\n\t- Takes input to neurons in output layer\n\t- Creates a probability distribution, sum of outputs adds up to 1\n\t- The neuron with the highest proability is the predicted label\n- Regression problem\n\t- Single neuron in output layer\n\t- Linear activation function\n\n---\n\n# Loss/Cost functions\n\n- During training, for each training example (x^i,y^i), we present x^i to neural network\n\t- Compare predicted output with label y^1\n\t- Need loss function to measure difference between predicted & expected output\n- Use *Cross entropy* loss function for classification problems\n- And *Quadratic* loss function for regression problems\n\t- Quadratic cost function is also called *Mean Squared Error (MSE)*\n\n---\n\n# Cross Entropy Loss/Cost functions\n\n![Cross Entropy loss function](/training-material/topics/statistics/images/FNN_Cross_Entropy_Loss.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n![Cross Entropy cost function](/training-material/topics/statistics/images/FNN_Cross_Entropy_Cost.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Quadratic Loss/Cost functions\n\n![Quadratic loss function](/training-material/topics/statistics/images/FNN_Quadratic_Loss.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n![Quadratic cost function](/training-material/topics/statistics/images/FNN_Quadratic_Cost.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Backpropagation (BP) learning algorithm\n\n- A *gradient descent* technique\n\t- Find local minimum of a function by iteratively moving in opposite direction of gradient of function at current point\n- Goal of learning is to minimize cost function given training set\n\t- Cost function is a function of network weights & biases of all neurons in all layers\n\t- Backpropagation iteratively computes gradient of cost function relative to each weight and bias\n\t- Updates weights and biases in the opposite direction of gradient\n\t- Gradients (partial derivatives) are used to update weights and biases\n\t- To find a local minimum\n\n---\n\n# Backpropagation error\n\n![Backpropagation error](/training-material/topics/statistics/images/FNN_BP_Error.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Backpropagation formulas\n\n![Backpropagation formulas](/training-material/topics/statistics/images/FNN_BP.png) <!-- https://pixy.org/3013900/ CC0 license-->\n\n---\n\n# Types of Gradient Descent\n\n- *Batch* gradient descent\n\t- Calculate gradient for each weight/bias for *all* samples\n\t- Average gradients and update weights/biases\n\t- Slow, if we have too many samples\n\n- *Stochastic* gradient descent\n\t- Update weights/biases based on gradient of *each* sample\n\t- Fast. Not accurate if sample gradient not representiative\n\n- *Mini-batch* gradient descent\n\t- Middle ground solution\n\t- Calculate gradient for each weight/bias for all samples in *batch*\n\t\t- batch size is much smaller than training set size\n\t- Average batch gradients and update weights/biases\n\n---\n\n# Vanishing gradient problem\n\n- Second BP equation is recursive\n\t- We have derivative of activation function\n\t- Calc. error in layer prior to output: 1 mult. by derivative value\n\t- Calc. error in two layers prior output: 2 mult. by derivative values\n\n- If derivative values are small (e.g. for Sigmoid), product of multiple small values will be a very small value\n\t- Since error values decide updates for biases/weights\n\t- Update to biases/weights in first layers will be very small\n\t\t- Slowing the learning algorithm to a halt\n\t- The reason Sigmoid not used in deep networks\n\t\t- Why ReLU is popular in deep networks\n\n---\n\n# Car purchase price prediction\n\n- Given 5 features of an individual (age, gender, miles driven per day, personal debt, and monthly income)\n\t- And, money they spent buying a car\n\t- Learn a FNN to predict how much someone will spend buying a car\n- We evaluate FNN on test dataset and plot graphs to assess the model’s performance\n\t- Training dataset has 723 training examples\n\t- Test dataset has 242 test examples\n\t- Input features scaled to be in 0 to 1 range\n\n---\n\n# For references, please see tutorial's References section\n\n---\n\n- Galaxy Training Materials ([training.galaxyproject.org](https://training.galaxyproject.org))\n\n![Screenshot of the gtn stats page with 21 topics, 170 tutorials, 159 contributors, 16 scientific topics, and a growing community](/training-material/topics/introduction/images/gtn_stats.png)\n\n???\n\n- If you would like to learn more about Galaxy, there are a large number of tutorials available.\n- These tutorials cover a wide range of scientific domains.\n\n---\n\n# Getting Help\n\n- **Help Forum** ([help.galaxyproject.org](https://help.galaxyproject.org))\n\n\n  ![Galaxy Help](/training-material/topics/introduction/images/galaxy_help.png)\n\n\n- **Gitter Chat**\n    - [Main Chat](https://gitter.im/galaxyproject/Lobby)\n    - [Galaxy Training Chat](https://gitter.im/Galaxy-Training-Network/Lobby)\n    - Many more channels (scientific domains, developers, admins)\n\n???\n\n- If you get stuck, there are ways to get help.\n- You can ask your questions on the help forum.\n- Or you can chat with the community on Gitter.\n\n---\n\n# Join an event\n\n- Many Galaxy events across the globe\n- Event Horizon: [galaxyproject.org/events](https://galaxyproject.org/events)\n\n![Event schedule](/training-material/topics/introduction/images/event_horizon.png)\n\n???\n\n- There are frequent Galaxy events all around the world.\n- You can find upcoming events on the Galaxy Event Horizon.\n"],"hands_on":true,"slides":true,"mod_date":"2024-05-29 15:37:52 +0000","pub_date":"2021-04-28 06:47:19 +0000","version":29,"workflows":[{"workflow":"Intro_To_FNN_v1_0_10_0.ga","tests":true,"url":"https://training.galaxyproject.org/training-material/topics/statistics/tutorials/FNN/workflows/Intro_To_FNN_v1_0_10_0.ga","path":"topics/statistics/tutorials/FNN/workflows/Intro_To_FNN_v1_0_10_0.ga","wfid":"statistics-FNN","wfname":"intro_to_fnn_v1_0_10_0","trs_endpoint":"https://training.galaxyproject.org/training-material/api/ga4gh/trs/v2/tools/statistics-FNN/versions/intro_to_fnn_v1_0_10_0","license":"CC-BY-4.0","creators":[{"class":"Person","identifier":"0000-0001-6585-3619","name":"Kaivan Kamali"}],"name":"Intro_To_FNN_v1_0_10_0","title":"Intro_To_FNN_v1_0_10_0","test_results":null,"modified":"2024-06-24 07:46:43 +0000","mermaid":"flowchart TD\n  0[\"ℹ️ Input Dataset\\nX_test\"];\n  style 0 stroke:#2c3143,stroke-width:4px;\n  1[\"ℹ️ Input Dataset\\nX_train\"];\n  style 1 stroke:#2c3143,stroke-width:4px;\n  2[\"ℹ️ Input Dataset\\ny_test\"];\n  style 2 stroke:#2c3143,stroke-width:4px;\n  3[\"ℹ️ Input Dataset\\ny_train\"];\n  style 3 stroke:#2c3143,stroke-width:4px;\n  4[\"Create a deep learning model architecture\"];\n  5[\"Create deep learning model\"];\n  4 -->|outfile| 5;\n  6[\"Deep learning training and evaluation\"];\n  5 -->|outfile| 6;\n  1 -->|output| 6;\n  3 -->|output| 6;\n  7[\"Model Prediction\"];\n  6 -->|outfile_object| 7;\n  0 -->|output| 7;\n  8[\"Plot actual vs predicted curves and residual plots\"];\n  2 -->|output| 8;\n  7 -->|outfile_predict| 8;"}],"api":"https://training.galaxyproject.org/training-material/api/topics/statistics/tutorials/FNN/tutorial.json","tools":["toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_builder/keras_model_builder/1.0.10.0","toolshed.g2.bx.psu.edu/repos/bgruening/keras_model_config/keras_model_config/1.0.10.0","toolshed.g2.bx.psu.edu/repos/bgruening/keras_train_and_eval/keras_train_and_eval/1.0.10.0","toolshed.g2.bx.psu.edu/repos/bgruening/model_prediction/model_prediction/1.0.10.0","toolshed.g2.bx.psu.edu/repos/bgruening/plotly_regression_performance_plots/plotly_regression_performance_plots/0.1"],"supported_servers":{"exact":[{"url":"https://usegalaxy.cz/","name":"UseGalaxy.cz","usegalaxy":false},{"url":"https://usegalaxy.eu","name":"UseGalaxy.eu","usegalaxy":true},{"url":"https://usegalaxy.org","name":"UseGalaxy.org (Main)","usegalaxy":true},{"url":"https://usegalaxy.org.au","name":"UseGalaxy.org.au","usegalaxy":true}],"inexact":[{"url":"https://usegalaxy.no/","name":"UseGalaxy.no","usegalaxy":false}]},"topic_name_human":"Statistics and machine learning","admin_install":{"install_tool_dependencies":true,"install_repository_dependencies":true,"install_resolver_dependencies":true,"tools":[{"name":"keras_model_builder","owner":"bgruening","revisions":"66d7efc06000","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"keras_model_config","owner":"bgruening","revisions":"f22a9297440f","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"keras_train_and_eval","owner":"bgruening","revisions":"818f9b69d8a0","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"model_prediction","owner":"bgruening","revisions":"9991c4ddde14","tool_panel_section_label":"Machine Learning","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"},{"name":"plotly_regression_performance_plots","owner":"bgruening","revisions":"389227fa1864","tool_panel_section_label":"Graph/Display Data","tool_shed_url":"https://toolshed.g2.bx.psu.edu/"}]},"admin_install_yaml":"---\ninstall_tool_dependencies: true\ninstall_repository_dependencies: true\ninstall_resolver_dependencies: true\ntools:\n- name: keras_model_builder\n  owner: bgruening\n  revisions: 66d7efc06000\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: keras_model_config\n  owner: bgruening\n  revisions: f22a9297440f\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: keras_train_and_eval\n  owner: bgruening\n  revisions: 818f9b69d8a0\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: model_prediction\n  owner: bgruening\n  revisions: 9991c4ddde14\n  tool_panel_section_label: Machine Learning\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n- name: plotly_regression_performance_plots\n  owner: bgruening\n  revisions: 389227fa1864\n  tool_panel_section_label: Graph/Display Data\n  tool_shed_url: https://toolshed.g2.bx.psu.edu/\n","tours":false,"video":false,"slides_recordings":false,"translations":{"tutorial":[],"slides":[],"video":false},"license":"CC-BY-4.0","type":"tutorial"}